Lev Manovich
EL SOFTWARE TOMA EL MANDO
Traducción de Software Takes Command (versión del 30 de septiembre de 2012, publicada bajo licencia Creative
Commons en manovich.net) por Everardo Reyes-García. Este documento tiene únicamente intenciones
educativas, artísticas y científicas.
Nota: todo el contenido y notas de pié de página fueron traducidos por mí. He tratado de apegarme lo más
posible al estilo del autor. Me he permitido introducir anglicismos y tecnicismos según lo consideré necesario. He
revisado varias veces esta versión y espero que el lector no encuentre errores ortográficos, si los hubiera puede
contactarme libremente.
Versión 1.1
Procesador de texto: Microsoft Word 14.4.2 para Mac
Fuente: Franklin Gothic Book

2
INTRODUCCIÓN 4
COMPRENDER LOS MEDIOS DE COMUNICACIÓN 4
SOFTWARE, EL MOTOR DE LAS SOCIEDADES CONTEMPORÁNEAS 8
¿QUÉ SON LOS ESTUDIOS DEL SOFTWARE? 12
SOFTWARE CULTURAL 19
APLICACIONES DE MEDIOS 22
DE LOS DOCUMENTOS A LOS ACTOS 31
¿POR QUE NO EXISTE UNA HISTORIA DEL SOFTWARE CULTURAL? 37
SUMARIO DE LA NARRATIVA DEL LIBRO 40
PRIMERA PARTE: LA INVENCIÓN DEL SOFTWARE DE MEDIOS 50
CAPÍTULO 1. LA MÁQUINA UNIVERSAL DE MEDIOS DE ALAN KAY 50
APARIENCIA VS. FUNCIÓN 50
“LA SIMULACIÓN ES LA NOCIÓN CENTRAL DEL DYNABOOK” 59
LA EXTENSIBILIDAD PERMANENTE 77
LA COMPUTADORA COMO META-MEDIO 86
CAPÍTULO 2. PARA ENTENDER LOS METAMEDIOS 92
LOS COMPONENTES BÁSICOS 92
TÉCNICAS INDEPENDIENTES Y ESPECÍFICAS A LOS MEDIOS 98
ADENTRO DE PHOTOSHOP 107
SOLAMENTE HAY SOFTWARE 127
SEGUNDA PARTE: HIBRIDACIÓN Y EVOLUCIÓN 138
CAPÍTULO 3: HIBRIDACIÓN 138
HIBRIDACIÓN VS. MULTIMEDIA 138
LA EVOLUCIÓN DE UN METAMEDIO COMPUTACIONAL 149
HIBRIDACIÓN: EJEMPLOS 157
ESTRATEGIAS DE LA HIBRIDACIÓN 166
CAPÍTULO 4. EVOLUCIÓN DEL SOFTWARE 169
ALGORITMOS Y ESTRUCTURAS DE DATOS 170
¿QUÉ ES UN “MEDIO”? 175
¿EL METAMEDIO O EL MONOMEDIO? 196
LA EVOLUCIÓN DE LAS ESPECIES DE MEDIOS 206

3
TERCERA PARTE: EL SOFTWARE EN ACCIÓN 213
CAPÍTULO 5. DISEÑO DE MEDIOS 213
AFTER EFFECTS Y LA REVOLUCIÓN INVISIBLE 213
LA ESTÉTICA DE LA HIBRIDACIÓN 223
REMEZCLA PROFUNDA 231
CAPAS, TRANSPARENCIA Y COMPOSICIÓN 240
LA INTERFAZ DE AFTER EFFECTS: DEL TIEMPO A LA COMPOSICIÓN 245
EL ESPACIO 3D COMO PLATAFORMA DE DISEÑO DE MEDIOS 251
IMPORTAR-EXPORTAR: FLUJO DE TRABAJO DEL DISEÑO 257
FORMA VARIABLE 267
AMPLIFICACIÓN 278
CONCLUSIÓN 283
SOFTWARE, HARDWARE Y MEDIOS SOCIALES 283
LOS MEDIOS DESPUÉS DEL SOFTWARE 289
EPISTEMOLOGÍA DEL SOFTWARE 291

4
Introducción
Comprender los medios de comunicación
A mi anterior estudio sobre las nuevas formas culturales que son posibles con la
computadora lo llamé The Language of New Media. Este libro fue escrito en 1999. En ese
tiempo, el proceso de adopción de herramientas basadas en software en todas las áreas
de la producción de medios de comunicación profesionales ya se había dado y el “arte de
los nuevos medios” estaba en su etapa memorable y resplandeciente, ofreciendo muchas
posibilidades que aún no se veían en el software comercial o en productos electrónicos
para consumidores.
Diez años después, la mayoría de los medios se volvieron “nuevos medios”. Los
desarrollos de los años 90 se han diseminado a cientos de millones de personas que
están escribiendo blogs, subiendo fotos y videos a los sitios sociales, y usando de forma
libre (o casi) herramientas de software de producción y de edición que hace algunos años
costaban decenas de miles de dólares.
En gran medida, gracias a las prácticas iniciadas por Google, el mundo está ahora
acostumbrado a usar aplicaciones y servicios Web, que nunca han sido oficialmente
terminados o lanzados, pero que están en una constante etapa Beta. Debido a que estas
aplicaciones y servicios funcionan en servidores remotos, pueden ser actualizados en
cualquier momento, sin la necesidad de que el consumidor haga algo (de hecho, Google
actualiza su algoritmo de búsqueda unas cuantas veces al día; lo mismo Facebook,
actualiza su código diariamente). Bienvenidos al mundo del cambio permanente: el mundo
que ya no está definido por las máquinas industriales pesadas, que casi no mutan, sino
por software que está siempre en flujo.
¿Por qué las humanidades, las ciencias sociales, las ciencias de la comunicación, los
estudios culturales se preocupan por el software? Porque sólo fuera de algunas áreas
aisladas, como las artesanías y las bellas artes, el software ha reemplazado diversos
conjuntos de tecnologías físicas, mecánicas y electrónicas que se usaban el siglo pasado

5
para crear, almacenar, distribuir e interactuar con artefactos culturales. Cuando escribes
una carta en Word (o su equivalente open source), estás usando software. Cuando envías
un “tweet” o publicas un comentario en Facebook o haces una búsqueda entre billones de
videos en YouTube, estás usando software (específicamente, aquellos que se llaman
“webware” o “aplicaciones Web”, o sea el software que es accedido vía navegadores Web y
que reside en servidores.
Y cuando juegas un videojuego, exploras una instalación interactiva en un museo, diseñas
un edificio, creas efectos especiales para una película, diseñas un sitio Web, usas un
teléfono celular para ver la reseña de una película (o para ver la película misma), o cuando
llevas a cabo otras miles de “actividades culturales”, en términos prácticos estás haciendo
la misma cosa, es decir, usando software. El software se ha vuelto nuestra interfaz con el
mundo, con otras personas, con nuestra memoria e imaginación; un lenguaje universal
mediante el cuál habla el mundo, un motor universal mediante el cuál funciona el mundo.
El software es para los inicios del siglo XXI lo que fueron la electricidad y los motores de
combustión para los inicios del siglo XX.
Este libro trata del “software de los medios”, programas como Word, PowerPoint,
Photoshop, Illustrator, After Effects, Final Cut, Firefox, Blogger, WordPress, Google Earth y
Maya. El software de los medios es un subconjunto particular de software o webware que
permite la creación, publicación, intercambio y recreación de imágenes, secuencias de
imágenes en movimiento, diseños 3D, textos, mapas, elementos interactivos, así como
varias combinaciones de estos: sitios Web, aplicaciones interactivas, animaciones,
guantes virtuales, etc.). El software de los medios incluye también navegadores Web como
Firefox y Safari, programas para e-mail y chat, lectores de noticias y demás tipos de
software de aplicaciones cuyo objetivo principal es acceder al contenido (manteniendo en
ocasiones la facultad de crearlo y editarlo).
Las herramientas computacionales para crear, compartir e interactuar con medios
representan un grupo particular de aplicaciones software (incluyendo las aplicaciones web)
en general. Así, podemos pensar que todas estas herramientas heredan ciertos “rasgos”
comunes a todo el software contemporáneo. ¿Significará esto que, sin importar que estés
trabajando en el diseño de un edificio, creando efectos especiales para una película,

6
diseñando un sitio Web o haciendo gráficos de información, tu proceso de diseño siga una
lógica similar? ¿Existirán propiedades estructurales que las animaciones, el diseño gráfico,
los sitios Web, el diseño de productos, los edificios y los videojuegos compartan debido al
hecho que fueron diseñados con software? De manera más amplia, ¿de qué manera las
interfaces y las herramientas de software de creación de medios están dando forma a la
estética contemporánea y los lenguajes visuales de las diferentes formas de los medios?
Pero detrás de estas preguntas, que este libro aborda a detalle, existe otra cuestión
teórica. Esta cuestión es la columna vertebral de la narrativa de este libro y ha motivado la
elección de los temas a tratar. ¿Qué sucede con la noción de “medio” una vez de que las
herramientas que fueron hechas específicamente para uno de ellos han sido simuladas y
extendidas en el software? Después de todo, ¿es valido seguir hablando de diferentes
medios? ¿O más bien nos encontramos en un nuevo mundo: el de un mono-medio o metamedio
(para tomar prestado el término del protagonista de este libro, Alan Kay)?
En pocas palabras, ¿qué son los “medios” después del software?
Para poner esta pregunta en términos más radicales: ¿Existen todavía los “medios”?
Este libro es una explicación teórica sobre el software de los medios y sus efectos en la
práctica y en la noción misma de “medio”. En las últimas dos décadas, el software de los
medios ha reemplazado la mayoría de las otras tecnologías de los medios que surgieron en
los siglos XIX y XX. Hoy es ubicuo e incuestionado. Sorprendentemente, sólo pocas
personas conocen su historia y las ideas teóricas detrás de su desarrollo. Es probable que
conozcas los nombres de los artistas del Renacimiento que divulgaron el uso de la
perspectiva lineal en el arte occidental (Brunelleschi, Alberti) o a los inventores del
lenguaje cinematográfico de principios del siglo XX (D.W. Griffith, Eisenstein, etc.), pero
apuesto que no sabes de dónde viene Photoshop, o Word, o cualquier otra herramienta de
medios que usas a diario. Más importante aún, quizá no sepas porqué estas herramientas
fueron inventadas en su origen, es decir, cómo y porqué las computadoras originales del
tamaño de un cuarto, que sólo eran usadas por el gobierno, por las grandes empresas y
por los científicos fueron reinventadas como máquinas personales de medios.

7
¿Cuál es la historia intelectual del software de los medios? ¿cuáles eran las ideas y
motivaciones de personajes clave (como J.C. Licklider, Ivan Sutherland, Ted Nelson,
Douglas Engelbart, Alan Kay, Nicholas Negroponte, y de los grupos de investigación que
dirigían), quienes entre 1960 y los 70s crearon la mayoría de los conceptos y técnicas que
han dado forma al software de medios de hoy? Como he descubierto (y espero que
igualmente compartas mi sorpresa al leer mi análisis a los textos originales de estas
personas), fueron tan teóricos de los medios como ingenieros en computación. Este libro
pretende estudiar sus teorías de medios y confrontarlas con el desarrollo de los medios
digitales de las décadas subsecuentes. Como veremos, las ideas teóricas de estas
personas, y sus colaboradores, siguen vigentes; nos ayudan a entender mejor el software
cultural contemporáneo que usamos para crear, leer, recrear y compartir.
Bienvenidos entonces a la “historia secreta” de nuestro software cultural. Y digo secreta no
porque haya estado oculta deliberadamente, sino porque sólo hasta ahora, emocionados
con las rápidas transformaciones de la computación cultural, no nos habíamos
preocupado por examinar sus orígenes. Este libro tratará de convencerte de que dicha
examinación vale mucho la pena en nuestros tiempos.
Mi investigación se encuentra en un paradigma intelectual más amplio conocido como
“estudios del software”. Desde esta perspectiva, la contribución de este libro es el análisis
de las ideas que llevaron al software de los medios durante los 1980’s y 1990’s, y de los
efectos en la adopción de este tipo de software en el diseño de medios contemporáneos y
en la cultura visual.
Hay que notar que la categoría software de los medios es un subgrupo de la categoría
software de aplicaciones; ésta última es a su vez un subgrupo de la categoría software1,
que no sólo incluye software de aplicaciones, software de sistemas y herramientas de
programación computacional, sino también servicios de redes sociales y tecnologías de
medios sociales2.
1 http://en.wikipedia.org/wiki/List_of_software_categories, Julio 7, 2011.
2 Andreas Kaplan y Michael Haenlein definen a los medios sociales como “un grupo de aplicaciones
basadas en Internet que funcionan sobre las bases ideológicas y tecnológicas del Web 2.0, que
permite la creación e intercambio de contenido generado por los usuarios”. Andreas Kaplan &

8
Si entendemos el software desde esta amplia perspectiva, podemos preguntarnos: ¿qué
significa vivir en una “sociedad de software”? ¿qué significa ser parte de una “cultura de
software”? Estas son las cuestiones que abordamos en la siguiente sección.
Software, el motor de las sociedades contemporáneas
A inicios de los 1990’s, las marcas globales más famosas eran las compañías que estaban
en el negocio de la producción de bienes materiales o del procesamiento de materias
físicas. Hoy, por el contrario, en las listas de las marcas más reconocidas están nombres
como Google, Yahoo y Microsoft. De hecho, en 2007, Google se volvió la número 1 en
términos de identidad de marca. En Estados Unidos, los periódicos y revistas más leídos,
como The New York Times, USA Today, Business Week, etc., publican a diario noticias
sobre YouTube, Facebook, Twitter, Apple, Google y otras compañías de tecnologías de la
información (TI).
¿Pero qué pasa en otros medios? Cuando estaba trabajando en la primera versión de este
libro, en 2008, entré al sitio Web de CNN y navegué por su sección de negocios en donde
vi datos mercantiles de sólo diez compañías e índices mostrados desde la página
principal3. Aunque la lista cambia diario, es recurrente que incluya algunas de las mismas
marcas de TI. Tomemos el 21 de enero de 2008, por ejemplo. En aquel día, la lista de CNN
contenía las siguientes compañías e índices: Google, Apple, S&P 500 Index, Nasdaq
Composite Index, Dow Jones Industrial Average, Cisco Systems, General Electric, General
Motors, Ford, Intel4.
Esta lista dice mucho. Las compañías relacionadas con bienes físicos y energía aparecen
en la segunda parte: General Electric, General Motors, Ford. Después, tenemos dos
Michael Haenlein, "Users of the world, unite! The challenges and opportunities of Social Media,"
Business Horizons 53, no. 1 (Enero–Febrero 2010), pp. 59–68,
http://dx.doi.org/10.1016/j.bushor.2009.09.003.
3 http://money.cnn.com, January 21, 2008.
4 Ibid.

9
compañías que producen hardware: Intel hace microprocesadores, Cisco hace equipo de
redes. ¿Qué hay acerca de las dos compañías en la cabeza, Google y Apple? La primera
parece que está en el negocio de la información, mientras que la segunda hace
electrónicos consumibles: computadoras portátiles, monitores, reproductores de música,
etc. Pero en realidad ambas están haciendo algo diferente. Y, aparentemente, lo que es
diferente es tan crucial para el funcionamiento de la economía de Estados Unidos (y por
consecuencia del mundo global también), que estas compañías aparecen diario en las
noticias de negocios. Otras compañías de Internet que también aparecen de forma
cotidiana (Facebook, Twitter, Amazon, eBay, Yahoo) están también en el mismo negocio.
Esto que es “algo diferente” es el software. Motores de búsqueda, sistemas de
recomendación, aplicaciones de mapas, herramientas de blog, herramientas de subasta,
clientes de mensajes instantáneos y, por supuesto, plataformas que permiten escribir
nuevo software (iOS, Android, Facebook, Windows, Linux) están en el centro de la
economía global, la cultura, la vida social y, cada vez más, la política. Y este “software
cultural” (cultural en el sentido que es usado directamente por cientos de millones de
personas y que lleva “átomos” de cultura: medios e información, así como interacciones
humanas alrededor de estos medios e información) es sólo una parte visible de un
universo de software más grande.
En la propuesta del libro Software Society, que Benjamin Bratton y yo hicimos a la editorial
MIT Press en 2003, describíamos el papel central que juega el software y su relativa
invisibilidad en las humanidades y ciencias sociales:
El software controla el vuelo de un misil inteligente hacia su blanco durante la
guerra, ajustando su trayectoria en el vuelo. El software recorre almacenes y
líneas de producción de Amazon, Gap, Dell y muchas otras compañías, para
ensamblar y despachar objetos materiales a todas partes del mundo, casi de
forma instantánea. El software permite a las tiendas y supermercados reabastecer
automáticamente sus estantes, así como determinar automáticamente cuáles
productos deben rebajarse, a cuánto, cuándo y en qué parte de la tienda. El
software, claro, es lo que organiza el Internet, encaminando mensajes de e-mail,
enviando páginas Web desde un servidor, intercambiando tráfico de redes,

10
asignando direcciones IP y mostrando páginas Web en un navegador. La escuela y
el hospital, la base militar y el laboratorio científico, el aeropuerto y la ciudad
(todas los sistemas sociales, económicos y culturales de la sociedad moderna),
funcionan con software. El software es el pegamento invisible que une todo.
Mientras varios sistemas de la sociedad moderna hablan en diferentes lenguajes
y tienen objetivos diferentes, todos comparten la sintaxis del software: estados de
control “si-entonces” y “mientras-haz”, operadores y tipos de datos incluyendo
cadenas de caracteres y números con puntos decimales, estructuras de datos
como listas y convenciones de interfaz con menús y cajas de diálogo.
Si la electricidad y el motor de combustión hicieron posible la sociedad industrial,
de forma similar el software hace posible la sociedad de la información. Los
“trabajadores del conocimiento”, los “analistas de símbolos”, las “industrias
creativas” y las “industrias de servicios”. Todos estos actores económicos clave de
la sociedad de la información no pueden existir sin software. El software de
visualización de datos que usa un científico, la hoja de cálculo que usa un analista
financiero, el software de diseño Web que usa un diseñador de transnacional
anunciando energía, el software de reservaciones que usa una aerolínea. El
software dirige también el proceso de globalización, permitiendo a las compañías
la distribución de nodos administrativos, instalaciones para la producción y puntos
de almacenamiento y consumo alrededor del mundo. Sin importar en cuál nueva
dimensión de la existencia contemporánea se haya enfocado alguna teoría social
de las últimas décadas (sociedad de la información, sociedad del conocimiento,
sociedad en red), todas estas dimensiones son facilitadas por el software.
Paradójicamente, mientras sociólogos, filósofos, críticos culturales y teóricos de
los medios y de los nuevos medios parece que ya han cubierto todos los aspectos
de la revolución de las TI, incluso creando nuevas disciplinas como la cibercultura,
los estudios de Internet, los estudios de videojuegos, las teorías de los nuevos
medios, la cultura digital y las humanidades digitales, el motor que mantiene todo
esto, el software, ha recibido comparativamente poca atención.

11
Todavía hoy, diez años después, cuando la gente interactúa y actualiza decenas de
aplicaciones en sus teléfonos móviles y demás aparatos computacionales, el software
como categoría teórica es invisible para la mayoría de los académicos, artistas y
profesionales de la cultura interesados en las TI y sus efectos sociales y culturales.
Hay algunas excepciones importantes. Una de ellas es el movimiento Open Source y las
cuestiones relacionadas con los derechos de autor y propiedad intelectual que ya han sido
discutidas ampliamente en muchas disciplinas académicas. También vemos un rápido
número creciente de libros profesionales sobre Google, Facebook, Amazon, eBay, Oracle y
otros gigantes del Web.
Algunos de estos libros ofrecen discusiones interesantes sobre el software y sus conceptos
básicos, tal como fueron desarrollados por dichas compañías, así como los efectos
sociales, políticos, cognitivos y epistemológicos del software. Para un buen ejemplo,
puedes consultar John Battelle, The Search: How Google and Its Rivals Rewrote the Rules
of Business and Transformed Our Culture5. En este ámbito, los periodistas están más
avanzados que los académicos de las humanidades y ciencias sociales, que necesitan
actualizarse.
Hoy estamos en una mejor situación que cuando hicimos nuestra propuesta, Bratton y yo,
en el 2003. Creo que aún es significativo citar un fragmento de dicho texto (he añadido
nuevos fenómenos de la cultural del software como los “medios sociales”, que aún no eran
visibles en 2003):
Si limitamos las discusiones críticas sobre la cultura digital a las nociones de
“acceso libre”, “producción por pares”, “ciber”, “digital”, “Internet”, “redes”,
“nuevos medios” o “medios sociales”, nunca llegaremos a eso que está detrás de
las nuevos medios de representación y comunicación, y a entender lo que
realmente es y lo que hace. Si no cuestionamos al software mismo, estamos en
peligro de lidiar siempre con sus efectos en lugar de las causas: el resultado que
5 John Battelle, The Search: How Google and Its Rivals Rewrote the Rules of Business and
Transformed Our Culture (Portfolio Trade, 2006).

12
aparece en la pantalla de la computadora en lugar de los programas y culturas
sociales que producen estos resultados. Es tiempo de concentrarnos en el
software mismo.
Un sentimiento similar es expresado por Noah Wardrip-Fruin en Expressive Processing
(2009), cuando dice (en relación a los libros sobre literatura digital): “casi todos ellos se
han enfocado en cómo las máquinas de medios digitales se ven desde afuera: sus
resultados… sin importar la perspectiva, casi todos los estudios sobre medios digitales
ignoran algo crucial: los verdaderos procesos que hacen que funcionen los medios
digitales, las máquinas computacionales que hacen posible los medios digitales”. Mi libro
se enfoca en lo que considero la parte clave de estas “máquinas” (debido a que es la parte
que la mayoría de los usuarios ve y usa directamente): el software de aplicaciones.
¿Qué son los estudios del software?
Este libro pretende hacer aportaciones al desarrollo del paradigma intelectual de los
“estudios del software”. ¿Qué son los estudios del software? He aquí algunas definiciones.
La primera es de mi libro El lenguaje de los nuevos medios (escrito en 1999, publicado por
el MIT Press en 2001) en donde, según mi conocimiento, los términos “estudios del
software” y “teoría del software” aparecieron por primera vez. Yo escribía: “los nuevos
medios exigen una nueva etapa en la teoría de medios, cuyos orígenes pueden remontarse
a los trabajos revolucionarios de Robert Innis y Marshall McLuhan en los 1950’s. Para
comprender la lógica de los nuevos medios necesitamos mirar hacia las ciencias
computacionales. Es ahí en donde podemos esperar el descubrimiento de nuevos
términos, categorías y operaciones que caracterizan a los medios que se han vuelto
programables. De los estudios de medios, nos movemos a algo que puede llamarse
estudios del software; de la teoría de medios a las teoría del software.
Al leer esta frase hoy, siento que deben hacerse algunos ajustes. Las ciencias
computacionales estaban vistas como una verdad absoluta, como un todo dado que puede
explicarnos cómo funciona la cultura en la sociedad del software. Pero la informática es,
ella misma, parte de la cultura. Así, pienso que los Estudios del Software deben investigar

13
el papel del software en la formación de la cultura contemporánea y, al mismo tiempo, los
factores culturales, sociales y económicos que influyen en el desarrollo del software
mismo, así como su distribución en la sociedad.
La primera obra que demostró la necesidad de esta segunda perspectiva fue The New
Media Reader, editado por Noah Wardrip-Fruin y Nick Montfort (MIT Press, 2003). La
publicación de esta innovadora antología delimito el marco teórico para el estudio histórico
del software y su relación con la historia de la cultura. A pesar de que el libro no usa
explícitamente el término “estudios del software”, sí propuso un nuevo modelo para
pensar sobre del software. Mediante una yuxtaposición sistemática de textos
fundamentales de, por un lado, pioneros de la computación cultural y, por otro, artistas
activos en los mismos periodos históricos, la antología demostró que ambos pertenecían a
los mismos grandes epistemes. De forma recurrente, una misma idea era articulada
simultáneamente por artistas y científicos que estaban inventando justamente la
computación cultural. Por ejemplo, el Reader inicia con la historia de Jorge Luis Borges
(1941) y el famoso artículo de Vannevar Bush (1945). Ambos contienen la idea de una
estructura enorme con ramificaciones como modo de organización de datos y
representación de la experiencia humana.
En febrero 2006, Matthew Fuller, quien ya había publicado un libro precursor sobre el
software como cultura (Behind the Blip, essays on the culture of software, Autonomedia,
2003), organizó el primer taller de estudios del software en el Instituto Piet Zwart, en
Rotterdam. Para introducir el taller, Fuller escribió: “el software es comúnmente un punto
ciego en la construcción teórica y estudio de medios digitales y en red. Pero se trata de la
base y la “cosa” del diseño de medios. En cierto sentido, todo trabajo intelectual es ahora
“estudio del software”, debido a que el software provee los medios y el contexto. Pero hay
pocos lugares en donde la naturaleza específica, la materialidad del software es estudiada,
excepto como tema de ingeniería” 6.
Concuerdo con Fuller cuando dice “todo trabajo intelectual es ahora “estudio del
software”. Pero tomará un tiempo mientras los intelectuales se den cuenta de ello. Para
6 http://pzwart.wdka.hro.nl/mdr/Seminars2/softstudworkshop, enero 21, 2008.

14
motivar el cambio, en 2008 nos unimos Matthew Fuller, Noah Wardrip-Fruin y yo para crear
la colección Estudios del Software en el MIT Press. Los títulos que ya han sido publicados
son Software Studies: A Lexicon editado por Fuller (2008), Expressive Processing: Digital
Fictions, Computer Games, and Software Studies de Wardrip-Fruin (2009), Programmed
Visions: Software and Memory de Wendy Hui Kyong Chun (2011), Code/Space: Software
and Everyday Life de Rob Kitchin and Martin Dodge (2011), y Speaking Code: Coding as
Aesthetic and Political Expression de Geoff Cox y Alex Mclean (2012). En 2011, Fuller, en
conjunto con varios investigadores ingleses, fundaron Computational Culture, una revista
arbitrada, de acceso libre y revisión por pares, que seguramente será una plataforma para
más publicaciones y discusiones.
De forma paralela a nuestra colección, que aborda específicamente estudios del software,
es estimulante ver otros libros relacionados, como aquellos publicados por Doug Sery en el
“área de nuevos medios” en el MIT Press7 y de otras editoriales. Desde la perspectiva de
los estudios de las plataformas, las humanidades digitales, la cibercultura, los estudios del
Internet, y otros, se hallan observaciones interesantes que nos ayudan a entender mejor la
categoría del software. En lugar de enlistar todos los títulos, daré algunos ejemplos que
tratan estas perspectivas (seguramente habrá más para cuando estés leyendo esto).
Estudios de las plataformas: Racing the Beam: The Atari Video Computer System (2009)
de Nick Montfort y Ian Bogost; The Future Was Here: The Commodore Amiga (2012) de
Jimmy Maher; Mechanisms: New Media and the Forensic Imagination (2008) de Matthew
G. Kirschenbaum. Humanidades digitales: The Philosophy of Software: Code and Mediation
in the Digital Age (David Berry, 2011); Reading Machines: Toward an Algorithmic Criticism
(Stephen Ramsay, 2011); How We Think: Digital Media and Contemporary Technogenesis
(Katherine Hayles, 2012) 8.
7 http://mitpress.mit.edu/.
8 Nick Montfort y Ian Bogost, Racing the Beam: The Atari Video Computer System (The MIT Press,
2009); Jimmy Maher, The Future Was Here: The Commodore Amiga (The MIT Press, 2012); David
Berry, The Philosophy of Software: Code and Mediation in the Digital Age (Palgrave Macmillan, 2011);
Stephen Ramsay. Reading Machines: Toward an Algorithmic Criticism (University of Illinois Press,
2011), Katherine Hayles, How We Think: Digital Media and Contemporary Technogenesis (University
of Chicago Press, 2012).

15
Otro cuerpo de estudios muy relevante para entender los roles y funciones de los sistemas
de software proviene de personas que fueron formadas en las ciencias computacionales
pero que se sienten igualmente cómodas en la teoría cultural, la filosofía, el arte digital y
otras áreas de las humanidades: Phoebe Sengers, Warren Sack, Fox Harrell, Michael
Mateas, Paul Dourish, Phil Agre.
Una categoría de libros no menos valiosa son los estudios históricos de los laboratorios y
grupos de investigación que fueron piezas clave en el desarrollo de la sociedad moderna
del software. En ellos de observan la influencia que tuvieron en las tecnologías de
información el nuevo software (como Internet) y el desarrollo de prácticas profesionales de
la ingeniería del software. Como ejemplos tenemos (mencionados en orden cronológico):
Where Wizards Stay Up Late: The Origins Of The Internet (1998) de Katie Hafner y Mathew
Lyon; Dealers of Lightning: Xerox PARC and the Dawn of the Computer Age (2000) de
Michael Hiltzik; From Airline Reservations to Sonic the Hedgehog: A History of the Software
Industry (2004) de Martin Campbell-Kelly; y, The Computer Boys Take Over: Computers,
Programmers, and the Politics of Technical Expertise (2010) de Nathan Ensmenger9.
Mi libro favorito sigue siendo Tools for Thought publicado por Howard Rheingold en 1985,
justo antes del inicio de la era de la domesticación de las computadoras y software, y que
llevó a su reciente ubicuidad. Este libro gira en torno a la idea que las computadoras y el
software no son sólo “tecnologías”, sino más bien el nuevo medio en que se puede pensar
e imaginar de forma diferente. Esta premisa la entendieron los héroes quienes, junto con
sus colaboradores, inventaron aquellas “herramientas para el pensamiento”: J. C. R.
Licklider, Ted Nelson, Douglas Engelbart, Bob Taylor, Alan Kay, Nicholas Negroponte. A la
fecha, muchos académicos de las humanidades y ciencias sociales aún no han captado
esta idea fundamental. Se sigue pensando que el software pertenece estrictamente al
departamento de Ciencias Computacionales de las universidades, como algo que está ahí
9 Katie Hafner y Mathew Lyon, Where Wizards Stay Up Late: The Origins Of The Internet (Simon &
Schuster, 1998); Michael A. Hiltzik, Dealers of Lightning: Xerox PARC and the Dawn of the Computer
Age (HarperBusiness, 2000); Martin Campbell-Kelly, From Airline Reservations to Sonic the
Hedgehog: A History of the Software Industry (The MIT Press, 2004); Nathan L. Ensmenger, The
Computer Boys Take Over: Computers, Programmers, and the Politics of Technical Expertise (The MIT
Press, 2010).

16
para ayudarlos a ser más eficientes, en lugar de verlo como el lugar en donde radica la
creatividad del intelecto humano.
Al reconocer que los rasgos de los estudios del software existen en varios libros, Fuller
escribe en el prefacio a nuestra colección:
El software está íntimamente ligado a nuestra vida contemporánea (económica, cultural,
creativa y políticamente), de formas obvias pero a la vez invisibles. Se escribe mucho
acerca de cómo se usa el software y de las actividades que permite hacer y que modela,
pero el pensamiento sobre el software mismo ha sido sobretodo técnico en la mayor parte
de su historia. Sin embargo, cada vez más, artistas, investigadores, ingenieros, hackers,
diseñadores y académicos de las humanidades y ciencias sociales se están dando cuenta
que para abordar las cuestiones que enfrentan, y hacer las cosas que necesitan, es
necesario expandir el conocimiento sobre el software. Para este entendimiento, pueden
echar mano a textos sobre la historia de la computación y nuevos medios, pueden tomar
partido en la rica e implícita cultura del software, y también pueden contribuir al desarrollo
de la emergente y transdisciplinaria alfabetización computacional. Esto provee los
fundamentos para los Estudios del Software10.
De hecho, una parte de los trabajos iniciales de los principales teóricos de medios de
nuestro tiempo, entre ellos, Friedrich A. Kittler, Peter Weibel, Bruno Latour, Katherine
Hayles, Lawrence Lessig, Manual Castells, Alex Galloway, puede ser vista retroactivamente
como “estudios del software” 11. Por esta razón, pienso que este paradigma ya ha existido
durante algún tiempo, pero no había sido nombrado explícitamente hasta hace unos años.
En su introducción al taller de Rotterdam en 2006, Fuller apuntaba que: “el software
puede ser visto como un objeto de estudio y como un terreno para la práctica del arte y de
la teoría del diseño y de las humanidades, para los estudios culturales y la ciencia y la
tecnología, y para una visión reflexiva aún en desarrollo para las ciencias
10 Mathew Fuller, Software Studies series introduction,
http://mitpress.mit.edu/catalog/browse/browse.asp?btype=6&serid=179, July 14, 2011.
11 Consulta la reseña que Michael Truscello hace a “Behind the Blip: Essays on the Culture of
Software,” Cultural Critique 63, Primavera 2006, pp. 182-187.

17
computacionales”. Una nueva disciplina académica puede ser definida ya sea por su
objeto de estudio único, por su nueva metodología de investigación, o por una
combinación de ambos. ¿Cómo podemos pensar los estudios del software? La premisa de
Fuller implica que “el software” sea un nuevo objeto de estudio que debe ser incluido en la
agenda de disciplinas ya existentes, y que pueda ser estudiado con métodos conocidos,
por ejemplo la teoría del actor-red, la semiótica social o la arqueología de medios.
Creo que hay buenas razones para apoyar esta premisa. Pienso en el software como una
capa que cubre todas las áreas de las sociedades contemporáneas. De esta manera, si
queremos comprender las técnicas contemporáneas de control, comunicación,
representación, simulación, análisis, toma de decisiones, memoria, visión, escritura e
interacción, nuestro análisis no puede estar completo hasta que consideremos esta capa
de software. Esto significa que todas las disciplinas que tienen qué ver con la cultura y la
sociedad contemporánea (arquitectura, diseño, crítica de arte, sociología, ciencias
políticas, humanidades, ciencia y tecnología, y demás) deben explicar el rol del software y
sus efectos en cualesquiera que sean sus temas de investigación.
Al mismo tiempo, el trabajo existente en estudios del software demuestra que si nos
enfocamos en el software mismo, necesitamos nuevas metodologías. O sea, es de gran
ayuda practicar lo que uno escribe. No es por casualidad que los intelectuales que hasta
ahora han escrito de forma sistemática sobre el software en la sociedad y la cultura hayan
sido programadores o que hayan estado involucrados en proyectos y prácticas culturales
que requieren escribir y enseñar software. Entre ellos: Ian Bogost, Jay Bolter, Matthew
Fuller, Alexander Galloway, Matthew Kirschenbaum, Florian Cramer, Wendy Chun, Bruno
Latour, Geert Lovink, Peter Lunenfeld, Adrian Mackenzie, Paul D. Miller, William J. Mitchell,
Nick Montfort, Janet Murray, Katie Salen, Bruce Sterling, Noah Wardrip-Fruin, Eric
Zimmerman. Por el contrario, los académicos sin esta experiencia técnica o
involucramiento no han discutido el software en sus sabias, influyentes y teóricas
reflexiones sobre medios y tecnologías modernas. Por ejemplo: Manuel Castells, Sean
Cubitt, Oliver Grau, Katherine Hales, Mark Hansen, Paul Virilio, Siegfried Zielinski.
En los 2000’s, el número de estudiantes en arte de medios, diseño, arquitectura y
humanidades que usan la programación en sus trabajos ha crecido substancialmente, por

18
lo menos respecto de 1999 cuando mencioné por primera vez “estudios del software” en
El lenguaje de los nuevos medios. Fuera de las industrias culturales y académicas, muchas
personas están también escribiendo software hoy en día. En gran medida, esto es
resultado de los nuevos lenguajes de código y programación como Processing, PHP, Perl y
ActionScript. Otro factor importante ha sido la publicación de las API que las empresas
Web 2.0 iniciaron a mediados de los 2000’s. Una API, acrónimo de Application
Programming Interface, es un código que permite a otros programas computacionales
acceder a los servicios ofrecidos por una aplicación. Por ejemplo, las personas pueden
usar el API de Google Maps para incrustar un mapa completo en sus páginas Web. Estos
lenguajes y API’s no hicieron forzosamente más fácil la programación, más bien la hicieron
más eficiente. Imagina que si un joven diseñador puede hacer un diseño interesante con
tan sólo una docena de líneas de código escritas en Processing, en comparación con un
largo programa en Java, seguramente será más atractivo iniciarse en la programación. De
igual forma, si sólo unas líneas de JavaScript te permiten integrar toda la funcionalidad de
Google Maps en tu sitio, esto es una gran motivación para seguir adelante con JavaScript.
Una última razón por la cuál muchas personas hoy escriben software es el surgimiento del
mercado masivo de aplicaciones para dispositivos móviles, que a diferencia de las
computadoras convencionales, no está dominado por unas cuantas compañías. Según
reportes informales, a principios del 2012, un millón de programadores estaban
desarrollando apps para la plataforma iOS (iPad e iPhone) solamente.
En 2006, Martin LaMonica hacía una reseña de otros ejemplos de nuevas tecnologías que
permiten a personas con poca o nula experiencia en programación desarrollar su propio
software (como Ning). En este artículo, LaMonica hablaba de la posibilidad a futuro de
“una larga cola de apps” 12. Pocos años después, esto fue justamente lo que sucedió. En
julio de 2012, 500 mil apps estaban disponibles desde la App Store de Apple13 y más de
600 mil apps Android en Google Play14.
12 Martin LaMonica, “The do-it-yourself Web emerges,” CNET News, July 31, 2006,
http://www.news.com/The-do-it-yourself-Web-emerges/2100-1032_3-6099965.html>, March 23,
2008.
13 http://www.mobilestatistics.com/mobile-statistics, July 30, 2012.
14 http://play.google.com/about/apps/, July 30, 2012.

19
A pesar de estas sorprendentes cifras, sigue viva la brecha entre personas que saben
programar y las que no. Es evidente que las tecnologías para consumidores que permiten
capturar y editar medios son mucho más fáciles de usar que cualquier lenguaje de
programación, por más alto nivel que tenga. Pero esto no tiene que ser así siempre.
Considera, por ejemplo, lo que tomó montar un estudio de foto y hacer fotografías en 1850
contra el hecho de oprimir un botón de una cámara digital o teléfono portátil en los
2000’s. Queda claro, estamos muy lejos de lograr esa simplicidad en la programación.
Pero no veo ninguna razón lógica para pensar que no llegaremos algún día.
Por ahora, el número de personas que saben programar sigue creciendo. Aunque estamos
lejos de una verdadera “larga cola” de software, su desarrollo se está democratizando
cada vez más. Estamos en el momento adecuado para empezar a pensar teóricamente
sobre el software que está dando forma a nuestra cultura y cómo, a su vez, es modelado
por la cultura. La era de los “estudios del software” ha llegado.
Software cultural
El teórico literario y de medios alemán Friedrich Kittler escribió que los alumnos de hoy
deben conocer por lo menos dos lenguajes de software, sólo así “serán capaces de hablar
acerca de la “cultura” 15. El mismo Kittler programaba en lenguaje ensamblador, lo que
probablemente determinó su desconfianza de las interfaces gráficas de usuario que usa el
software de aplicaciones. Con un clásico gesto modernista, Kittler argumentaba que era
necesario poner atención en la “esencia” de la computadora, que para él eran los
fundamentos lógico-matemáticos así como sus primeros desarrollos, caracterizados por
herramientas tales como los lenguajes ensambladores.
El presente libro está determinado en gran parte por mi historia personal con las
computadoras como programador, animador y diseñador, artista de medios y profesor.
Este perfil práctico empieza en los 1980’s, que fue la década de la programación
15 Friedrich Kittler, 'Technologies of Writing/Rewriting Technology' Auseinander 1, no.3 (Berlin,
1995), citado en Michael Truscello, “The Birth of Software Studies: Lev Manovich and Digital
Materialism,” Film-Philosophy 7, no. 55 (Diciembre, 2003), http://www.film-philosophy.com/vol7-
2003/n55truscello.html.

20
procedural (Pascal) y no del lenguaje ensamblador. También fue la década que vio la
introducción de las computadoras personales, de sus primeros impactos en la industria
editorial y los hipertextos empezaban a ser analizados por académicos literarios. De hecho,
yo llegué a Nueva York de Moscú en 1981, que fue cuando IBM lanzó su primera PC. Mi
primera experiencia con las gráficas computacionales fue entre 1983 y 1984 en una Apple
II. En 1984, vi la primera interfaz gráfica de usuario exitosa en la Apple Macintosh. El
mismo año obtuve un empleo en una de las primeras compañías de animación digital
(Digital Effects) en donde aprendí a programar modelos y animaciones digitales 3D. En
1986 escribía programas que procesaban fotos para que parecieran pinturas. En enero de
1987, Adobe Systems lanzó Illustrator, seguido de Photoshop en 1989. En este año,
James Cameron realizó The Abyss, pionera en el uso de CGI para crear el primer personaje
virtual complejo. En Navidad de 1990, Tim Berners-Lee ya había creado todos los
componentes del World Wide Web, tal como existe hoy: servidores, páginas y navegadores
Web.
En tan sólo una década, la computadora dejó de ser una tecnología invisible para la
cultura y se volvió su nuevo motor creativo. Por supuesto, el progreso del hardware y la ley
de Moore ayudaron en esta revolución, pero fue más crucial la llegada de software hecho
para no especialistas: las nuevas interfaces gráficas de usuario (GUI), el procesamiento de
texto, dibujo, pintura, modelado 3D, animación, composición musical, manejo de la
información, creación de multimedios e hipermedios (HyperCard, Director) y ambientes con
redes de información (World Wide Web). Con este software que era fácil de usar, el
escenario estaba puesto para la década de los 1990’s, cuando la mayoría de las industrias
culturales se movieron al software: diseño gráfico, arquitectura, diseño de producto, diseño
de espacios, cinematografía, animación, diseño de medios, música, educación superior y
administración cultural.
Aunque aprendí a programar en 1975, cuando iba a la secundaria en Moscú, mi interés
por los estudios del software empezó con el software que usaba GUI. Personalmente,
pienso que debemos estudiar el software los más ampliamente posible. Es decir, no sólo
se trata de considerar el software “visible” para los consumidores sino también aquel que
corre todos los sistemas y procesos de la sociedad contemporánea. Ya he dicho que los
Estudios del Software se ocupan de todo tipo de software pero debido a que yo no tengo

21
experiencia escribiendo software de logística o de automatización industrial, no abordaré
esos temas. Mi preocupación está en ese tipo particular de software que uso y enseño en
mi vida profesional y al que llamo software cultural.
El término “software cultural” ya fue empleado previamente de forma metafórica (consulta
J.M. Balkin, Cultural Software: A Theory of Ideology, 2003), pero aquí lo voy a usar de
forma literal para referirme al tipo que soporta acciones asociadas generalmente con la
“cultura”. Estas acciones culturales pueden ser divididas en ciertas categorías, por
ejemplo:
1) Crear, intercambiar y acceder a artefactos culturales que contienen
representaciones, ideas, creencias y valores estéticos. Por ejemplo: editar un
video musical; diseñar un empaque para un producto; leer un periódico en un
teléfono portátil; ver un video YouTube en una TV, un teléfono, una tableta, una
computadora o interfaces de videojuegos.
2) Participar en experiencias culturales interactivas. Por ejemplo: jugar un
videojuego.
3) Crear y compartir información y conocimiento en línea. Por ejemplo, editar un
artículo en Wikipedia o añadir lugares en Google Earth.
4) Comunicarse con otras personas. Por ejemplo: e-mail; mensajes instantáneos; voz
por Internet; chat de texto y video; redes sociales (muro, toques, eventos,
etiquetas, lugares, anotaciones, etc.)
5) Participar en la ecología de información en línea mediante preferencias y
metadatos. Por ejemplo: usar Google; hacer clic en botones como “+1” en
Google+ o “Me gusta” en Facebook; etiquetar imágenes en Flickr.
6) Desarrollar herramientas y servicios de software que hagan posible todas estas
actividades. Por ejemplo: programar una librería para Processing que permita
enviar y recibir datos por Internet16; escribir un plug-in para Photoshop; crear un
nuevo tema en WordPress.
16 http://www.processing.org/reference/libraries/, Julio 7, 2011.

22
Técnicamente, el software cultural puede ser implementado de diferentes maneras. Entre
las más populares (conocidas en la industria computacional como “arquitectura de
software”) están las aplicaciones independientes (“stand-alone”), que funcionan en los
aparatos y dispositivos; las aplicaciones distribuidas, que un cliente corre en un aparato
que se comunica con un servidor remoto; y las redes uno a uno (“peer-to-peer”), en donde
cada computadora se vuelve cliente y servidor. Si todo esto no te suena nada familiar, no
te preocupes. Todo lo que hay que entender es que el software cultural abarca una amplia
gama de productos y servidores, más allá de Word, Photoshop, Firefox y otros software de
aplicaciones. Por ejemplo: aplicaciones profesionales para la producción y post-producción
de cine y efectos visuales que necesitan hardware especial que no todas las laptop tienen,
como Smoke, Flame y Lustre de Autodesk17; apps para el gran público como iMovie;
medios sociales y servicios de redes sociales como Facebook y Vimeo. Para este último
caso, el software incluye múltiples programas y bases de datos que corren en el servidor
de la compañía (por ejemplo, en 2007, Google operaba con más de un millón de
servidores alrededor del mundo, según aproximaciones18) y sitios Web o apps para enviar
mails, chat, subir videos, publicar texto (posts y comentarios).
Aplicaciones de medios
Veamos con detalle algunos de los tipos de software que enlisté arriba.
1) La primer categoría es software de aplicaciones para acceder, crear, distribuir y
gestionar (o publicar, compartir y recrear) contenido de medios. Los ejemplos son
Microsoft Word, PowerPoint, Photoshop, Illustrator, Final Cut, After Effects, Firefox, Internet
Explorer, Blogger, Wordpress, Gmail, y varios editores HTML. Esta categoría es el núcleo de
este libro. En la industria se refieren a este software con términos como “producción de
medios” o “edición de medios”. Por mi parte, me voy a referir a esta categoría simplemente
como software de medios.
17 http://usa.autodesk.com/flame/, Julio 7, 2011.
18 Panda Search & Social, “Google: one million servers and counting",
http://www.pandia.com/sew/481-gartner.html.

23
Daré por hecho que como todos usamos programas de aplicaciones, o “apps”, ya tenemos
un conocimiento básico de lo que son. Igualmente, también asumo que ya sabemos a qué
se refiere la palabra “contenido” en la cultura digital. Pero sólo para estar seguros, he aquí
un par de maneras para entenderla. Primero, hagamos una lista con varios tipos de
medios creados, compartidos y accedidos con software de medios y con herramientas en
medios y sitios sociales. Esta lista contiene: textos, imágenes, videos digitales,
animaciones, objetos y ambientes 3D, mapas, así como diversas combinaciones entre
ellos. La segunda manera para definir “contenido” sería con una lista por géneros, por
ejemplo: páginas Web, “tweets”, juegos casuales, juegos multiusarios en línea, videos
hechos por usuarios, resultados de motores de búsqueda, muros Facebook, direcciones
URL, ubicaciones en un mapa, enlaces compartidos, etc.
La cultura digital tiende a hacer modular el contenido, es decir, a dejar que los usuarios
creen, distribuyan y re-usen partes de “contenido” a diferentes escalas (animaciones en
secuencia como fondo de un video, objetos 3D para crear animaciones 3D complejas,
partes de código para usarse en sitios Web y blogs, etc.) Esta modularidad va de la mano
con el principio fundamental de la ingeniería de software moderna de diseñar programas a
partir de pequeñas partes reutilizables, llamadas funciones o procedimientos.
Entre los años 1970’s y los 2000’s, los programas de aplicaciones para la edición de
medios habían sido diseñados para correr en la computadora de un usuario
(microcomputadoras, PC, estaciones de trabajo científicas, laptops). En los siguientes cinco
años, las compañías crearon gradualmente más y más versiones capaces de funcionar en
la “nube”. Algunos de estos programas están disponibles desde las páginas oficiales, pero
otros están integrados en servicios de hospedaje de medios, por ejemplo Photobucket, un
editor de imágenes y video. Muchas aplicaciones están implementadas como clientes que
corren en teléfonos móviles (mapas en el iPhone), tabletas y plataformas TV, y que se
comunican con servidores y sitios Web. Ejemplos de esas plataformas son el iOS de Apple,
Android de Google, y Smart TV de LG. Incluso hay otras apps que funcionan de forma
independiente en los aparatos móviles, como Adobe Photoshop Touch19. Al momento de
19 http://www.adobe.com/products/mobileapps/, marzo 12, 2012.

24
escribir este libro, todavía son limitadas las posibilidades de edición de medios de las apps
para móviles, pero quizá esta situación cambié para al momento de su impresión.
El desarrollo de software para plataformas móviles ha dado una importancia creciente a
ciertos tipos de aplicaciones de medios (y sus correspondientes actividades culturales)
como los “publicadores de medios” (apps diseñadas para subir contenido a sitios de
intercambio de medios). Para ponerlo en otros términos, la gestión de contenidos
mediáticos (por ejemplo organizar fotos en Picasa) y también su “meta-gestión” (es decir,
gestionar los sistemas que gestionan el contenido, como lo sería organizar un flujo de
blogs) se ha vuelto tan importante para la vida cultural como la creación de contenido.
Este es un libro sobre software de medios: su historia conceptual; las formas en que
redefinió las prácticas para el diseño de medios; la estética de los medios creados; y, la
idea de los creadores y usuarios acerca de los “medios”. ¿Cómo ubicar el software de
medios en otras categorías y cómo descomponerlo en subcategorías? Empecemos con
nuestra definición ya mencionada antes. El software de medios son programas que se
usan para crear e interactuar con ambientes y objetos de los medios. Es un subgrupo de la
categoría “software de aplicaciones” (un término que está cambiando debido a la
suplantación de las aplicaciones de escritorio por aplicaciones móviles y basadas en Web.
El software de medios permite la creación, publicación, acceso, intercambio y recreación
de diferentes tipos de medios como imágenes, secuencias animadas, modelos 3D,
personajes, espacios, textos, mapas, elementos interactivos, así como varios proyectos y
servicios que usan estos elementos. Estos proyectos puede que no sean interactivos,
como los diseños 2D, los gráficos animados, las tomas de las películas, o que sí lo sean,
como las superficies de medios u otras instalaciones. Los servicios en línea son por
definición interactivos (sitios Web, blogs, redes sociales, juegos, wikis, tiendas electrónicas
como iTunes Store o Google Play, etc.). Quizá el usuario no siempre pueda crear contenido,
pero sí puede navegarlo e interactuar con él.
Debido a que la industria cultural de hoy, global y multimillonaria, está basada en software
de medios, es interesante notar que no hay un consenso para su clasificación. El artículo
de Wikipedia sobre “software de aplicación” incluye las categorías de “software de
desarrollo de medios” y “software de acceso a contenidos” (divididos en navegadores Web,

25
lectores de medios y aplicaciones para presentaciones) 20. Esto puede ser útil pero no del
todo exacto, por ejemplo casi todos los software de lectura incluyen también funciones de
edición. SeaMonkey, un navegador de la Fundación Mozilla incluye un editor HTML21;
QuickTime Player puede ser usado para cortar y pegar partes de un video; iPhoto permite
realizar operaciones de edición de imágenes. Siguiendo la misma línea, el software de
“desarrollo de medios” (o de “creación de contenidos”) como Word o PowerPoint es en
muchos casos el mismo para hacer y acceder al contenido. Esta coexistencia de funciones
de autor y lector es una característica distintiva del software cultural. Si visitamos el sitio
Web de las compañías que producen estos software, como Adobe o Autodesk, veremos
que catalogan sus productos de acuerdo al mercado (Web, difusión masiva, arquitectura,
etc.) o de acuerdo a subcategorías como “profesionales” y “consumidores”. Todo esto
puede ser satisfactorio superficialmente, pero es una razón más para usar nuestras
herramientas teóricas en el estudio del software de medios.
3-4) Aquí nos enfocaremos en las aplicaciones de medios hechas para crear y acceder a
los “contenidos” (es decir, los artefactos de los medios), no obstante el software cultural
también abarca herramientas y servicios diseñados específicamente para la comunicación
e intercambio de información y conocimiento, por ejemplo el “software social”. Entre los
casos, podemos citar los motores de búsqueda, navegadores Web, editores de blog,
aplicaciones de e-mail, aplicaciones de mensajes instantáneos, wikis, enlaces
compartidos, redes sociales, mundos virtuales y, predicción de mercados. Entre los
nombres populares están los productos Google (Google Search, Gmail, Google Maps,
Google+, etc.), Skype, MediaWiki o Blogger. Sin embargo, desde finales de los 2000s,
numerosas aplicaciones empezaron a incluir funciones de e-mail, publicación y chat
(comúnmente a través del menú “Compartir”). En cierto modo, todo el software se volvió
software social.
Claro está que las personas no comparten todo en línea, por lo menos aún no y no con
todos. Por eso, es necesario incluir a las herramientas software para el manejo personal
de información, como los administradores de proyectos, aplicaciones de bases de datos y
20 http://en.wikipedia.org/wiki/Application_software.
21 http://www.seamonkey-project.org/.

26
los simples editores de textos o aplicaciones de anotaciones que incluye cualquier aparato
computacional a la venta.
Ésta, y todas las demás categorías de software, cambian con el tiempo. Por ejemplo, en los
años 2000’s la frontera entre “información personal” e “información pública” fue
reconfigurada a medida que la gente empezó a publicar sus medios en sitios de
intercambio y a comunicarse con otros por medio de las redes sociales.
De hecho, la verdadera razón detrás de la existencia de los medios sociales, de los
servicios de redes sociales y del hospedaje Web es borrar esta frontera cada vez más. Por
medio de incentivar la participación de los usuarios en sus sitios, estos servicios pueden
vender más anuncios a más personas, y asegurar el crecimiento constante de su base de
datos (si varios de tus amigos usan un servicio en particular, en donde discuten o
comparten información y medios, es más probable que tú también te unas a ese servicio).
A medida que estos servicios empezaron a ofrecer cada vez más variadas y sofisticadas
herramientas para editar medios y manejar la información, junto con sus funciones
originales de hospedaje y comunicación, también empezaron a borrar otro tipo de frontera,
aquella entre programas de aplicación, sistemas operativos y datos. Facebook, en
particular, fue muy agresivo al definirse como una “plataforma social” que podía
reemplazar varios programas y servicios independientes.
Hasta la llegada de medios sociales y la proliferación de plataformas móviles de medios,
se podían estudiar la producción, la diseminación y el consumo de los medios como
procesos separados. De la misma manera, podíamos distinguir entre herramientas de
producción, tecnologías de difusión y dispositivos de acceso y plataformas (por ejemplo, el
estudio de TV, las cámaras, la iluminación, la máquinas editoras, los sistemas de
transmisión, los televisores como aparatos de visión). Los medios sociales, y la
computación en la nube en general, borran estas fronteras (sobretodo cuando se trata de
contenidos hechos por los usuarios) pero al mismo tiempo imponen nuevas (clienteservidor,
acceso abierto-comercial). El reto para los Estudios del Software es poder
emplear términos como “contenido” y “software de aplicaciones” tomando en cuenta que
sus definiciones están en constante reconfiguración.

27
Los ambientes de programación también pueden ser considerados como software cultural.
Piensa en aplicaciones de desarrollo como Dreamweaver o Flash, que permiten la creación
de medios interactivos con sus propias funcionalidades pero que comúnmente exigen
también un poco de programación y scripting. No sólo eso, las interfaces de medios
(íconos, carpetas, sonidos, animaciones, superficies que vibran, campos de entrada del
usuario) son también software cultural, ya que median las interacciones entre personas
con medios y con otras personas. Me detendré aquí por el momento, pero queda claro que
la lista puede ser extendida hacía más categorías de software.
La categoría de las interfaces tiene especial importancia en este libro. Me interesa cómo el
software se muestra a los usuarios, es decir, qué funciones ofrece para crear, compartir,
reusar, mezclar, gestionar y comunicar contenido; qué interfaces usa para presentar
dichas funciones; qué supuestos y qué modelos tiene del usuario; sus necesidades; y,
cómo la sociedad está codificada en estas funciones y en el diseño de la interfaz.
Estas funciones están incluidas dentro de las aplicaciones en forma de comandos y
herramientas. Éstas definen lo que puedes hacer y lo que no. Esto es relativamente claro,
pero quisiera hacer un comentario al respecto para evitar confusiones. Muchas personas
todavía piensan que los aparatos computacionales contemporáneos usan una Interfaz
Gráfica de Usuario (GUI). En realidad, las GUI originales de los años 1980’s (íconos,
carpetas, menús) han sido ampliadas gradualmente para incluir otros medios y sentidos
(sonidos, animaciones, vibraciones, comandos de voz, gestos multi-toque, etc.). De aquí
que el término “interfaz de medio” (usado en la industria) parezca más preciso para
describir cómo funcionan nuestras interfaces actualmente. Aquí podemos incluir interfaces
de sistemas operativos, como Windows o Mac OS, Android o iOS; de consolas de
videojuegos, teléfonos celulares, tiendas interactivas22 o instalaciones de museos23, que
emplean todo tipo de medios, además de los gráficos, para comunicar con los usuarios.
22 Observa el proyecto Nanika para Nokia y Diesel, http://www.nanikawa.com/; en la Audi City de
Londres, abierto en 2012.
23 Por ejemplo, ve la instalación interactiva Nobel Chamber, Nobel Field, and Nobel Eletronic Wall
Papers en el Nobel Peace Center de Oslo, http://www.nobelpeacecenter.org/en/exhibitions/peaceprize-
laureates/.

28
En este momento, debo hacer un comentario sobre las categorías “medios-contenidos” vs.
“datos-conocimiento”, usadas para organizar mi lista de tipos de software cultural. Así
como sucederá con otras categorías que usaré en este libro, pienso en ellas como dos
partes de una misma dimensión, y no en cajas delimitadas para situar éste o aquel
elemento. Una película es un buen ejemplo para el primer binomio. Una hoja de cálculo
Excel lo puede ser para el segundo. Pero veamos el siguiente ejemplo. Si hago una
visualización de la información contenida en una hoja de cálculo, esta visualización podría
estar en ambas distinciones. Siguen siendo “datos”, pero están representados de una
forma tal que nos permiten llegar a ideas y “conocimiento”, pero también se vuelve una
especie de medio visual que es atractiva a nuestros sentidos, así como las fotografías y las
pinturas.
La razón por la cuál la sociedad tiende a situar estos dos juegos de nociones en oposición
está ligada a la historia de los medios y de las industrias de información. Los medios
modernos son el resultado de las tecnologías y de las instituciones que se desarrollaron
entre la segunda parte del siglo XVIII y la primera del XX. Periódicos masivos, editoriales de
revistas y libros, fotografía, cine, radio, televisión e industria de la grabación. “Datosinformación”
viene de diversos campos profesionales, con sus historias diferentes:
estadística social, economía, administración de negocios, mercados financieros. No es
hasta el siglo XXI que los datos dejan el campo profesional y se vuelven del interés de la
sociedad masiva. Los datos se volvieron “sexy” y de moda: gobiernos y ciudades crearon
sus propios portales de datos (data.gov, data.gov.uk); las visualizaciones de datos llegaron
a las exhibiciones de los principales museos (en el MoMA: Design and Elastic Mind en
2008); los “nerds” de las computadoras se convirtieron en los héroes de las películas de
Hollywood (Social Network, 2010); y, Google, Facebook y Flickr empezaron a publicar datos
sobre tu sitio personal y cuenta de intercambio de medios. Por supuesto, como las
operaciones de los software de medios (así como cualquier otro procesamiento informático
de medios para la investigación, el comercio o el arte) sólo son posibles debido a que
representan a los medios como datos (elementos discretos como pixeles o ecuaciones que
definen gráficas vectoriales en archivos tipo EPS), el desarrollo y adopción de software de
medios como tecnología clave, fue un factor importante para la gradual convergencia de
datos y medios.

29
El software abarca muchos otros tipos y tecnologías, y las computadoras y aparatos
móviles realizan otras funciones además de crear y reproducir medios. Y claro que el
software necesita hardware para funcionar, y las redes se han vuelto esenciales para
nuestra cultura digital. Por esto mismo, es posible que mi enfoque en el software de
medios pueda ser incómodo para algunos lectores. No todos usan Photoshop, Flash, Maya
y demás aplicaciones para crear medios. Un gran número de personas trabajan con
medios a través de programas y códigos que ellos mismos escriben o modifican de otras
personas. Estos son los programadores de los sitios y aplicaciones Web. Artistas del
software, científicos computacionales trabajando en nuevos algoritmos, estudiantes
usando Processing y otros lenguajes de alto nivel. Todos ellos pueden preguntarse por qué
favorezco software en forma de productos consumibles (aplicaciones) y no la práctica de la
programación en sí. ¿Y qué pasa con la creciente democratización del desarrollo de
software y la número cada vez mayor de profesionales culturales y estudiantes que saben
programar? ¿No deberíamos esforzarnos por promover la programación en lugar de
explicar las aplicaciones?
La razón de mi elección es mi compromiso con el entendimiento de la cultura dominante
sobre las excepciones. A pesar de que no tenemos cifras exactas, asumo que las personas
que trabajan con medios y que saben programar son pocas en comparación con la armada
de usuarios de aplicaciones. Hoy en día, un típico diseñador gráfico, editor de cine,
diseñador de producto, arquitecto, músico, o un clásico usuario que sube videos a
YouTube, no sabe programar o leer código de software. Cabe señalar que leer y modificar
código HTML es muy diferente que programar. Entonces, si queremos entender cómo el
software ha remodelado los medios, tanto conceptual como prácticamente, tenemos que
ver de cerca las herramientas cotidianas que usan la mayoría de profesionales y amateurs.
Este libro pone énfasis en software de aplicaciones, más que en software basado en Web y
software para móviles. La explicación es que actualmente todavía es necesario tener
computadoras poderosas, en RAM, memoria y rapidez de procesamiento, para crear
medios profesionales. Además, los otros tipos de software están en rápida evolución y es
necesario adoptar una mirada con cierto reparo. Software como Photoshop o Final Cut
cambian de forma aditiva con cada versión.

30
Cualquier definición está condenada a agradar a algunos y disgustar a otros. Así, me
gustaría añadir una objeción a mi definición de “software cultural” (incluyendo de paso el
“software de medios”). El término “cultura” no es reductible ni a medios independientes ni
a “objetos” de diseño que puedan existir como archivos de computadora y/o
programas/códigos de software ejecutables. Más bien incluye símbolos, significados,
valores, lenguaje, hábitos, creencias, ideologías, rituales, religión, códigos de vestido y de
comportamiento, y muchos otros elementos materiales o inmateriales. En consecuencia,
creo que los antropólogos culturales, lingüistas, sociólogos, entre otros humanistas,
pueden estar en desacuerdo con lo que puede parecer una reducción acrítica de
dimensiones en una simple caja de herramientas para crear y jugar con los medios.
Pero tampoco quero decir que “la cultura hoy” es equivalente a un subconjunto de
software de aplicaciones u objetos de medios y las experiencias que pueden ser generadas
con ellos. Lo que sí quiero decir es que, al final del siglo XX, las personas añadieron una
nueva dimensión a todo lo que es considerado “cultura”. Esta dimensión es el software en
general, y el software de aplicaciones para crear y acceder a los contenidos en particular.
Debo decir que estoy usando a propósito la metáfora de “nueva dimensión”. O sea que el
“software cultural” no sólo es un nuevo objeto (sin importar su amplitud e importancia) que
entró en el espacio que llamamos “cultura”. Sería impreciso pensar en el software como
un término más que podemos agregar a la larga lista que incluye música, diseño visual,
espacios de construcción, códigos de vestido, lenguas, comida, culturas de club, normas
corporativas, modos de hablar y usar el cuerpo, etc. A pesar de que efectivamente
podemos estudiar la “cultura del software” (prácticas de programación, valores e
ideologías de los programadores y de las compañías de software, las culturas de Silicon
Valley y de Bangalore, etc.), si sólo nos quedamos en este aspecto nos estamos perdiendo
la verdadera importancia del software. De igual manera que el alfabeto, las matemáticas,
la imprenta, el motor de combustión, la electricidad, los circuitos integrados, el software reajusta
y re-modela todo aquello en donde se implanta (al menos potencialmente). Así como
una nueva dimensión añade una nueva coordenada a cada elemento en el espacio,
“añadir” software a la cultura cambia la identidad de todo aquello que constituye a la
cultura. Al respecto, el software es el ejemplo perfecto a lo que se refería McLuhan con su

31
frase “el mensaje de cualquier medio o tecnología es el cambio de escala, ritmo o patrones
que introduce en los asuntos humanos” 24.
Para resumir. Nuestra sociedad contemporánea puede ser vista como una sociedad del
software, y nuestra cultura puede ser llamada una cultura del software. Esto es debido al
rol preponderante que juega el software en la confección de elementos materiales y de
estructuras inmateriales que, en su conjunto, hacen la “cultura”.
De los documentos a los actos
El uso del software reconfigura las prácticas sociales y culturales más básicas,
haciéndonos repensar los conceptos y teorías que hemos creado para describirlas.
Tomemos como ejemplo la unidad básica moderna, el nuevo “átomo” de la creación,
transmisión y memoria cultural: los “documentos”, es decir, cualquier contenido
almacenado de forma física, que es distribuido mediante copias físicas (libros, películas,
discos) o electrónicas (televisión). En la cultura del software ya no existen los
“documentos”, “trabajos”, “mensajes “o “grabaciones” como en el siglo XX. En lugar de
documentos fijos cuyos contenidos y significados podían estudiados con un análisis de su
estructura y contenido (un procedimiento típico del análisis y teoría cultural del siglo
pasado, desde el Formalismo Ruso hasta el Darwinismo Literario), ahora interactuamos
con “actos dinámicos del software”. Y uso la palabra “acto” porque lo que estamos
experimentando está siendo construido por el software en tiempo real. Ya sea que
exploremos un sitio Web, juguemos un videojuego o usemos una app en un teléfono móvil
para localizar lugares o amigos cercanos, estamos conviviendo con resultados dinámicos
de la computación en tiempo real de nuestro aparato o servidor y ya no con documentos
estáticos con contenido predefinido. Los programas computacionales pueden usar una
variedad de componentes para crear estos procesos: diseño de plantillas, archivos
almacenados en una máquina local, medios almacenados en una base de datos del
24 Marshall McLuhan, Understanding Media: The Extensions of Man (New York: McGraw Hill, 1964),
citado en New Media Reader, editado por Noah Wardrip-Fruin y Nick Montfort (MIT Press, 2003), p.
203.

32
servidor en red, las acciones en tiempo real de un ratón, pantallas táctiles, controles de
juego, movimientos del cuerpo y demás interfaces. En consecuencia, aunque algunos
archivos estáticos estén implicados, la experiencia mediática final construida por el
software no corresponde a un documento guardado en algún soporte. En otras palabras, a
diferencia de las pinturas, los trabajos literarios, las partituras de música, las películas, los
diseños industriales o los edificios, un estudioso no puede consultar únicamente un solo
“archivo” que contenga el contenido de la totalidad del trabajo.
Aún en casos tan simples como ver un documento PDF o abrir una foto en un reproductor
de medios, estamos tratando con “actos del software”, debido a que es el software mismo
quien define las opciones de navegación, edición e intercambio del documento, en lugar
de que sea el documento mismo. Si examinamos un PDF o un JPG de la misma manera en
la que un estudioso del siglo XX estudiaba una novela, una película o un programa de TV
sólo tendremos un acercamiento a la experiencia que se obtiene de interactuar con el
documento vía software, pero no de todo lo demás. Esta experiencia está determinada por
la interfaz y las herramientas que provee el software. Esta es la razón por la cuál el estudio
de las herramientas, interfaces, supuestos, conceptos e historia del software cultural
(incluyendo teorías e inventores que en los 1960’s y 1970’s definieron muchos de estos
conceptos) son esenciales para dar sentido a los medios contemporáneos.
Este giro en la naturaleza de lo que constituye un “documento” de medios nos obliga a
cuestionar teorías culturales ya establecidas que se basan este concepto. Consideremos el
paradigma intelectual que ha dominado el estudio de los medios desde los 1950’s: la
perspectiva de la “transmisión” de la cultura desarrollada en los Estudios de la
Comunicación. Los académicos tomaron el modelo de la transmisión de la información
formulado por Claude Shannon en su artículo A Mathematical Theory of Communication
(1948) 25, y su subsecuente publicación como libro en 1949 por Warren Weaver26, y lo
adaptaron a la comunicación de los medios de masa. Este paradigma describía los medios
25 C.E. Shannon, "A Mathematical Theory of Communication", Bell System Technical Journal, vol.
27, pp. 379–423, 623-656, Julio, Octubre, 1948, http://cm.belllabs.
com/cm/ms/what/shannonday/shannon1948.pdf.
26 Claude E. Shannon & Warren Weaver, The Mathematical Theory of Communication (University of
Illinois Press, 1949).

33
masivos como procesos comunicativos entre creadores/emisores de “mensajes” y las
audiencias que los “recibían”. Según esta corriente, los mensajes no siempre eran
decodificados completamente por cuestiones técnicas (ruido en la transmisión) o
semánticas (incomprensión del significado).
Esta recepción parcial fue considerada como un problema por las teorías clásicas de la
comunicación y las industrias de medios. Por el contrario, Stuart Hall, fundador de los
Estudios Culturales Británicos, argumentó en su influyente artículo “Encoding/decoding”
(1980)27, que este fenómeno era positivo. Hall proponía que las audiencias construyen sus
propios significados con la información que reciben. En lugar de ser una falla de
comunicación, los nuevos significados eran actos de reinterpretación intencional de los
mensajes enviados. Pero ambas corrientes, tanto los estudios culturales como las teorías
clásicas de la comunicación, daban por hecho que el mensaje era algo completo y
definido, sin importar si estaba guardado en un medio físico (una cinta magnética) o si era
creado en tiempo real por el emisor (una emisión de TV). Entonces se asumía que el
receptor leía toda la publicidad, veía toda la película o escuchaba toda una canción y sólo
después interpretaba, malinterpretaba, daba nuevos significados, se lo apropiaba, lo
recreaba, etc.
A pesar de que este supuesto ya fue puesto en tela de juicio desde 1999, con la
introducción de los DVR (grabadores digitales de video) que llevó al fenómeno de cambio
del tiempo, con la llegada de los medios basados en software simplemente ya no aplica.
Las interfaces de las aplicaciones de acceso a medios (como navegadores y buscadores),
la arquitectura del World Wide Web y las interfaces de servicios de medios en línea (que
ponen al alcance un gran número de artefactos de medios para reproducir, comprar o previsualizar,
como Amazon, Google Play, iTunes, Rhapsody, Netflix, etc.) invitan a “navegar”,
a moverse rápida e instantáneamente, de forma horizontal (del resultado de una búsqueda
a otro, de una canción a la otra) y vertical, a través de los artefactos de medios. También
hicieron más fácil el acceso y la salida de un medio en algún momento arbitrario. En otras
palabras, el “mensaje” que el usuario “recibe” no sólo es “construido” dinámicamente
27 Stuart Hall, “Encoding/decoding,” in Culture, Media, Language, editado por Centre for
Contemporary Cultural Studies (London: Hutchinson, 1980).

34
(mediante una interpretación cognitiva) sino también “gestionado” activamente
(definiendo cuál y cómo información recibe).
Se ha vuelto trascendente que cuando el usuario interactúa con una aplicación de
software con contenido de medios, éste no tenga fronteras finitas y definidas. Por ejemplo,
en Google Earth, es probable que el usuario experimente una “Tierra” diferente cada vez
que entra a la aplicación. Puede ser que Google haya actualizado las fotografías satelitales
o que haya añadido nuevas vistas de calles o nuevos edificios en 3D o nuevas capas y
nueva información en capas ya existentes. Además, en cualquier momento un usuario
puede cargar más datos geoespaciales creados por otros usuarios y compañías, ya sea
mediante el comando “Añadir” del menú (en su versión 6.2.1) o abriendo directamente un
archivo KLM. Google Earth es un típico ejemplo de un nuevo tipo de medio heredado del
web. Un “documento” interactivo que no tiene un contenido predefinido. Su contenido
cambia y crece con el tiempo.
En algunos casos puede ser que esto no afecte de manera significativa el resto de los
mensajes comunicados. Por ejemplo, siguiendo con Google Earth, no importa cuáles capas
estén activas o cuál contenido nuevo haya sido añadido por los usuarios, esto no afecta
otras funciones y convenciones, como la representación en modo Proyección de
Perspectiva General (un método de proyección de mapa particular de la cartografía28).
Sin embargo, como los usuarios pueden añadir su propio contenido e información, creando
así proyectos complejos y ricos en medios sobre la geo-información existente, Google Earth
ya no es sólo un “mensaje”. Se trata de una plataforma para que los usuarios construyan
en ella. Aunque exista cierta continuidad con los usuarios que rehacen creativamente los
medios comerciales del siglo XX (pop art, música, video y ficción29), podemos ver que hay
más diferencias que semejanzas.
28 http://en.wikipedia.org/wiki/Google_earth#Technical_specifications, Marzo 14, 2012.
29 Consultar, por ejemplo, Constance Penley, "Feminism, Psychoanalysis, and the Study of Popular
Culture," in Cultural Studies, ed. Lawrence Grossberg (Rutledge, 1992).

35
Este cambio que va de los mensajes a las plataformas estuvo al centro de la
transformaciones del Web entre el 2004 y 2006, que lo llevó a su etapa Web 2.0. En los
1990’s, los sitios web presentaban un contenido particular creado por los usuarios (o sea,
comunicando “mensajes”) pero fueron sustituidos por redes sociales y sitios de medios
sociales en donde se podía compartir, comentar y etiquetar. El artículo de Wikipedia sobre
el Web 2.0 describe estas diferencias de la siguiente manera: “Un sitio Web 2.0 permite
que sus usuarios interactúen y colaboren entre ellos, en un diálogo de medios sociales
como creadores (prosumidores) de contenidos de una comunidad virtual, a diferencia de
los sitios en donde los usuarios (consumidores) están limitados a ver pasivamente el
contenido. Ejemplos de Web 2.0 son sitios de redes sociales, blogs, wikis, intercambio de
video, servicios de hospedaje, aplicaciones Web, mashups y folksonomías” 30. Para seguir
con Google Earth, los usuarios añadieron muchas formas de información con conocimiento
global, entre ellas certificados de comercio justo, datos de Greenpeace y del Observatorio
de los Objetivos de Desarrollo de las Naciones Unidas31. Así como Google Maps, Wikipedia
u algún otro sitio Web 2.0 que permita integrar su contenido en forma de mashup, se trata
de una manera más directa de apreciar el contenido en nuestras propias plataformas.
La gran adopción de servicios Web 2.0 y de herramientas en línea (foros de discusión
sobre software popular, edición colaborativa en Wikipedia, Twitter, etc.) permite identificar
rápidamente omisiones, selecciones, censura y otras formas de “malos comportamientos”
de los editores de software. Esto es otra característica que distingue entre el contenido
distribuido por compañías Web y el de los medios masivos del siglo pasado. Por ejemplo,
cada artículo de Wikipedia sobre un servicio Web 2.0 tiene una sección especial sobre
controversias, críticas o errores.
En muchas casos, la gente puede usar alternativas “open source” a sus equivalentes en
versión comercial y cerrada. El software de código abierto y/o el libre (no todo el software
libre es de código abierto) permite formas de creación adicional para crear, recrear y
compartir tanto contenido como nuevas funciones del software (aunque esto no significa
tampoco que el software open source emplee diferentes supuestos o tecnologías que el
30 http://en.wikipedia.org/wiki/Web_2.0, Marzo 14, 2012.
31 http://en.wikipedia.org/wiki/Google_earth, Marzo 14, 2012.

36
software comercial). Por ejemplo, entre las alternativas a Google Maps y Google Earth
están OpenStreetMap, Geocommons, WorldMap, y demás software libre32. Es interesante
mencionar que las compañías comerciales usan muy seguido datos de esos sistemas
abiertos debido a que contienen más información que ellos mismos. OpenStreetMap, que
a inicios del 2011 tenía 340,000 contribuidores33, es usado por Flickr y Foursquare34. Sólo
basta que un usuario examine el código abierto del software para que entienda sus
supuestos y tecnologías.
Repasemos algunos mecanismos que ayudan a distinguir a los medios interactivos, en red,
basados en software de los documentos de medos del siglo XX: sitios y servicios Web que
cambian y crecen constantemente; variedad de formas de navegación e interacción; la
posibilidad de añadir nuestro contenido y recrearlo a partir de varias fuentes; arquitecturas
para creación y modificación colaborativas; mecanismos para monitorear a los
proveedores. Aún cuando un usuario trabajé con un simple documento de medios
guardado en un solo archivo de computadora (una situación bastante rara en nuestros
días), este documento mediado por la interfaz de software ya tiene una identidad
diferente. La experiencia del usuario está definida parcialmente por el contenido del
archivo y su organización. El usuario tiene libertad de navegar el documento, de elegir la
información que quiere ver y su secuencia. Mientras que los “viejos medios” ya incluían
este acceso aleatorio, las interfaces de los medios basados en software proveen formas
adicionales para buscar medios y seleccionar qué y cómo leerlos.
Por ejemplo, Adobe Acrobat puede mostrar en forma de viñetas todas las páginas de un
documento PDF; Google Earth puede ejecutar rápidamente acercamientos o alejamientos
de la vista actual; las bibliotecas digitales y bases de datos con artículos científicos y
resúmenes (ACM, IEEE Xplore, PubMed, Science Direct, SciVerse Scopus y Web of Science)
muestran artículos con citas al que estamos consultando en ese momento. Lo más
importante es que estas nuevas herramientas no están atadas a los documentos de
medios (como lo es el acceso aleatorio a un libro impreso) o a las máquinas para
32 http://geocommons.com/, http://www.openstreetmap.org, worldmap.harvard.edu.
33 http://en.wikipedia.org/wiki/Counter-mapping#OpenStreetMap, Marzo 27, 2012.
34 http://en.wikipedia.org/wiki/OpenStreetMap#Derivations_of_OpenStreetMap_Data, Marzo 27,
2012.

37
accederlos (como la radio), más bien forman parte de una capa aparte, de la capa del
software. Esta arquitectura de medios permite añadir fácilmente nuevas herramientas de
navegación y gestión sin tener que modificar los documentos mismos. Por ejemplo, con un
simple clic, podemos añadir botones de intercambio en un blog, permitiendo así nuevas
formas de circulación del contenido. Cuando abrimos un documento de texto en el lector
de medios Preview de OS X, puedo resaltar partes, añadir comentarios e hipervínculos,
dibujar objetos y poner diálogos de notas. Photoshop permite guardar las composiciones
en diferentes “capas de ajustes”, sin modificar la imagen original, etc.
¿Por que no existe una historia del software cultural?
“Всякое описание мира сильно отстает от его развития.”
(Traducción del ruso: “Toda descripción del mundo está siempre
muy retrasada respecto de su desarrollo real”).
Тая Катюша, VJ de MTV.ru, 2008.35
Vivimos en una cultura del software, o sea, una cultura en donde la producción,
distribución y recepción de la mayoría de los contenidos están mediadas por software. Sin
embargo, muchos de los creativos profesionales no saben nada de la historia intelectual
del software que usan a diario, ya sea Photoshop, Illustrator, GIMP, Final Cut, After Effects,
Blender, Flame, Maya, MAX o Dreamweaver.
¿De dónde vino la cultura contemporánea del software? ¿de dónde salieron sus metáforas
y técnicas? ¿y porqué se hicieron en un principio? Actualmente, la mayoría de las
compañías más prominentes en la computación y el web son cubiertas por los medios.
Conocemos relativamente bien su historia: Facebook, Google, Apple. Pero esto es sólo la
punta del iceberg. La historia del software para crear y editar medios sigue siendo
desconocida. A pesar de las afirmaciones que dicen que la revolución digital es igual de
importante que la invención de la imprenta, ignoramos cómo la pieza clave de esta
revolución (el software de los medios) fue inventada. Es sorprendente. Las personas en el
35 http://www.mtv.ru/air/vjs/taya/main.wbp, Febrero 21, 2008.

38
negocio de la cultura saben acerca de Guttenberg (imprenta), Brunelleschi (perspectiva),
los hermanos Lumière, Griffith y Einsenstein (cine), Le Corbusier (arquitectura moderna),
Isadora Duncan (danza moderna) y Saul Bass (gráficos animados). Pero todavía en
nuestros días, relativamente pocas personas han oído hablar de J.C. Licklider, Ivan
Sutherland, Ted Nelson, Douglas Engelbart, Alan Kay, y demás colaboradores quienes,
entre 1960 y 1978, convirtieron gradualmente a la computadora en la máquina cultural
que es hoy.
Asombrosamente, la historia del software cultural como tal no existe. Lo que tenemos hoy
son largas biografías de algunas figuras clave y de laboratorios de investigación, como el
Xerox PARC o el Media Lab, pero no existe la síntesis de algo como un árbol genealógico de
las herramientas de medios. Tampoco tenemos estudios detallados que relacionen la
historia del software cultural con la historia de los medios, la teoría de los medios y la
historia de la cultura visual.
Las instituciones de arte moderno (como el MOMA o la TATE, los editores como Phaidon y
Rizzoli, etc.) promueven la historia de su arte moderno. Incluso Hollywood está orgulloso de
su propia historia (las estrellas, los directores, los realizadores, las películas clásicas).
Entonces, ¿cómo se puede entender la negligencia de la historia de la computación
cultural por parte de nuestras instituciones culturales e industrias informáticas? ¿Por qué,
por ejemplo, Silicon Valley no tiene un museo del software cultural? El museo de la Historia
de la Computadora en Mountain View, California, incluye una vasta exhibición permanente
enfocada en hardware, sistemas operativos y lenguajes de programación, pero no en la
historia del software cultural36.
Creo que una de las principales razones tiene que ver con la economía. Cuando al inicio el
arte moderno fue malinterpretado y ridiculizado, eventualmente se volvió una categoría
legítima para las inversiones. De hecho, a mediados de los 2000’s, las pinturas de
numerosos artistas del siglo XX se vendían más caras que los trabajos de muchos artistas
clásicos. Igualmente, Hollywood sigue obteniendo ganancias con películas viejas
reeditadas en nuevos formatos. ¿Qué hay acerca de las TI? ésta no genera recursos con
36 Ver http://www.computerhistory.org.

39
software viejo, y aún así no hace nada para promover su historia. Estamos de acuerdo: les
versiones contemporáneas de Microsoft Word, Adobe Photoshop y Autodesk Autocad,
entre otros, están basadas en versiones anteriores que datan, en varios casos, de los años
1980’s. Si bien es cierto que estas compañías siguen generando ingresos de las patentes
registradas en las primeras versiones, éstas no existen como productos que puedan ser relanzados
actualmente (a diferencia de los videojuegos de esa misma época). Incluso, sería
posible imaginar una compañía de software dedicada exclusivamente a la comercialización
de versiones de software que fueron exitosas pero ya descontinuadas, por ejemplo Aldus
Pagemaker. De hecho, la cultura consumista explota sistemáticamente la nostalgia de los
adultos sobre sus experiencias culturales en años adolescentes. Es sorprendente que esas
primeras versiones no hayan sido aún comercializadas. Yo usaba MacWrite y MacPaint a
mediados de los 1980’s, Photoshop 1.0 y 2.0 a principios de los 1990’s. Esto constituía
parte de mi “genealogía cultural”, de la misma manera que las películas y las obras de arte
que veía al mismo tiempo. Seamos claros, no estoy promoviendo una nueva categoría de
productos comerciales. Si las primeras versiones de software estuvieran disponibles como
simulaciones, creo que despertaría gran interés por el software… de la misma manera que
los videojuegos en emuladores y plataformas móviles impulsaron los estudios de los
juegos (“game studies”).
Muchos teóricos han considerado al software cultural como un tema aparte, distinto a los
“medios sociales”, “redes sociales”, “nuevos medios”, “arte de los medios”, “Internet”,
“interactividad” y “cibercultura”. En consecuencia, no disponemos de una historia
conceptual del software de producción de medios, ni tampoco de investigaciones
sistemáticas de los roles del software en la producción de medios. Por ejemplo, de qué
manera el popular programa After Effects modificó el lenguaje de las imágenes animadas
en los 1990’s? En la misma década, ¿en que medida la adopción de paquetería 3D, por
estudiantes y jóvenes arquitectos, ha influenciado el actual lenguaje de la arquitectura? ¿Y
qué podemos decir de la co-evolución de las herramientas de diseño Web y de la estética
de los sitios Web, del primitivo HTML en 1994 al dinamismo de Flash cinco años después?
Es posible que encuentres algunas referencias a esta cuestiones en artículos y
conferencias pero, hasta donde sé, no hay un estudio del tamaño de un libro. Es común
que obras especializadas en arquitectura, gráficos animados, diseño, y demás artes
aplicadas, aborden rápidamente la importancia de las herramientas computacionales en el

40
acceso a nuevas oportunidades y posibilidades, pero después ya no se profundiza en esta
discusión.
Resumiendo: aún no ha sido desarrollado un estudio sistemático de las conexiones entre
los trabajos en software de medios contemporáneos y los nuevos lenguajes de
comunicación en el diseño y en los medios (diseño gráfico, diseño web, diseño de
productos, gráficos animados, animación y cine). Aunque no se puede hacer toda la tarea
en un libro, espero proveer de modelos generales para tejer dichas relaciones, así como de
un análisis detallado de cómo el software ha redefinido algunas áreas culturales (gráficos
animados y diseño visual).
Mediante un acercamiento a la teoría del software para el diseño de medios, este libro
intenta complementar el trabajo de otros teóricos que han examinado el software para
plataformas de juegos y diseño (Ian Bogost, Nick Montfort, William Huber) así como de
literatura (Noah Wardrip-Fruin, Matthew Kirschenbaum).
En este ámbito, hay otros campos relacionados que están jugando un papel importante,
como los estudios de códigos y de plataformas desarrollados por Mark Marino24, Nick
Montfort y Ian Bogost. Según Marino, con quien concuerdo completamente, estos tres
campos se complementan de la siguiente manera: “los estudios críticos de códigos
computacionales es un campo emergente relacionado con los estudios del software y los
estudios de las plataformas, pero hacen hincapié en el código mismo de un programa, más
que en su interfaz y usabilidad (estudios del software) o su hardware esencial (estudios de
las plataformas)” 37.
Sumario de la narrativa del libro
Entre principios de los 1990’s y mediados de los 2000’s, el software de los medios ha
sustituido casi todas las tecnologías de los medios surgidas en los siglos XIX y XX. Gran
parte de los medios contemporáneos es creado y accedido a través de software cultural,
37 http://chnm2011.thatcamp.org/05/24/session-proposal-critical-codestudies/, julio 14, 2011.

41
pero lo sorprendente es que poca gente conoce su historia. ¿Cuáles fueron las ideas y
motivaciones de las personas que entre los 1960’s y los 1970’s crearon los conceptos y
técnicas que dieron forma a nuestro software cultural? ¿De qué manera cambió nuestro
concepto de “medios” con el paso hacia métodos de producción basados en software en
los 1990’s? ¿De qué manera las interfaces y las herramientas de desarrollo de contenidos
han transformado la estética y los lenguajes visuales que se usan en el diseño y medios
contemporáneos? Estas son las preguntas que abordo en este libro.
No es mi objetivo trazar una historia del software cultural en general, o del software para la
producción de medios en particular. Tampoco intento abordar todas las técnicas creativas
que éste software permite en diferentes campos culturales. Más bien, definiré un camino
particular a través de esta historia que nos llevará desde 1960 hasta hoy en día, pasando
por sus momentos más cruciales. A continuación resumo esta narrativa e introduzco
algunos conceptos clave desarrollados en cada parte del libro.
La parte 1 se enfoca en los años 1960’s y 1970’s. Muchos teóricos de los medios han
dedicado gran esfuerzo a comprender las relaciones entre los medios digitales y los
medios viejos, físicos y electrónicos, pero las fuentes originales aún no han sido explotadas
suficientemente (los textos y proyectos de Ivan Sutherland, Douglas Engelbart, Ted Nelson,
Alan Kay, y demás gente que trabajaba en el software cultural en aquella época). ¿Cuáles
fueron las razones detrás de los conceptos y técnicas que permiten a nuestras
computadoras representar o “remediar” otros medios? ¿por qué esta gente ha trabajado
para que la computadora se vuelva sistemáticamente una máquina de creación y
manipulación de medios? La parte 1 puede verse como una revisión al “movimiento del
software cultural”, en palabras de Alan Kay. Como nota al margen, diré que es posible
construir una historia alternativa, más exclusiva, considerando decenas de personas
igualmente brillantes que contribuyeron a dar forma al DNA del software de medios
contemporáneo. Hablo de Bob Taylor, Charles Thacker, John Warnock, de personas que
trabajaban en Xerox PARC en los 1970’s o de personas que construyeron el diseño de la
primera Macintosh38. Sin embargo, como ni siquiera disponemos de un análisis teórico de
38 Puedes consultar historias que documentan la invención de múltiples tecnologías que intervinieron
en el diseño de la Macintosh original en www.folklore.com.

42
cómo las ideas de los más reconocidos cambiaron a los medios, este libro empieza por
ellos y por sus textos.
Lo que observo es que Kay, y otros pioneros del software cultural, pretendían crear un tipo
particular de nuevo medio, en lugar de simular únicamente la apariencia de medios
clásicos. Estos nuevos medios se inspiran de formatos de representación existentes pero
al mismo tiempo añaden otras propiedades inexistentes hasta el momento. La visión de
Kay era que estos medios son expandibles, es decir que los usuarios mismos serían
capaces de agregar fácilmente nuevas propiedades o incluso inventar nuevos medios. Kay
considera las computadoras como el primer meta-medio cuyo contenido es “una amplia
gama de medios existentes y de medios que aún no han sido inventados”.
Los fundamentos necesarios para la existencia de dicho meta-medio ya había sido
establecidos entre los 1960’s y 1970’s. En este periodo la mayoría de los medios
disponibles en formato físico y electrónico ya habían sido sistemáticamente simulados en
software, y varios nuevos medios ya habían sido inventados. Este desarrollo nos lleva
desde los primeros programas de diseño interactivo (por ejemplo Sketchpad, de Ivan
Sutherland en 1962) hasta las aplicaciones comerciales de producción de medios que las
hicieron disponibles a usuarios profesionales y consumidores: AutoCAD (1982), Word
(1984), PageMaker (1985), Alias (1985), Illustrator (1987), Director (1987), Photoshop
(1989), After Effects (1993), y demás.
Pero entonces, ¿qué pasa después? ¿las formulaciones teóricas de Kay elaboradas en
1977 permiten predecir los desarrollos de los siguientes treinta años o existen nuevos
desarrollos que su concepto de “meta-medio” no contempló? Actualmente usamos varios
medios existentes simulados en software junto con otros tipos que no existían. Ambos son
constantemente ampliados con nuevas propiedades. ¿Este proceso de expansión es
aleatorio o sigue ciertos patrones? En otras palabras, ¿cuáles son los mecanismos clave
que permiten el desarrollo de la computadora como meta-medio?
Las partes 2 y 3 están dedicadas a estas cuestiones. En ellas se observan los diferentes
mecanismos que condujeron el desarrollo y expansión de la computadora como metamedio,
con un enfoque en los años 90s, cuando el software de medios fue gradualmente

43
adoptado en todas las áreas de la producción profesional de medios. Empleo tres
conceptos distintos para describir estos desarrollos y las nueva estética de los medios
visuales: hibridación de medios, evolución y remix profundo. La parte 2 hace un análisis
teórico de las segunda fase del desarrollo del meta-medio, ilustrando con ejemplos de
varios géneros de medios digitales. La parte 3 se concentra a detalle en el uso del
software para el diseño visual (gráficos animados y diseño gráfico), analizando las
relaciones entre la nueva estética de las imágenes, las composiciones y las operaciones e
interfaces del software usado para ello, como After Effects.
Mi argumento es que en esta transición (que va de las tecnologías de medios físicos y
electrónicos al software) todas las técnicas y herramientas individuales que antes eran
exclusivas a un medio en particular ahora “convergen” en uno mismo ambiente de
software. Esta convergencia tiene consecuencias fundamentales en el desarrollo cultural
humano y en la evolución de los medios. Se trata de una disrupción y una transformación
de todo el panorama de las tecnologías de medios, de las profesiones creativas que los
usan y del concepto mismo de medio.
Una vez que son simuladas en la computadora, las técnicas que antes eran incompatibles
entre diferentes medios se empiezan a combinar de forma casi infinita, lo que provoca
nuevos medios híbridos, o para usar una metáfora biológica, nuevas “especies de medios”.
Piensa por ejemplo en Google Earth, que combina técnicas de cartografía tradicional junto
con sistemas de información geográficos (GIS), gráficos 3D, animación, software social,
búsquedas y otros elementos y funciones. Para mí, esta habilidad de combinar técnicas
previamente separadas representa un nuevo nivel en la historia de los medios humanos,
de las semiosis humana y de la comunicación humana, sólo posible con la
“softwareización”.
Para referirme a esta nueva etapa uso el concepto de “hibridación”. En su primera etapa,
la mayoría de los medios existentes eran simulados en la computadora y al mismo tiempo
una variedad de nuevos medios sólo posibles con la computadora empezaron a surgir. En
la segunda etapa, estas simulaciones y nuevos medios empiezan a intercambiar
propiedades y técnicas.

44
Para distinguir estos procesos de otras formas más familiares de remix, propongo el
término de “remix profundo”. Un remix es, normalmente, una combinación de contenidos
de un mismo medio (como remix de música) o de algunos cuantos (un video anime musical
combina contenidos del anime con música y video). Sin embargo, los ambientes de
producción de software permiten a los diseñadores hacer un remix no sólo de contenidos
sino también de sus técnicas fundamentales, métodos de trabajo y formas de
representación y expresión.
Hoy en día vemos que la hibridación y el remix profundo están en todas las áreas de la
cultura en donde se usa el software. Personalmente me enfocaré en el diseño visual en
general, y en los gráficos animados en particular. Los gráficos animados son una parte
dinámica de la cultura contemporánea y, hasta donde sé, no han sido analizados
teóricamente a detalle. Los antecedentes de los gráficos animados actuales pueden
hallarse en los 1950’s y 1960’s, en los trabajos de Saul Bass y Pablo Ferro, pero su
crecimiento exponencial se dio en los 1990’s y está directamente relacionado con la
adopción de software para el diseño de imágenes en movimiento, en especial After Effects,
publicado por Adobe en 1993. El remix profundo es esencial para la estética de los
gráficos animados. Esto significa que la gran parte de los proyectos que se hacen por
todas partes del mundo basa sus efectos estéticos en la combinación novedosa de
técnicas y tradiciones de medios (animación, dibujo, tipografía, fotografía, gráficos 3D,
video, etc.). Como parte de mi análisis reviso el típico modo de trabajo (la maneras en que
un proyecto se mueve de una aplicación de software a otra) basado en software de un
estudio de diseño.
La siguiente gran ola de la digitalización de la cultura viene con diferentes tipos de
software (redes sociales, servicios de medios sociales, aplicaciones para aparatos
móviles). Las redes sociales y los medios sociales empiezan lentamente, surgen en 2005-
2006 (Flickr, YouTube) y de ahí avanzan en su expansión. Podemos decir que la revolución
de medios de los 1990’s impactó a los creativos profesionales; la revolución de medios de
los 2000’s nos impactó a todos (cientos de millones de personas que usan Facebook,
Twitter, Firefox, Safari, Google Search and Maps, Flickr, Picasa, Vimeo, Blogger y
numerosas aplicaciones para aparatos móviles).

45
Debido a que aún estamos a la mitad de la difusión de los medios sociales, con algunos
servicios sociales que decaen y otros que ganan adeptos (piensa en lo que sucedió con
MySpace, por ejemplo), y las funciones “sociales” del software en mutación constante, he
decidido que su análisis a detalle es prematuro. Más bien, me enfocaré en los desarrollos
fundamentales que hicieron posible y dieron forma a los medios digitales antes de su
explosión social: las ideas sobre la computadora como máquina para generar y producir
medios en los 1960’s-1970’s; su implementación en aplicaciones de medios en los
1980’s-1990’s; y , las transformaciones en los lenguajes visuales de los medios que le
siguieron.
Para ser más preciso podemos enmarcar esta historia entre 1961 y 1999. En 1961, Ivan
Sutherland, en el MIT, diseñó Sketchpad, que se convirtió en el primer sistema de diseño a
computadora disponible al público. En 1999, After Effects 4.0 introdujo Premiere Import39,
Photoshop 5.5 añadió formas vectoriales40 y Apple mostró la primera versión de Final Cut
Pro41. En resumen, el paradigma actual de interoperabilidad entre herramientas de
producción de medios capaces de crear contenidos profesionales, sin necesidad de otro
hardware además de la computadora, quedó listo. Y mientras las herramientas de medios
profesionales siguen evolucionando, los cambios hasta ese momento fueron
exponenciales. De igual forma, los lenguajes de los medios visuales profesionales creados
con este software no han cambiado significativamente después de su transformación
radical en la segunda parte de los 1990’s.
Para ilustrar esta continuidad, los ejemplos que analizaré serán tomados de los 1990’s y
de los 2000’s. No obstante, cuando hable sobre interfaces y comandos de aplicaciones de
medios, usaré las versiones recientes con el fin de que la discusión sea más relevante
para los usuarios de software. Siguiendo esta línea, también considero las capacidades
sociales en todos los software de medios dedicados a los consumidores que aparecieron al
final de los 2000’s (por ejemplo el menú de Compartir en iPhoto). Pero así como el
mecanismo de la hibridación no está limitado al software profesional de medios y a los
39 http://en.wikipedia.org/wiki/After_Effects#History, Julio 7, 2011.
40 http://en.wikipedia.org/wiki/Adobe_Photoshop_release_history, Julio 7, 2011.
41 http://en.wikipedia.org/wiki/Final_Cut_Pro#History, Julio 7, 2011.

46
medios creados profesionalmente, también incluiré ejemplos de servicios como Google
Earth en donde dicho mecanismo también juega un rol clave en la evolución del software y
servicios de Web social.
Debo hacer un comentario sobre mi elección de ciertas aplicaciones de software de
medios. Decidí enfocarme en las aplicaciones para computadoras que se usan más en la
producción de medios: Photoshop, Illustrator, InDesign, Dreamweaver, After Effects, Final
Cut, Maya, 3ds Max, Word, PowerPoint. Estos programas ejemplifican diferentes
categorías: edición de imágenes, gráficos vectoriales, composición, diseño Web, gráficos
animados, edición de video, modelación y animación 3D, procesamiento de texto y
presentaciones. También haré referencias a navegadores web populares (Firefox, Chrome,
Internet Explorer), herramientas de blog y servicios de publicación (Wordpress, Blogger),
redes sociales (Facebook, Twitter, Google+), servicios de intercambio de medios (Flickr,
Pinterest, YouTube, Vimeo), servicios y clientes e-mail (GMail, Microsfot Outlook), suites de
oficina en línea (Google Docs), y sistemas de información geográfica para consumidores
(Google Earth, Bing Maps). Debido a que me interesa cómo los usuarios interactúan con
los medios, otra categoría esencial de software son los reproductores de medios que
vienen pre-instalados en las nuevas computadores (Windows Media Player, iTunes,
QuickTime) así como los pre-visualizadores de documentos (Acrobat Reader, Mac OS
Preview). A medida que algunos programas y servicios web se vuelven menos populares y
que los nuevos ganan más mercado, quizá esta lista te parezca algo diferente para cuando
leas esto (incluso puede ser que algunas aplicaciones hayan ya migrado completamente
de la computadora al web), pero las categorías seguirán siendo las mismas.
Mi deseo es que mi argumentación sea lo más relevante posible para los diseñadores y
artistas contemporáneos. En ocasiones deberé mencionar superficialmente algunos
programas importantes históricos o no existentes. Entre ellos, anticipo QuarkXPress,
WordPerfect y Macromedia Director. Por suerte, los dos programas que analizaré con
detenimiento, Photoshop en el capítulo 2 y After Effects en el capítulo 5, siguen siendo tan
populares hoy como lo fueron en los 1990’s.
Dejaré de lado otro tipo de sistemas de producción de medios que fueron importantes en
los 1980’s y 1990’s. Durante este periodo las capacidades gráficas de las computadoras

47
personales eran todavía muy limitadas y algunos de estos sistemas corrían en estaciones
de trabajo poderosas para la época, como SGI y otros hardware exclusivos. He aquí
algunos ejemplos en orden cronológico, con la función del sistema, nombre de la
compañía y año de aparición: Paintbox (gráficas para difusión televisiva, Quantel, 1981),
Mirage (procesador de efectos digitales de video en tiempo real, Quantel, 1982)42,
Personal Visualizer (modelación y animación 3D, Wavefront, 1988), Henry y Hal (sistemas
de producción de efectos y composición de gráficos, Quantel, 1992) Inferno y Flame
(capas de video para cine y video, Discreet Logic, 1992).
A mediados de los 1990’s, Flame, junto con la estación de trabajo SGI, costaba 450,000
dólares; Inferno, por su parte, 700,00043. Inferno 5 y Flame 8 fueron anunciados en 2003
con un precio de lista de $571,500 USD y $265,500 USD, respectivamente44. Con estos
precios, dichos sistemas sólo estaban al alcance de estudios de cine y TV o grandes
compañías de efectos de video.
Hoy en día, las áreas más exigentes en la producción de medios, que incluye el manejo de
cantidades masivas de datos como el cine profesional, las animaciones profesionales y los
comerciales de TV, aún está basadas en esos costos sistemas de software. A pesar de que
al final de los 1990’s las compañías empezaron a ofrecer versiones ligeras de sus
programas para PC, Mac y Linux, al momento de escribir estas líneas las versiones más
completas siguen necesitando hardware especial y sus precios siguen siendo elevados.
Por ejemplo, la versión 2010 de Autodesk Flame Premium, un paquete que incluye Smoke,
Flame y Lustre, usados en la producción de video, efectos y corrección de color, estaba
anunciada en 125,000 dólares.
No espero que el lector común de este libro tenga experiencia con estos caros sistemas,
así que no haré referencia a ellos. Sin embargo, una historia estricta de las imágenes en
42 http://en.wikipedia.org/wiki/Quantel_Mirage, Agosto 23, 2012.
43 "Discreet Logic Inc. History," http://www.fundinguniverse.com/company-histories/discreet-logicinc-
history/.
44 Autodesk, “Discreet Delivers inferno 5, flame 8 and flint 8,” Enero 23, 2003,
http://investors.autodesk.com/phoenix.zhtml?c=117861&p=irol-newsArticle&ID=374002.

48
movimiento de los 1980’s y 1990’s (que espero alguien escriba en el futuro)
definitivamente debe hacer una arqueología y genealogía de esos sistemas y sus usos.
Para cerrar esta sección, cabe una última explicación. Algunos lectores encontrarán
aburrido que me concentre en aplicaciones comerciales para la producción y diseño de
medios, en lugar de sus alternativas en software libre. Por ejemplo, yo elegí Photoshop y no
GIMP; Illustrator y no Inkscape. Personalmente, me fascina, apoyo y uso software libre.
Desde 1994 todos mis artículos están disponibles para su descarga gratuita desde mis
sitio web manivich.net. Y cuando en 2007 armé un laboratorio de investigación
(softwarestudies.com) para el desarrollo de técnicas para el análisis y visualización de
grandes colecciones de medios, decidimos seguir la estrategia del software libre, haciendo
de herramientas disponibles gratis y permitiendo que fueran modificadas45.
La razón por la cuál este libro gira en torno al software comercial para el diseño y
producción de medios y no sus equivalentes abiertos es muy sencilla. En casi todas las
áreas de la cultura del software la gente usa aplicaciones libres y servicios Web. Entre ellos
tenemos a los navegadores web, e-mail, redes sociales, apps para dispositivos móviles y
lenguajes de programación y script. Las compañías no cobran por estas aplicaciones
porque obtienen sus ganancias de otras formas (publicidad, costo de funcionalidades
extra, cuotas de membresía, venta de aparatos). Sin embargo, en el caso de las
herramientas profesionales para diseño y producción de medios, el software comercial
sigue dominando. Esto no es necesariamente algo mejor, solamente indica que es usado
por más personas. Como ejercicio, busca “Photoshop” y “Gimp” en Google Insights para
ver que desde 2004 el número de búsquedas del primero es 8 veces mayor que el
segundo. A mi me interesa describir las experiencias del usuario común y las
características de la estética de medios comunes en millones de obras creadas con las
herramientas más usadas, que son todas productos comerciales. Este es el software que
he elegido. Analizo herramientas para el acceso y colaboración de medios, y los productos
más populares, que pueden ser ofrecidos gratis por compañías comerciales (Safari, Google
Earth) y por software libre (Firefox).
45 http://lab.softwarestudies.com/p/software-for-digital-humanities.html.

49

50
PRIMERA PARTE: La invención del software de medios
Capítulo 1. La máquina universal de medios de Alan Kay
Medio:
8.
a. Un tipo específico de técnica artística o forma de expresión
determinada por los materiales usados o los métodos creativos
involucrados: el medio de la litografía.
b. Los materiales usados en una técnica artística específica:
óleo como medio.
American Heritage Dictionary, 4a. edición (Houghton Mifflin,
2000).
“La mejor manera de predecir el futuro es inventándolo”.
Alan Kay.
Apariencia vs. función
Entre su invención a mediados de los 40s y la llegada de la PC a inicios de los 80s, la
computadora digital era usada sobretodo con fines militares, científicos y empresariales
(cálculos y procesamiento de datos). No era interactiva. No estaba diseñada para ser
usada por una sola persona. En pocas palabras, difícilmente podía funcionar para la
creación cultural.
Como resultado de numerosos desarrollos en los 80s y 90s, entre ellos el surgimiento de
la industria de las computadoras personales, la adopción de las interfaces gráficas de
usuario (GUI), la expansión de redes computacionales y del World Wide Web, las
computadoras se movieron hacia la cultura. El software reemplazó muchas herramientas y

51
tecnologías de los creativos profesionales. También ha dado la oportunidad a cientos de
millones de personas de crear, manipular y compartir medios. ¿Pero será posible decir que
esto nos ha llevado a la invención de nuevas formas de cultura? Actualmente, las
compañías de medios están ocupadas inventando libros digitales y televisión interactiva;
los consumidores están felizmente comprando discos y películas en formato digital,
tomando fotos y videos con sus cámaras digitales y teléfonos celulares; los oficinistas
están leyendo documentos PDF que imitan al papel.
O sea, parece que la revolución en términos de producción, distribución y acceso a medios
no ha sido acompañada de una revolución similar en cuanto a la sintaxis y semántica de
los medios. ¿Quién debe ser culpado por esto? ¿Debemos apuntar a los pioneros de la
computación cultural (J.C. Licklider, Ivan Sutherland, Ted Nelson, Douglas Engelbart,
Seymour Paper, Nicholas Negroponte, Alan Kay, entre otros)? ¿O, como Nelson y Kay están
dispuestos a señalar, el problema radica en la forma en que la industria ha implementado
sus ideas?
Antes de culpar a la industria de mala implementación (podemos regresar a este
argumento más tarde, si es necesario), observemos la visión de los inventores de la
computación cultural. Por ejemplo, ¿qué nos dice la persona que guió el desarrollo del
prototipo de la computadora personal moderna (Alan Kay)?
Entre 1970 y 1981, Alan Kay trabajó en Xerox PARC, un centro de investigación creado por
Xerox en Palo Alto, California. Ahí, el equipo Learning Research, dirigido por Kay, se inspiró
del trabajo de Sutherland, Nelson, Engelbart, Licklider, Seymour Papert, entre otros, y
articuló sistemáticamente el paradigma y las tecnologías de la computación de medios
vernaculares que existe en nuestro días46.
46 Kay ha expresado sus ideas en escasos artículos y en gran número de entrevistas y conferencias.
A continuación mis fuentes primarias: Alan Kay y Adele Goldberg, Personal Dynamic Media, IEEE
Computer 10, no. 3 (1977); Alan Kay, “The Early History of Smalltalk,” The 2nd ACM SIGPLAN
Conference on History of Programming Languages (New York: ACM, 1993), 69-95; Alan Kay, “A
Personal Computer for Children of All Ages,” Proceedings of the ACM 1972 National Conference
(Boston, 1972); Alan Kay, Doing with Images Makes Symbols, videocassette (University Video
Communications, 1987), http://archive.org/details/AlanKeyD1987/; Alan Kay, “User Interface: A

52
A pesar de que un grupo selecto de artistas, cineastas, músicos y arquitectos ya estaban
usando computadoras desde los 50s, desarrollando su propio software en colaboración
con ingenieros informáticos de diversos laboratorios (Bell, IBM Watson Research Center,
etc.), la mayoría de sus producciones estaba orientada a producir un tipo particular de
imagen, animación o música, acorde con sus ideas específicas. Además, cada programa
estaba diseñado para correr en un tipo de máquina particular. Así, estos programas no
podían funcionar como herramientas genéricas que pudieran ser transmitidas a otros
usuarios.
Es bien sabido que muchos de los ingredientes clave de nuestras actuales computadoras
personales vinieron de Xerox PARC: la interfaz gráfica de usuario con sus ventanas
sobrepuestas e iconografía, la pantalla de pixeles, los gráficos de colores, la interconexión
vía Ethernet, el mouse, la impresora láser y la impresión WYSIWYG (acrónimo del inglés
“What You See Is What You Get”, que puede traducirse como “lo que ves es lo que
obtienes”). Pero algo que es igualmente importante es que Kay y sus colegas también
desarrollaron aplicaciones para la manipulación y creación de medios usando también una
interfaz gráfica. Entre ellos destacan un procesador de texto, un sistema de archivos, un
programa de dibujo y coloreado, un programa de animación, un programa de creación
musical, etc. Tanto la interfaz gráfica general y los programas de manipulación de medios
estaban escritos en el mismo lenguaje de programación, Smalltalk. Algunas aplicaciones
fueron hechas por miembros del equipo de Kay pero otras más por jóvenes de nivel
secundaria47. Esto iba de la mano con la filosofía de Kay: dar a los usuarios un ambiente
de programación, ejemplos de programas y herramientas genéricas pre-determinadas para
que los usuarios fueran capaces de crear sus propias herramientas creativas.
Cuando Apple lanzó la primera Macintosh, en 1984, acercó la visión de Xerox PARC a los
consumidores (la nueva computadora tenía un precio de 2,495 dólares). La Macintosh
Personal View,” in The Art of Human-Computer Interface Design, ed. Brenda Laurel (Reading, Mass:
Addison-Wesley, 1990), 191-207; David Canfield Smith at al., “Designing the Star user Interface,”
Byte, Número 4 (1982).
47 Alan Kay y Adele Goldberg, “Personal Dynamic Media,” in New Media Reader, editado por Noah
Wardrip-Fruin y Nick Montfort (The MIT Press, 2003), 399.

53
128K incluía un procesador de texto y una aplicación para dibujo (MacWrite y MacDraw,
respectivamente). En pocos años se unieron otros software para diseñar y producir otros
medios: Word, PageMaker y VideoWorks (1985) 48, SoundEdit (1986), Freehand e
Illustrator (1987), Photoshop (1990), Premiere (1991), After Effects (1993), y demás. Para
inicios de los 90s ya existían funcionalidades similares en computadoras con sistema
operativo Microsoft Windows49. Pero ni PC ni Mac eran tan rápidas como para ser una
verdadera competencia a las tecnologías y herramientas tradicionales (a excepción del
procesamiento de texto). Empero, otros sistemas específicamente optimizados para el
procesamiento de medios empezó a reemplazar estas tecnologías tradicionales ya desde
los 80s. Hablamos de las estaciones de trabajo Next, producidas entre 1989 y 1996;
Amiga entre 1985 y 1994; y Paintbox, lanzada en 1981.
Alrededor de 1991 la nueva identidad de la computadora como productor personal de
medios había quedado establecida. Ese año Apple lanzó QuickTime, que dio video a sus
sistemas, y el mismo año James Cameron estrenó Terminator II, que introdujo pioneros
efectos especiales hechos a computadora. La visión del Xerox PARC se había vuelto
realidad, o más bien una parte importante de esta visión que involucraba la conversión de
la computadora en máquina personal para mostrar, crear y manipular contenido en
diferentes medios. Kay y sus colaboradores lograron implementar una variedad de
programas en un solo ambiente, con apariencia y comportamiento consistentes. Aunque
ellos no inventaron todos los programas (ya vimos que las aplicaciones de animación y
dibujo existían desde la segunda parte de los 60s50), sí establecieron un nuevo paradigma
de la computación de medios.
Creo que he expuesto mi punto. La evidencia es irrefutable. Ha sido Alan Kay y sus
colaboradores en PARC quienes se ocuparon de que las computadoras imitaran otros
medios. Mediante el desarrollo de software basado en GUI fáciles de usar para producir
medios, Kay y su equipo convirtieron a la computadora en una máquina para simular
“viejos medios”. Para ponerlo en términos del influyente libro Remediation: understating
48 Videoworks cambió su nombre a Director en 1987.
49 1982: AutoCAD; 1989: Illustrator; 1992: Photoshop, QuarkXPress; 1993: Premiere.
50 Ver: http://sophia.javeriana.edu.co/~ochavarr/computer_graphics_history/historia/.

54
new media (2000), de Jay Bolter y Richard Grusin, este software basado en GUI hizo de la
computadora una “máquina de remediación”, es decir, una máquina que representa
eficazmente medios existentes. Otras tecnologías desarrolladas en PARC como el monitor
de color de bitmaps, la impresora láser y el primer lenguaje de descripción de páginas (que
dio paso a Postscript) fueron también diseñados para que la computadora cumpliera su rol
como máquina de simulación de medios físicos.
Bolter y Grusin definen a la remediación como “la representación de un medio dentro de
otro” 51. Según su ideas, los nuevos medios siempre remedian a los viejos y por eso no
deberíamos esperar que las computadora se comportaran de forma diferente. Esta
perspectiva enfatiza la continuidad entre medios computacionales y otros tipos más
antiguos. En lugar de quedar separados por lógicas diferentes, todos los medios,
incluyendo la computadora, siguen la misma lógica de la remediación. La única diferencia
entre las computadoras y otros medios radica en cómo y qué remedian. Esto lo exponen
Bolter y Grusin en su primer capítulo, “lo nuevo de los medios digitales está en sus
estrategias particulares para remediar a la televisión, el cine, la fotografía y la pintura”. En
otro lugar del mismo capítulo, ofrecen otro argumento sólido que no deja ambigüedades
en su postura: “Lo que decimos es que la remediación es una característica esencial de los
nuevos medios digitales”.
Si hoy tomamos en cuenta todos los medios digitales creados por profesionales y por
consumidores (o sea, fotografías y videos digitales hechos con cámaras de bajo costo y
teléfonos celulares; el contenido de blogs personales; ilustraciones creadas con
Photoshop; películas hechas con AVID, etc.), en su superficie los medios digitales parecen
exactamente iguales a sus predecesores analógicos. Por eso, si nos seguimos limitando a
su nivel superficial, la remediación puede describir precisamente muchos de los medios
computacionales. Pero en lugar de aceptar esta condición como una consecuencia
inevitable de la lógica universal de la remediación, deberíamos preguntarnos porqué
sucede de esta manera. En otras palabras, si los medios computacionales
contemporáneos imitan otros medios, ¿cómo hemos llegado aquí? En las formulaciones
51 Bolter y Grusin, Remediation: Understanding New Media (The MIT Press, 2000).

55
teóricas originales sobre la computación digital de Turing o de Von Neumann no se
especifica nada sobre la imitación de medios como libros, fotos o películas.
Hay un abismo técnico y conceptual enorme que separa a las computadoras del tamaño
de un cuarto completo (usadas por los militares para calcular blancos con armas
antiaéreas y descifrar códigos de comunicación alemanes) de las computadoras
contemporáneas usadas por millones de personas para almacenar y producir sus propios
medios. La identidad contemporánea de la computadora como procesador de medios
tomó 40 años en gestarse, desde 1949 cuando el Laboratorio Lincoln del MIT inició sus
investigaciones en computadoras interactivas hasta 1989 cuando la primera versión de
Photoshop fue lanzada comercialmente. Tomó generaciones de mentes brillantes en
inventar la multitud de conceptos y técnicas que hoy hacen posible que las computadoras
“remedien” otros medios. ¿Cuáles fueron las razones para ello? ¿cuáles eran sus ideas? O
sea, ¿por qué estas personas dedicaron sus vidas a la invención de la última “máquina de
remediación”?
En los años 90s y 2000s, los teóricos de medios han dedicado grandes esfuerzos en tratar
de entender las relaciones entre medios digitales y otros medios físicos y electrónicos,
pero han dejado de lado fuentes importantes (como ya hemos dicho, sobretodo los textos
de Sutherland, Engelbart, Nelson, Kay y otros pioneros de los 60s y 70s). Este libro no
pretende hacer una historia intelectual completa de las invención de la computación de
medios. Por ello, no voy a considerar el pensamiento de todos las figuras centrales de esa
historia (lo cual se trataría de un proyecto de más de un libro). Más bien, me centraré en el
presente y el futuro. Específicamente, quiero explicar algunas de las transformaciones
radicales en lo que son los medios, lo que pueden hacer y cómo podemos usarlos… las
transformaciones que están conectadas con el cambio de las tecnologías de medios al
software. Algunas de estas transformaciones tuvieron lugar en los 90s pero no fueron
suficientemente discutidas en su momento (por ejemplo, el surgimiento de un nuevo
lenguaje de las imágenes en movimiento y del diseño visual en general). Otras no han sido
nombradas aún. Otras más, como el remix y los mashups culturales, son evocadas
constantemente pero el análisis de cómo fueron posibles por la evolución del software de
medios no ha sido abordado.

56
En resumen, quiero explicar qué son los “medios después del software”, o sea, qué pasó
con las técnicas, lenguajes y demás conceptos de los medios del siglo veinte como
resultado de su informatizadas. Precisamente, qué ha pasado con los medios después que
fueron softwareizados. Pero como en el espacio de un libro sólo puedo considerar algunas
de esas técnicas, lenguajes y conceptos, me centraré en aquellos que, en mi opinión, no
han sido discutidos por otros. Para lograrlo, trazaré el camino entre la historia conceptual
de la computación de medios desde los 60s hasta nuestros días.
Para hacer esto más eficiente, en este capítulo echaremos un vistazo más de cerca a uno
de los lugares en donde la identidad de la computadora como “máquina de remediación”
cobró vida, el grupo de investigación de Alan Kay en Xerox PARC. Haremos dos preguntas:
primero, qué es exactamente lo que Kay quería hacer y, segundo, qué hicieron él y sus
colaboradores para lograrlo. Una breve respuesta, que será desarrollada más adelante, es
que Kay quería convertir a las computadoras en un “medio personal dinámico” que
pudiera ser usado para aprender, descubrir y crear artísticamente. Esto fue posible
mediante la simulación sistemática de la mayoría de medios existentes dentro de una
computadora y, al mismo tiempo, añadiendo nuevas funcionalidades a esos medios. Kay y
sus colaboradores también desarrollaron un nuevo tipo de lenguaje de programación que,
al menos en teoría, permitiría a los usuarios crear rápidamente nuevos tipos de medios
usando herramientas genéricas. A todas estas herramientas y medios existentes se les dio
una interfaz de usuario unificada, diseñada para activar múltiples formas de pensamiento
y formas de aprendizaje (kinestésico, icónico, simbólico).
Kay pensó en el “medio dinámico personal” como un nuevo tipo de medio con numerosas
propiedades sin precedentes históricos como, por ejemplo, la habilidad de guardar la
información de los usuarios, simular todos los medios en uno mismo e “involucrar al
aprendiz en una conversación bidireccional” 52 . Estas propiedades permiten nuevas
relaciones entre usuario y medio, ya sea que esté creando, modificando o viendo en su
computadora. Esto es esencial si queremos entender las relaciones entre computadoras y
52 Desde el trabajo de Kay y su grupo en los 70s, ingenieros en computación, hackers y diseñadores
han añadido muchas otras propiedades únicas, por ejemplo podemos mover medios rápidamente
vía Internet y compartirlos con millones usando Flickr, YouTube y demás sitios.

57
medios tradicionales. Dicho de otra forma, puede ser que los medios computacionales
visuales mimeticen otros medios, pero también funcionan de diferentes maneras.
Por ejemplo, pensemos en la foto digital que usualmente imita la apariencia de la foto
tradicional. Para Bolter y Grusin, esto es un ejemplo de cómo los medios digitales
“remedian” a sus predecesores. Pero en lugar de centrarnos únicamente en su apariencia,
pensemos en cómo funciona la foto digital. Si convertimos una foto digital en un objeto
físico (como ilustración de una revista, como póster en la pared, como estampado de una
playera), entonces efectivamente funciona de la misma manera que su predecesor (a
menos que tenga capacidades aumentadas como el catálogo e 2013 de IKEA53). Pero si
dejamos la misma foto en su ambiente computacional nativo, que puede ser una laptop,
un sistema de almacenamiento en red o cualquier otro dispositivo o Internet, puede
funcionar de formas que, en mi opinión, la vuelven radicalmente diferente a su equivalente
analógico. Para usar un término diferente, podemos decir que una foto digital ofrece más
posibilidades que sus antepasados ya que puede ser modificada o combinada
rápidamente con otras imágenes; puede ser publicada al mundo entero y compartida con
otras personas; puede ser integrada en un documento de texto o en un diseño
arquitectónico. Además, podemos automáticamente mejorar su contraste, hacerla más
nítida o incluso, en ocasiones, remover su borrosidad.
Nota que sólo algunas de estas propiedades son específicas a un medio en particular, en
nuestro ejemplo una fotografía digital, es decir un conjunto de pixeles representados como
números. Otras propiedades son comunes a una larga variedad de especies de medios,
por ejemplo, en el estado actual de cultura digital, todos los medios pueden ser añadidos a
un correo electrónico. Hay otros que tienen propiedades aún más generales del ambiente
computacional basado en GUI: la rápida respuesta de la computadora a las acciones de
los usuarios asegura “fluidez entre causa y efecto” 54. Otros más son posibles mediante
protocoles de redes como el TCP/IP que permite a computadoras y aparatos conectarse a
53 Roberto Baldwin, "Ikea’s Augmented Reality Catalog Will Let You Peek Inside Furniture," Julio 20,
2012, http://www.wired.com/gadgetlab/2012/07/ikeas-augmented-reality-catalog-lets-you-peekinside-
the-malm/.
54 Kay y Goldberg, Personal Dynamic Media, 394.

58
una misma red. En resumen, podemos decir que sólo una parte del “nuevo ADN” de la
fotografía digital nació de hecho en la cámara. Muchos otros son el resultado del
paradigma actual de la computación de redes en general.
Antes de explorar en detalle las ideas de Kay quisiera exponer las razones por las cuales
elegí enfocarme en él en lugar de otro. La historia que presentaré podría ser contada
diferente. Sería factible poner el trabajo de Sutherland al centro de la historia de la
computación de medios, o a Engelbart y su Centro de Investigación para el Aumento del
Intelecto Humano que introdujo varias novedades en los 60s, como el hipertexto (aparte
de Nelson), el mouse, las ventanas, el procesador de texto, pantallas de texto y gráficos. O
podemos enfocarnos en el trabajo del Architecture Machine Group del MIT que desde
1967 fue dirigido por Nicholas Negroponte (en 1985 este grupo se convirtió en el MIT
Media Lab). También es necesario recordar que al mismo tiempo que Kay y su grupo de
investigación en PARC develaron detalles de su GUI y programaron varios editores de
medios en Smalltalk (un programa de pintura, uno de ilustración, uno de animación, etc.),
varios artistas, cineastas y arquitectos usaban computadoras desde hace una década y se
montaron grandes exhibiciones de arte digital en los museos más importantes alrededor
del mundo, entre ellos el Museo de Arte Contemporáneo de Londres, el Museo Judío de
Nueva York, el Museo de Arte de Los Ángeles. Y ciertamente, en términos de técnicas
computaciones para la representación visual digital, otros grupos están muy avanzados.
Por ejemplo, a inicios de los 70s, la Universidad de Utah era el principal centro de
investigación de gráficas computacionales. Aquí se producían imágenes 3D superiores a
las imágenes creadas en PARC. Junto a la Universidad de Utah, una compañía llamada
Evans y Sutherland (dirigida por el mismo Ivan Sutherland que enseñaba en la
universidad), usaba gráficas 3D para la simulación de vuelos, siendo pionero en los
“espacios virtuales 3D navegables”.
El trabajo práctico hecho en Xerox PARC permitió ver a la computadora como una máquina
de medios. Otra razón por la cuál elegí enfocarme en Kay fueron sus formulaciones
teóricas que ubican a las computadoras en relación con otros medios y con la historia de
los medios. Vannevar Bush, J.C. Licklider y Douglas Engelbart estaban interesados en el
aumento del intelecto, mientras que a Kay también le interesaba la computadora como
“medio de expresión mediante el dibujo, la pintura, la animación de imágenes y la

59
composición y generación de música” 55. Entonces, si de verdad queremos entender cómo
y por qué la computadoras fueron redefinidas como medios culturales, y cómo los nuevos
medios computacionales son diferentes de otros medios físicos y electrónicos, creo que
Kay nos ofrece la mejor perspectiva teórica.
“La simulación es la noción central del Dynabook”
Alan Kay dio forma a sus ideas en diversos artículos y conferencias. El artículo que
coescribió con una de sus principales colaboradoras en PARC, la ingeniera en computación
Adele Goldberg, es un recursos particularmente útil para comprender los medios
computacionales contemporáneos. Aquí, Kay y Goldberg describen la visión del Learning
Research Group en PARC de la siguiente manera: crear “un medio personal dinámico del
tamaño de una libreta (el Dynabook) que pueda ser adquirido por cualquiera y tener el
poder de manejar virtualmente todas la necesidades de información de su propietario” 56.
Kay y Goldberg piden a sus lectores que imaginen este aparato con “suficiente poder para
sobrepasar tus sentidos de la vista y del oído, suficiente capacidad para almacenar y
acceder a miles de documentos tipo materiales de referencia, poemas, cartas, recetas,
archivos, dibujos, animaciones, partituras musicales, frecuencias de onda, simulaciones
dinámicas y cualquier cosa que necesites recordar y cambiar” 57.
Para mí, ”todo” en la frase es importante: significa que el Dynabook (o ambiente
computacional de medios en general, sin importar el tamaño del aparato en que se
implementa), debe soportar visualización, creación y edición de todos los medios posibles
usados tradicionalmente para la expresión y comunicación humana. De igual forma,
mientras diferentes programas de creación ya existían en diferentes medios, el grupo de
Kay los implementó por primera vez todos juntos en una misma máquina. En otras
palabras, el paradigma de Kay no consistía simplemente en crear un nuevo tipo de medio
basado en la computadora, que pudiera coexistir con otros medios físicos. Más bien, el
objetivo era establecer a la computadora como un paraguas, una plataforma para todo
55 Ibid., 393.
56 Ibid., 393. El énfasis en esta y las demás citas de este artículo es mío (Manovich)
57 Ibid., 394.

60
medio artístico existente (al final de artículo, Kay y Goldberg dan un nombre a este medio:
“metamedio”). Este paradigma cambia nuestro entendimiento sobre lo que son los medios.
Desde el Laocoon; or, On the Limits of Painting and Poetry (1766) de Lessing hasta los
Languages of Art (1968) de Nelson Goodman, el discurso moderno sobre los medios está
basado en el supuesto que diferentes medios tienes distintas propiedades y que, de
hecho, deberían ser entendidos en oposición los unos con otros. El hecho de ubicar todos
los medios en un mismo ambiente computacional no necesariamente borra todas las
diferencias de cómo cada uno puede representar y cómo poder ser percibido, pero sí los
acerca de varias formas. Algunas de esta nuevas conexiones ya eran visibles para Kay y
sus colegas, otras se volvieron evidentes décadas más tarde cuando la nueva lógica de los
medios de PARC se desarrolló más ampliamente y, algunas, quizá aún sean invisibles para
nosotros debido a que no han sido realizadas de forma práctica. Un ejemplo claro de estas
conexiones es el surgimiento de la multimedia como una forma estándar de comunicación:
páginas Web, presentaciones PowerPoint, obras multimedia, mensajes multimedia
móviles, blogs de medios y otras formas de comunicación que combinan unos cuantos
medios. Otro ejemplo es la adopción de convenciones en la interfaz y herramientas que
usamos para trabajar con diferentes tipos de medios, sin importar su origen: cámaras
virtuales, lupas de aumento y, por supuesto, los omnipresentes comandos de copiar, cortar
y pegar. Otro caso es la habilidad de “mapear” un medio en otro usando el software
apropiado para ello (imágenes en sonido, sonido en imágenes, datos cuantitativos en
formas 3D o sonidos, etc.), como se hace en los terrenos del DJ, VJ, cine en tiempo real,
performances y visualización de la información. Con todo, parece que los medios están
activamente tratando de tocarse entre ellos, intercambiando propiedades y dejando que
sus propiedades únicas sean tomadas en préstamo (esta situación es directamente
opuesta al paradigma de medios modernista de principios del singlo veinte, orientado a
descubrir el lenguaje único de cada medio artístico).
Alan Turing definió teóricamente la computadora como una máquina que puede simular
una gran variedad de clases de otras máquinas, y es esta habilidad de simulación la que
es responsable de la proliferación de las computadoras en la sociedad moderna. Pero
como ya lo he mencionado, ni él ni otros inventores de la computadoras digitales consideró
explícitamente que esta simulación pudiera también incluir a los medios. Fue Kay su

61
generación quienes extendieron la idea de simulación de medios (convirtiendo la Máquina
Universal de Turing en una Máquina Universal de Medios).
Al respecto, Kay y Goldberg escriben: “en un sentido muy real, la simulación es la noción
central del Dynabook” 58. Cuando usamos las computadoras para simular algún proceso
del mundo real (el comportamiento de un sistema climático, el procesamiento de
información en el cerebro, la deformación de un choque de automóvil) nuestra
preocupación está en modelar correctamente las características necesarias de este
proceso o sistema. Deseamos ser capaces de probar cómo nuestro modelo se comportaría
en diferentes condiciones con diferentes datos, y la última cosa que quisiéramos que haga
la computadora es introducir características que no especificamos nosotros mismos.
Cuando usamos computadoras como un medio general de simulación, queremos que este
medio sea completamente “transparente”.
¿Pero qué pasa cuando simulamos diferentes medios en una computadora? En este caso,
la aparición de nuevas propiedades de puede ser bienvenida debido a que puede extender
el potencial expresivo y comunicativos de estos medios. Cuando Kay y sus colegas crearon
simulaciones computacionales de medios físicos existentes (herramientas para
representar, crear, editar y ver estos medios), lo que hicieron fue “añadir” apropiadamente
muchas nuevas propiedades. Por ejemplo, en el caso del libro, Kay y Goldberg indican que
“no deber ser tratado como un libro de texto simulado debido a que este es un nuevo
medio con nuevas propiedades. Una búsqueda dinámica puede ser hecha en algún
contexto particular. La naturaleza no-secuencial y el uso de manipulación dinámica
permite contar una historia desde múltiples puntos de vista” 59. Kay y su equipo también
añadieron varias propiedades a la simulación computacional de documentos en papel. Kay
se ha referido a ellas en otro artículo. Su idea no era de únicamente imitar el papel sino de
crear “papel mágico” 60. Por ejemplo, PARC ofreció a los usuarios la capacidad de modificar
la tipografía y de crear nuevas. También implementaron una idea del equipo de Engelbart
en los 60s: la posibilidad de crear diferentes vistas de una misma estructura. Y también
58 Ibid., 399.
59 Ibid., 395.
60 Alan Kay, “User Interface: A Personal View,” p. 199.

62
Engelbart y Nelson ya había “añadido” otra cosa: la habilidad de conectar diferentes
documentos o diferentes partes de un mismo documento mediante hipervínculos (lo que
se conoce actualmente como hipertexto e hipermedios). El grupo de Engelbart también
desarrolló la posibilidad de que múltiples usuarios también trabajaran en un mismo
documento. Y la lista sigue: correo electrónico en 1965, grupos de discusión en 1979, el
World Wide Web en 1991, etc.
Cada una de estas nuevas propiedades tiene amplias consecuencias. Tomemos la
búsqueda de información. A pesar de que la habilidad de buscar en un documento del
tamaño de una hoja puede que no suene como una innovación radical, sí lo es a medida
que el documento se va haciendo grande. Y se vuelve absolutamente crucial si tenemos
una colección de documentos vasta como lo serían todas las páginas del World Wide Web.
Con todo y que los motores de búsqueda están lejos de ser perfectos y que las tecnologías
siguen evolucionando, imagina cómo sería de diferente la cultura Web sin ellos.
O piensa en la capacidad de un número de usuarios para colaborar en un mismo
documento(s) que se encuentra en una misma red. Esto ya era muy usado por las
compañías en los 80s y 90s, pero no fue hasta principios de los 2000s que el público
masivo vio el potencial de esta “adición” a los medios impresos. A través de la colecta de
pequeñas cantidades de labor y el expertise de un gran número de voluntarios, los
proyectos de software social (como Wikipedia), crearon un vasto y dinámico grupo de
conocimientos actualizables que sería imposible de lograrse con formas tradicionales (de
manera mucho menos visible, cada vez que hacemos una búsqueda en el Web, los
algoritmos toman en cuenta cuál de los resultados de búsquedas previas fue más útil para
las personas).
Al estudiar los textos y las presentaciones públicas de la gente que inventó los medios
interactivos computacionales (Sutherland, Engelbart, Nelson, Negroponte, Kay y otros),
queda claro que no llegaron a las nuevas propiedades de los medios digital en el último
momento. Al contrario, ellos sabían que estaban transformando los medios físicos en
nuevos medios. En 1968, Engelbart hizo su famosa demostración en la conferencia Fall
Joint, en San Francisco, ante unas miles de personas, entre ellas ingenieros
computacionales, gente de IBM y otras compañías involucradas con computadoras, y

63
agentes financieros de varias agencias de gobierno61. Aunque Engelbart disponía de 90
minutos, tenía mucho qué mostrar. Durante unos cuantos años previos, su equipo en el
Centro de Investigación para el Aumento del Intelecto había esencialmente desarrollado
ambientes de oficina tal como existen hoy en día (no confundir con ambientes de diseño
de medios, que fueron desarrollados más tarde en PARC). El sistema computacional NLS
incluía un procesador de texto lleno de funcionalidades, documentos conectados mediante
hipertexto, colaboración en línea (dos personas en ubicaciones remotas trabajando en un
mismo documento simultáneamente), manuales de usuario en línea, sistemas de
planeación de proyectos en línea, y otros elementos de lo que se llama actualmente
“trabajo colaborativo soportado por computadora”. El grupo también desarrolló elementos
clave de la interfaz de usuario moderna que después fueron refinados por PARC: un mouse
y múltiples ventanas.
Si ponemos atención en la secuencia del demo, notamos que Engelbart debía asegurarse
que su audiencia fuera capaz de relacionar el nuevo sistema computacional con lo que ya
sabían y conocían, pero su enfoque estaba puesto en las nuevas propiedades de la
simulación de medios, cosas que no existía previamente62. Engelbart dedica el primer
segmento del demo al procesamiento de texto, pero tan pronto como muestra la captura
de texto, así como cortar, pegar, insertar, nombrar y salvar archivos (o sea, la serie de
herramientas que hacen de un procesador de texto una herramienta más versátil que una
máquina de escribir), pasa a una demostración más detallada de propiedades de su
sistema que ningún otro medio tenía antes: “control de vistas”. Como Engelbart indica, el
nuevo medio de escritura puede cambiar entre diferentes vistas de una misma
información a conveniencia del usuario. Un archivo de texto puede ser ordenado de
diferentes formas. También puede ser organizado como una jerarquía con cierto número
de niveles, como en procesadores de índices y sumarios de software contemporáneo como
61 M. Mitchell Waldrop, The Dream Machine: J.C.R. Licklider and the Revolution That Made
Computing Personal (Viking, 2001), p. 287.
62 El video completo de la demostración de Engelbart está disponible en:
http://sloan.stanford.edu/MouseSite/1968Demo.html. Para una descripción detallada de las
funciones de NLS, consultar “NLS User Training Guide,” Stanford Research Institute: Menlo Park,
California), 1997, http://bitsavers.org/pdf/sri/arc/NLS_User_Training_Guide_Apr77.pdf.

64
Microsoft Word. Por ejemplo, una lista de elementos puede ser organizada por categorías,
que a su vez pueden ser desplegadas o replegadas.
Después, Engelbart muestra otro ejemplo del control de vistas que incluso hoy, 45 años
después, aún no está disponible en los software populares de gestión de documentos.
Empieza a hacer una larga lista de “cosas qué hacer” y las organiza por lugares. Enseguida
hace que la computadora muestra dichas locaciones en un gráfico visual (un conjunto de
puntos conectados por líneas). En frente de nuestros ojos, la representación en un medio
cambia a otro (el texto se vuelve un gráfico). Pero eso no es todo, el usuario puede
interactuar con este gráfico para ver diferentes cantidades de información (algo que
ninguna imagen en los medios físicos puede hacer). A medida que Engelbart va haciendo
clic sobre los puntos se van mostrando las partes correspondientes a su lista original (esta
funcionalidad de modificar qué y cuánta información debe mostrar una imagen es un
tópico importante para las aplicaciones de visualización de la información).
Engelbart presenta a continuación “una cadena de vistas” que ya tiene preparada de
antemano. Va cambiando entre estas vistas mediante “ligas”, que parecen los actuales
“hipervínculos” de nuestro Web (aunque tienen una función diferente). En lugar de crear
un camino entre muchos documentos diferentes (como lo haría el Memex de Vannevar
Bush), Engelbart usa las ligas como método para cambiar entre diferentes vistas de un
mismo documento organizado jerárquicamente. Por ejemplo, empieza por un renglón
ubicado en la parte alta de la pantalla, cuando hace clic sobre las palabras, aparece más
información en la parte inferior de la pantalla. Esta nueva información puede contener
ligas hacía otras vistas o incluso hacía información más detallada del texto63.
En lugar de usar a las ligas como una estrategia en el universo textual asociativo y
“horizontal”, ahora nos movemos verticalmente, entre información general e información
detallada. Apropiadamente, en el paradigma de Engelbart no “navegamos” sino
“cambiamos vistas”. Podemos crear muchas vistas diferentes de la misma información y,
63 La descripción a detalle de estas y otras funcionalidades de NLS puede ser consultada en:
Augmentation Research Center, “NLS User Training Guide,” Stanford Research Institute: Menlo
Park, California), 1997, http://bitsavers.org/pdf/sri/arc/NLS_User_Training_Guide_Apr77.pdf

65
claro, movernos entre ellas de diferente manera. Y esto es lo que Engelbart explica
sistemáticamente en la primera parte de su demo. Demuestra que podemos alternar
vistas mediante comandos, o mediante el tecleo de números que corresponden a las
distintas partes de una jerarquía, o haciendo clic sobre una imagen o texto. Cabe
mencionar que en 1967, Ted Nelson formula una idea similar para denominar un tipo
particular de hipertexto que permitiría a un lector “obtener mayor detalle sobre una tema
específico”, que después llamó “stretchtext” 64.
A partir de que la teoría y crítica de nuevos medios surgieron a inicios de los 90s, se ha
escrito mucho sobre interactividad, hipertexto, realidad virtual, ciberespacio, cibercultura,
ciborgs, y demás. Pero personalmente nunca he visto nada sobre “control de vistas” y no
obstante se trata de una de las nuevas técnicas más fundamentales y radicales para
trabajar con las información y los medios disponibles hoy en día. La usamos diario en
numerosas ocasiones. Los “controles de vista”, o la habilidad de movernos entre
diferentes vistas y tipos de vistas, ha sido implementada de varias formas no solo en
procesadores de texto y servidores de correo electrónico, también en “procesadores de
medios” (o sea, software para la producción de medios): AutoCad, Maya, AfterEffects, Final
Cut, Photoshop, InDesign, etc. Pensemos en el software 3D, en donde un modelo puede
ser visto en por lo menos media docena de formas diferentes: sólido, vértices y aristas, etc.
En el caso del software para animación y efectos visuales, en donde los proyectos
típicamente contienen docenas de objetos y cada uno con numerosos parámetros, éstos
pueden ser vistos de forma similar a un sumario de texto. Los usuarios pueden elegir entre
desplegar más o menos información: aquellos parámetros en donde estás trabajando
actualmente o “zoom out” de todo el proyecto. Cuando hacemos estos, las partes de la
composición no sólo se hacen más grandes o pequeñas, también muestran menor o mayor
información automáticamente. Por ejemplo, a cierta escala sólo podemos ver el nombre de
los parámetros pero haciendo un acercamiento se pueden ver una gráfica de cómo se
desarrollan en el tiempo.
64 Ted Nelson, “Stretchtext” (Hypertext Note 8), 1967, http://xanadu.com/XUarchive/htn8.tif.
Proponemos la traducción “texto elástico” para “stretchtext”.

66
Examinemos otro ejemplo: el concepto de hipertexto de Ted Nelson que formuló en los 60s
(de forma independiente y paralela a Engelbart) 65. En su artículo A File Structure for the
Complex, the Changing, and the Indeterminate (1965), Nelson habla sobre las limitantes
del libro, y otros sistemas basados en papel, para organizar información. Aquí presenta su
nuevo concepto:
Sin embargo, con la pantalla computacional y la memoria masiva, se ha vuelto
posible crear un nuevo medio, leíble, que se puede usar para la educación y el
entretenimiento, que permita al lector encontrar su nivel, satisfacer su gusto y
encontrar aquellas partes que le son significativas para su formación y placer.
Permítanme acuñar la palabra “hipertexto” para definir un cuerpo de material
escrito o gráfico, interconectado de manera tan compleja que no sería pertinente
presentarlo o representarlo en papel. (el énfasis en la palabra “hipertexto” es de
Lev Manovich).
“Un nuevo medio, leíble”. Estas palabras aclaran que Nelson no sólo estaba interesado en
mejorar libros y otros documentos de papel. Más bien, quería crear algo nuevo. ¿Pero
acaso el hipertexto de Nelson no era simplemente una extensión de antiguas prácticas
textuales como la exégesis (extensas interpretaciones de escrituras sagradas como la
Biblia, el Corán y el Talmud), las anotaciones y los pies de página? A menudo se propone
que éstos son predecesores históricos del hipertexto pero aún no hemos alcanzado la
visión de Nelson con la manera en que vivimos nuestros hipertextos actualmente, es decir
con el World Wide Web. Noah Wardrip-Fruin ya ha dicho que el Web sólo implementa uno
de los muchos tipos de estructuras de Nelson previó en 1965: las ligas estáticas de una
página a otra66.
65 Douglas C. Engelbart, Augmenting Human Intellect: A Conceptual Framework (Stanford Research
Institute, 1962), http://www.dougengelbart.org/pubs/augment-3906.html. Aunque la
implementación del hipertexto en el sistema NLS de Engelbart estaba muy limitada comparada con
el concepto de Nelson, vale la pena estudiar su visión sobre el aumento del intelecto, casi tan rica
como la de Nelson.
66 Noah Wardrip-Fruin, introducción al artículo de Theodor H. Nelson, “A File Structure for the
Complex, the Changing, and the Indeterminate” (1965), in New Media Reader, p. 133

67
De acuerdo con la implementación actual del Web, muchas gente cree que el hipertexto es
un cuerpo de texto conectado mediante ligas que van de un sentido a otro. Sin embargo, la
noción de “liga” ni siquiera aparece en la definición original de hipertexto. En su lugar,
Nelson habla de una nueva y compleja interconectividad, sin especificar ningún
mecanismo en particular que poder usado para lograrlo. Nelson de hecho presenta un
sistema particular para implementar su visión pero, como él mismo lo dice, puede haber
muchos otros.
“¿Qué tipo de estructuras son posibles de lograr con el hipertexto?”, pregunta Nelson en
una investigación de 1967. Contesta a su pregunta de forma breve pero concisa:
“cualquiera” 67. Después explica: “el texto ordinario puede ser visto como un caso especial
de hipertexto, el más simple y familiar, tal como el espacio tridimensional y el cubo
ordinario son los casos más simples y familiares del hiperespacio y del hipercubo68. EN
2007, Nelson actualizó esta idea de la forma siguiente: “el hipertexto, una palabra que
acuñé hace tiempo, no es una tecnología sino más bien la generalización más completa de
documentos y de literatura69.
Si el hipertexto no significa únicamente “ligas”, tampoco significa exclusivamente “texto”. A
pesar de que su uso más reciente hace referencia a texto conectado, Nelson también
tomaba en cuenta imágenes en su definición original70. En el párrafo siguiente también
acuña los términos hiperpelículas e hipermedia:
Las películas, las grabaciones sonoras, las grabaciones de video son también
secuencias lineales, básicamente por cuestiones mecánicas. Pero éstas también
67 Ted Nelson, “Brief Words on the Hypertext" (Hypertext Note 1), 1967,
http://xanadu.com/XUarchive/htn1.tif
68 Ibid.
69 Ted Nelson, http://transliterature.org/, version TransHum-D23, Julio 17, 2007.
70 Durante el simposio Digital Retroaction 2004, Noah Wardrip-Fruin recalcó que la visión de Nelson
incluía a los hipermedios y no sólo a los hipertextos. Noah Wardrip-Fruin, presentation at Digital
Retroaction: a Research Symposium, UC Santa Barbara, Septiembre 17-19, 2005, http://dcmrg.
english.ucsb.edu/conference/D_Retro/conference.html.

68
pueden ser organizadas como sistemas no lineales (por ejemplo, regiones)
siguiendo propósitos educativos o para mostrar con diferente énfasis… Las
hiperpelículas (una o múltiples secuencias de película que ser exploradas) es sólo
uno de los posibles hipermedios que nos llaman la atención71.
¿En dónde están las hiperpelículas hoy, casi 50 años después de que Nelson articuló su
concepto? Si entendemos las hiperpelículas de una manera tan limitada como lo hemos
hecho con los hipertextos (tomas conectadas mediante ligas sobre las cuáles un usuario
puede hacer clic), parecería que nunca se desarrollaron. A algunos proyectos pioneros no
se les dio seguimiento: Aspen Movie Map (Architecture Machine Group, 1978-79), Earl
King and Sonata (Grahame Weinbren, 1983-85; 1991-1993), el CD-ROMs de Bob Stein
Voyager Company, y Wax: Or the Discovery of Television Among the Bees (David Blair,
1993). Igualmente, películas interactivas y juegos con video “full-motion”, creados por la
industria de los videojuegos en la primera parte de los 90s, quedó pasada de moda
rápidamente y reemplazada por los juegos 3D que ofrecían más interactividad. Pero si
pensamos en las hiperpelículas en un sentido más amplio, tal como lo hizo Nelson,
cualquier estructura interactiva que conecta video o elementos fílmicos (en donde las
películas son un caso especial), nos daremos cuenta que las hiperpelículas son más
comunes de lo que parecen hoy en día. Los numerosos sitios interactivos basados en Flash
o HTML5 que usan video, los videoclips que ahora tienen marcadores que permiten a un
espectador saltar a un punto específico del video (por ejemplo los video de ted.com72) y el
cine de base de datos73 son sólo unos ejemplos de hiperpelículas contemporáneas.
Décadas antes de que los hipertextos e hipermedios se volvieran los modos comunes para
interactuar con la información, a Nelson le había quedado claro lo que esas ideas
significaban para nuestras prácticas y conceptos culturales establecidos. El anuncio de su
conferencia el 5 de enero de 1965 en Vassar College muestra esto en términos más
alusivos en nuestro contexto actual de lo que fueron en aquellos días: “las consecuencias
filosóficas de todo esto son muy profundas. Nuestros conceptos de “lectura”, “escritura” y
71 Nelson, A File Structure, 144.
72 www.ted.com, March 8, 2008.
73 Ver http://softcinema.net/form.htm.

69
“libro” caen y ahora el reto es diseñar hiperdocumentos y escribir “hipertextos” que
puedan tener más potencial educativo que cualquier otras cosa que haya sido impresa en
papel” 74.
Esta postura alinea el pensamiento y obra de Nelson con artistas y teóricos que de igual
forma querían desestabilizar las convenciones de la comunicación cultural. Los
académicos de medios digitales han discutido ampliamente las analogías entre Nelson y
los pensadores franceses de los 60s (Roland Barthes, Michel Foucault y Jacques Derrida)
75. Otros ya han señalado las similitudes entre Nelson y los experimentos literarios que se
llevaron a cabo al mismo tiempo, como Oulipo76. Incluso también podamos notar la
conexión entre el hipertexto de Nelson y la estructura no lineal de películas como
Hiroshima Mon Amour, L'Année dernière à Marienbad y À Bout de Souffle, entre otros
realizadores franceses que cuestionaban el estilo narrativo clásico.
¿Qué tan lejos debemos llevar estas similitudes? En 1987, Jay Bolter y Michael Joyce
escribieron que el hipertexto debería ser visto como ”una continuación de la “tradición”
moderna de la literatura experimental impresa” que incluye “modernismo, futurismo,
Dada, surrealismo, leterismo, la nueva novela, poesía concreta” 77. Por su lado, Espen J.
Aarseth objetó dicho argumento señalando que el hipertexto no es una estructura
modernista per se, aunque puede soportar la poética modernista si el autor así lo desea78.
¿Quién tiene la razón? Este libro sostiene que el software cultural convirtió a los medios en
meta-medios (un nuevo sistema semiótico y tecnológico que incluye a la mayoría de los
medios precedentes, su estética, sus técnicas y sus elementos fundamentales). Yo
74 Noticia sobre la conferencia de Ted Nelson en Vassar College, Enero 5,1965,
http://xanadu.com/XUarchive/ccnwwt65.tif.
75 George Landow (ed), Hypertext: The Convergence of Contemporary Critical Theory and Technology
(The Johns Hopkins University Press, 1991); Jay Bolter, The writing space: the computer, hypertext,
and the history of writing (Hillsdale, NJ: L. Erlbaum Associates, 1991).
76 Randall Packer & Ken Jordan, Multimedia: From Wagner to Virtual Reality (W. W. Norton &
Company, 2001); Noah Wardrip-Fruin & Nick Montfort, New Media Reader (The MIT Press, 2003).
77 Citado en Espen J. Aarseth, Cybertext: Perspectives on Ergodic Literature (The Johns Hopkins
University Press, 1997), p. 89.
78 Espen J. Aarseth, Cybertext, 89-90.

70
también pienso que el hipertexto es diferente a la tradición literaria modernista. Concuerdo
con Aarseth en que el hipertexto es mucho más general que cualquier poética.
Recordemos que en 1967 Nelson ya había dicho que el hipertexto podría soportar
cualquier estructura de información, incluyendo a los textos tradicionales, entre ellos las
diferentes poéticas modernistas. Curiosamente, esta premisa es replicada por Kay y
Goldberg en su definición de meta-medio, cuyo contenido es “una larga variedad de
medios existentes y aún no inventados”.
¿Qué hay acerca de los académicos que ven relaciones estrechas entre el pensamiento de
Nelson y el modernismo? Aunque Nelson dice que el hipertexto puede soportar cualquier
estructura de información y que la información se debe limitar al texto, sus ejemplos y su
estilo de escritura reflejan una sensibilidad estética inconfundible… la de una literatura
modernista. Claramente se opone al “texto ordinario”. Sus énfasis en la complejidad e
interconectividad y en el rompimiento de unidades convencionales de organización de la
información, como las páginas, alinean la propuesta de Nelson con la literatura
experimental de principios del siglo XX: las creaciones de Virginia Wolf, James Joyce, los
surrealistas, etc. Este vínculo con la literatura no es accidental debido a que la idea
original de Nelson en su investigación que lo llevó a los hipertextos era crear un sistema de
gestión de notas para escritos literarios y manuscritos en general. Nelson conocía acerca
de los textos de William Burroughs. El mismo título de su artículo (Un estructura de
archivos para lo complejo, el cambio y lo indeterminado) sería perfectamente adecuado
para un manifestó vanguardista de inicios del siglo veinte, siempre y cuando
sustituyéramos “estructura de archivos” por algún “ismo”.
La sensibilidad modernista de Nelson también se aprecia en su visión sobre nuevos
medios que pueden establecerse con la ayuda de la computadora. Sin embargo, su obra
no debe considerarse una simple continuación de la tradición modernista. Más bien, tanto
su investigación como la de Kay representan el siguiente nivel del proyecto vanguardista.
Los artistas vanguardistas de inicios del siglo XX estaban muy interesados en cuestionar
las convenciones de medios ya establecidos, como la fotografía, la prensa, el diseño
gráfico, el cine, la arquitectura. Entonces, no importa qué tan diferentes fueron las pinturas
de los futuristas, el orfismo, el suprematismo, o De Stijl, sus manifiestos seguían hablando
de ellos como pinturas, en lugar de un nuevo medio. En contraste, Nelson y Kay

71
explícitamente escribían sobre crear nuevos medios y no sólo cambiar los existentes.
Nelson dice: “Con el monitor informatizado y el la memoria masiva, se ha vuelto posible
crear un nuevo y leíble medio”. Kay y Goldberg: “el texto digital no debe ser tratado como
un libro de papel simulado debido a que es un nuevo medio, con nuevas propiedades”.
Otra diferencia entre la forma en que los modernistas y los pioneros del software cultural
trataron la labor de inventar nuevos medios y extender los existentes está capturada en el
título del artículo de Nelson que ya cité previamente: “Una estructura de archivos para lo
complejo, lo cambiante y lo indeterminado”. En lugar de algún “ismo” particular, tenemos
una estructura de archivos. El cubismo, expresionismo, futurismo, orfismo, suprematismo y
surrealismo propusieron nuevos y diferentes sistemas para organizar la información, pero
cada sistema luchaba contra los demás por la dominación en la memésfera cultural. Por el
contrario, Bush, Licklider, Nelson, Engelbart, Kay, Negroponte y demás crearon metasistemas
que pueden soportar muchos tipos de estructuras de información. Kay llamó a
ese sistema “un primer meta-medio”, Nelson se refirió a él como hipertexto e hipermedios,
Engelbart escribió sobre “manipulación automática de símbolos externos”… pero a pesar
de las diferencias en sus visiones subyacía una comprensión del radicalmente nuevo
potencial de las computadoras en la manipulación de información. Los prefijos “meta” e
“hiper” usados por Kay y Nelson fueron las caracterizaciones adecuadas de un sistema
que era nuevo y capaz de remediar a los demás en su forma particular. Más bien, el nuevo
sistema sería capaz de simular estos medios y sus estrategias de remediación (así como
también el desarrollo de medios aún no inventados). Y no acaba ahí porque igualmente
importante fue el rol de la interactividad. Los nuevos sistemas serían usados
interactivamente para desarrollo procesos de pensamiento, descubrimiento, toma de
decisiones y expresión creativa. Por su lado, la estética de los movimiento modernistas
podría ser vista como sistema de “formato de información”, usados para seleccionar y
organizar la información en un modo de presentación fijo que después sería pasado a los
usuarios, nada diferente a las plantillas PowerPoint. Finalmente, la tarea de definir nuevas
estructuras de información y técnicas de manipulación de medios (aunque de hecho,
podemos incluir todo el nuevo medio) era otorgada al usuario, en lugar de ser exclusiva de
los diseñadores. Más adelante discutiré sobre las consecuencias de todo esto en el
modelamiento de la cultura contemporánea. Una vez que las computadoras y el código de

72
programación se democratizan, se empiezan a usar creativamente para crear estas
estructuras y técnicas, en lugar de hacer sólo “contenido”.
Hoy, un típico artículo en ciencias computacionales o en ciencias de la información no
habla de inventar un “nuevo medio” como justificación de una investigación. Lo que
encontramos son más bien referencias a trabajos previos en alguna rama o sub-rama tales
como “descubrimiento de conocimiento”, “minería de datos”, “web semántico”, etc.
También puede hacer referencia a prácticas e industrias culturales y sociales actuales
como “e-learning”, “desarrollo de videojuegos”, “taggeo colaborativo” o “colaboración
masiva distribuida”. En cualquier caso, la justificación de una nueva investigación está
avalada por una referencia a prácticas establecidas o, por lo menos, populares como
paradigmas académicos que han que recibido financiamiento, industrias de gran escala o
rutinas sociales que cuestionan el orden social existente. Esto significa que prácticamente
toda la investigación en ciencias computacionales (tecnologías Web, computación social,
hipermedios, interfaces humano-computadoras, gráficas computacionales, etc.) que trata
sobre medios está orientada al uso típico de los medios.
Dicho en otras palabras, los investigadores en computación o están tratando de hacer más
eficiente el uso de las tecnologías por las industrias mediáticas (videojuegos, motores de
búsqueda Web, producción de cine, etc.) o están inventando nuevas tecnologías que serán
usadas en el futuro por dicha industria. La invención de nuevos medios en sí no es algo a
quien se quiere dedicar alguien o incluso que pueda recibir capital. Desde esta
perspectiva, la industria del software en general es más innovadora que la académica o la
científica. Por ejemplo, aplicaciones de medios sociales como Wikipedia, Flickr, YouTube,
Facebook, delicious, digg, etc. no fueron inventadas en la academia; tampoco lo fueron
Hypercard, Quicktime, HTML, Photoshop, After Effects, Flash o Google Earth. Esto no ha
sido diferente en décadas pasadas. Entonces no para sorprenderse que las carreras de
Ted Nelson y Alan Kay hayan sido hechas en la industria y no en la academia: Kay trabajó y
ha sido colaborador de Xerox PARC, Apple, Atari, y Hewlett-Packard; Nelson fue consultor
de los laboratorios Bell, Datapoint Corporation, Autodesk; y ambos han estado asociados a
Disney.

73
¿Por qué Nelson y Kay hallaron más apoyo en la industria que en la academia en su
búsqueda por inventar nuevos medios computacionales? ¿Y por qué la industria (a la cuál
me refiero simplemente como cualquier entidad que crea productos que pueden ser
vendidos en grandes cantidades o monetizados de otras formas, sin importar que sea una
transnacional o una start-up) está más interesada en tecnologías, aplicaciones y
contenidos innovadores de los medios más que las ciencias computacionales? La
respuesta necesitaría su propia investigación. Pero también, ¿qué tipo de innovación
puede soportar una institución moderna a lo largo del tiempo? Aquí podemos dar una
respuesta breve. Los negocios modernos tratan de crear nuevos mercados, nuevos
productos y nuevas categorías de productos. Aunque esto viene siempre con un alto
riesgo, las ganancias son rentables. Este fue el caso cuando Nelson y Kay fueron
respaldados por Xerox, Atari, Apple, Bell, Disney, etc. En los años 2000, después de la
globalización de los 90’s, todas las áreas de negocios han acogido la innovación de forma
sin precedentes. Este ritmo se acentuó alrededor del 2005 cuando las compañías
competían por hacer nuevo mercado en China, India y otras economías “anteriormente”
emergentes. Al mismo tiempo, vimos un incremento de productos novedosos: API’s
abiertos de los grandes sitios Web 2.0, anuncios cotidianos de nuevos servicios Web,
aplicaciones de geolocalización, productos para consumidores como el iPhone y Microsoft
Surface, nuevos paradigmas de imágenes como el HDR y la edición no destructiva, los
inicios de una “larga cola” de hardware, y demás.
Como podemos ver a partir de los ejemplos que hemos analizado, el objetivo de los
inventores de medios computacionales no era simplemente crear simulaciones de medios
físicos. Se trataba de crear “un nuevo medio con nuevas propiedades” que permitiera a la
gente comunicarse, aprender y crear de formas nuevas. La apariencias de estos nuevos
medios puede parecer similar a los viejos, pero no debemos dejarnos engañar. La novedad
no está en el contenido o en la apariencia sino en las herramientas de software usadas
para crear, editar, ver y compartir dicho contenido. Por eso, en lugar de observar el
“resultado” de las prácticas culturales basadas en software, debemos considerar el
software mismo, es él quien permite a la gente trabajar con medios de maneras
previamente desconocidas. Una vez más: la apariencia de los medios digitales puede ser
que sí remedie y represente formas anteriores, pero el software y ambiente en el que
“vive” es muy diferente.

74
Déjenme añadir un par de ejemplos más. Primero, Sketchpad que influyó profundamente
todos los trabajos subsecuentes en medios computacionales (incluyendo a Kay) no sólo
porque fue el primer sistema interactivo de producción de medios sino también porque
dejó claro que las simulaciones digitales de medios físicos pueden añadir muchas
propiedades nuevas a los medios que son simulados. En 1962, Ivan Sutherland creó
Sketchpad como parte de su tesis de doctorado en el MIT y fue el primer software que
permitía a sus usuarios crear y modificar interactivamente dibujos a base de líneas. Como
ya lo ha dicho Noah Wardrip-Fruin, “nos llevó fuera del papel mediante la posibilidad de
dibujar en cualquiera de los 2000 niveles de acercamiento; esto permitió crear proyectos
que los serían muy grandes o muy detallados para los medios físicos” 79. Sketchpad
concibió los elementos gráficos como objetos “que pudieran ser manipulados,
constreñidos, instanciados, representados, copiados, combinados y en los que podemos
aplicar y reaplicar estas operaciones” 80. Por ejemplo, si un diseñador definía nuevos
elementos gráficos como instancias de un elemento maestro, éste cambiaba
automáticamente si hacía cualquier cambio a dichas instancias.
Otra nueva propiedad que demostraba la manera en que el diseño y boceto a
computadora era radicalmente nueva era el uso de los constreñimientos o imposiciones en
Sketchpad. En palabras de Sutherland: “la propiedad principal que distingue a Sketchpad
del papel y lápiz es la habilidad que ofrece al usuario para especificar condiciones
matemáticas en partes ya trazadas de su dibujo de tal forma que la computadora pueda
aplicar dichos parámetros y que el usuario obtenga la forma deseada” 81. Por ejemplo, si
un usuario dibuja algunas líneas y luego les asigna un cierto parámetro, Sketchpad podría
mover dichas líneas para que sean exactamente paralelas unas con otras. Si el usuario
selecciona una línea y asigna otro comando, Sketchpad podría mover la línea para que sea
perpendicular a las otras líneas antes trazadas.
79 Noah Wardrip-Fruin, introducción a “Sketchpad. A Man-Machine Graphical Communication
System,” in New Media Reader, 109.
80 Ibid.
81 Ivan Sutherland, “Sketchpad. A Man-Machine Graphical Communication System,” Proceedings of
the AFIPS Spring Joint Computer Conference, Detroit, Michigan, Mayo 21-23, 1963, pp. 329-346; in
New Media Reader, eds. Noah Wardrip-Fruin and Nick Montfort.

75
No es nuestro objetivo hacer una lista exhaustiva de las nuevas propiedades que
Sutherland introdujo con Sketchpad, pero debe quedar claro que el primer editor
interactivo de gráficos no sólo simulaba medios existentes. El artículo que Sutherland
publicó en 1963 resaltaba las nuevas capacidades gráficas de su sistema, maravillándose
de cómo abre nuevos campos en “manipulación gráfica que nunca habían estado
disponibles” 82. Tan solo el título de su tesis doctoral anticipa su novedad: Sketchpad: un
sistema humano-máquina de comunicación gráfica. Es curioso observar que la dimensión
comunicativa también es resaltada por Kay y Goldberg, a la que se refieren como una
“conversación en dos sentidos” y que hace del meta-medio algo “activo” 83. Pero también
Licklider habló de simbiosis humano-máquina y Sketchpad sería un buen ejemplo aplicado
al diseño y producción de imágenes84.
Mi último ejemplo es el software de pintura. Quizá a primera vista pueda sonar
contradictorio. ¿Se puede decir que las aplicaciones que simulan a detalle el rango de
efectos que es posible lograr con brochas, pinceles, cuchillos, telas y papeles buscan
recrear la experiencia de trabajar con algún medio existente en lugar de crear uno nuevo?
Error. En 1997, Alvy Ray Smith, un artista y pionero de las gráficas computacionales
escribió su memo Digital Paint Systems: Historical Overview85. En este texto hace una
importante distinción entre programas de pintura digital y sistemas de pintura digital. En
su definición, “un programa de pintura digital no hace más que implementar una
simulación digital de la pintura clásica en tela y pincel. Un sistema de pintura digital llevará
más lejos la idea, usando la “simulación de la pintura” como metáfora para seducir al
artista en el nuevo, y quizá hostil, mundo digital”. Según la historia de Smith, la mayoría de
las aplicaciones comerciales, incluyendo Photoshop, están ubicada en la categoría de
sistema de pintura. Su genealogía de sistemas de pintura inicia con SuperPaint de Richard
82 Ibid., 123.
83 Kay y Goldberg, “Personal Dynamic Media,” 394.
84 J.C. Licklider, “Man-Machine Symbiosis,” IRE Transactions on Human Factors in Electronics,
volume HFE-1, March 1960, pp. 4-11, in New Media Reader, eds. Noah Wardrip-Fruin and Nick
Montfort.
85 Alvy Ray Smith, Digital Paint Systems: Historical Overview (Microsoft Technical Memo 14, Mayo 30,
1997). http://alvyray.com/.

76
Shoup, desarrollado en Xerox PARC entre 1972 y 197386, que permitía pintar con
diferentes pinceles y con diferentes colores. SuperPaint incluía algunas técnicas que no
son posibles en la pintura tradicional, como Shoup lo dice: “las zonas o los objetos pueden
ser redimensionados, desplazados, copiados, sobrepuestos, combinados, modificados de
color o guardados en nuestro disco para un uso futuro o para su destrucción” 87.
Pero quizá más importante era la habilidad de soportar fotogramas de video. Cuando se
cargaba en el sistema, cada fotograma podía ser usado como cualquier otra imagen. El
sistema también podía regresar el resultado como señal de video. Shoup tenía claro que
su sistema iba más allá que una simple forma de dibujar o pintar con la computadora. En
un artículo de 1979 califica a SuperPaint de un nuevo medio “videográfico” 88. En otro
artículo publicado un año más tarde afina su aseveración: “desde una perspectiva más
amplia, nos dimos cuenta que el desarrollo de SuperPaint señaló el comienzo de la
sinergia de dos de las más poderosas tecnologías jamás inventadas: la computación digital
y el video o la televisión” 89.
Estamos antes una idea sorprendente. Cuando Shoup escribía esto en 1980, eran
contadas con la mano las veces que las gráficas computacionales se veían en la TV.
Aunque es cierto que se volvieron más visibles en la década siguiente, fue sólo hasta
mediados de los 90’s que la sinergia que predijo Shoup se volvió visible. Ya veremos en el
capítulo dedicado a After Effects más tarde que el resultado fue una dramática
reconfiguración no sólo del lenguaje visual de la televisión sino de todas les técnicas
inventadas por los humanos hasta ese punto. En otras palabras, lo que empezó como un
nuevo medio “videográfico” en 1973 eventualmente cambió todos los medios visuales.
86 Richard Shoup, “SuperPaint: An Early Frame Buffer Graphics Systems,” IEEE Annals of the History
of Computing 23, número 2 (Abril-junio 2001), p. 32-37,
http://www.rgshoup.com/prof/SuperPaint/Annals_final.pdf; Richard Shoup, “SuperPaint…The
Digital Animator,” Datamation (1979), http://www.rgshoup.com/prof/SuperPaint/Datamation.pdf.
87 Shoup, “SuperPaint…The Digital Animator,” 152.
88 Ibid., 156.
89 Shoup, “SuperPaint: An Early Frame Buffer Graphics System,” 32.

77
Incluso si olvidamos las posibilidades revolucionarias de SuperPaint, seguimos ante un
nuevo medio creativo (en palabras de Smith). Tal como lo dijo Smith, el medio es el buffer
de fotogramas digitales90, una especie de memoria digital diseñada para almacenar
imágenes representadas como una serie de pixeles (hoy conocida como tarjeta de
gráficos). Un artista que trabaja con un sistema de pintura está, de hecho, modificando
valores de pixeles en un buffer de fotogramas (sin importar la operación o herramienta en
cuestión). Esto es una salida a nuevas operaciones de creación y modificación de
imágenes, que siguen una lógica diferente a la pintura física. Los ejemplos más claros se
pueden ver en un sistema de pintura llamado sencillamente Paint, desarrollado por Smith
entre 1975 y 1976. En las palabras de Smith, “en lugar de simular el trazo de una línea de
color, amplié la noción para abarcar: realiza cualquier manipulación de imagen que quieras
con los pixeles del pincel” 91. Con esta generalización conceptual, Smith incluyó varios
efectos que todavía usan la herramienta “pincel” pero que ya se refieren a la pintura del
mundo físico. Por ejemplo, en Paint “cualquier imagen de cualquier forma puede ser usada
como pincel”. Otro ejemplo: Smith introdujo “no pintar” que invertía el color de cada pixel
en su complemento cromático. También definió la “pintura de marcas” (smear paint) que
promediaba los colores de los pixeles a proximidad y lo mostraba en un solo pixel. Así
vemos que las implementaciones en donde el pincel se comportaba más como un pincel
físico eran sólo casos particulares de un universo más grande de nuevos comportamientos
posibles en un nuevo medio.
La extensibilidad permanente
Como hemos visto, Sutherland, Nelson, Engelbart, Kay y demás pioneros de los medios
computacionales han agregado varias propiedades que previamente no existían en los
medios que simulaban. Las generaciones subsecuentes de académicos, hackers y
diseñadores han añadido aún más y el proceso está lejos de haber acabado. De hecho, no
90 Alvy Ray Smith, “Digital Paint Systems: An Anecdotal and Historical Overview,” IEEE Annals of the
History of Computing. 2011, http://accad.osu.edu/~waynec/history/PDFs/paint.pdf.
91 Ibid., 18.

78
hay razón lógica o material para pensar que se terminará. La “naturaleza” de los medios
computacionales es interminable y nuevas técnicas se inventan continuamente.
Para agregar nuevas propiedades a los medios físicos es necesarios modificar su
substancia física. Pero como los medios computacionales existen como software, podemos
agregar nuevas propiedades o inventar nuevos tipos de medios cambiando o escribiendo
nuevo software. O añadiendo plug-ins y extensiones como ha sido el caso en Photoshop y
Firefox. O agrupando software, por ejemplo, en 2006 los usuarios extienden las
capacidades de medios de mapas, creando mashups que combinan servicios con datos de
Google Maps, Flickr, Amazon con medios generados por los usuarios.
O sea que los “nuevos medios” son “nuevos” porque sus nuevas propiedades (es decir,
nuevas técnicas del software) se les pueden añadir fácilmente. Dicho de otra manera, en
la era industrial, con tecnologías de medios producidas masivamente, el “hardware” y el
“software” era una misma cosa. Por ejemplo, las páginas de un libro estaban atadas de
manera que fijaban el orden y secuencia. El lector no podía cambiar este orden ni alterar el
nivel de detalle tal como lo hizo Engelbart con su “control de vistas”. De forma similar, un
proyector de películas combinaba “hardware” con lo que hoy llamamos software de “lector
de medios” en una misma máquina. Los controles de una cámara producida masivamente
en el siglo XX no podían ser modificados por el usuario a su voluntad. Aunque todavía hoy
el usuario de una cámara digital no puede modificar fácilmente el hardware, a partir del
momento en que transfiere sus imágenes a la computadora tiene acceso a un número
ilimitado de controles y opciones para modificar sus imágenes.
En los siglos XIX y XX había dos tipos de situaciones en que un medio industrial fijo era
más fluido. La primera situación era cuando el nuevo medio estaba siendo desarrollado,
por ejemplo, la invención de la fotografía entre 1820 y 1840. El segundo tipo de
situaciones es cuando los artistas experimentan y “abren” un medio industrializado, por
ejemplo los experimentos en cine y video de los años 60’s, a los que se llamó “cine
expandido”.
Lo que fueron dos momentos distintos de experimentaciones con medios durante la era
industrial se volvió la regla en la sociedad del software. Podemos decir que la computadora
legitima la experimentación con los medios. ¿Por qué sucede esto? Lo que hace diferente

79
a un moderna computadora digital de otras máquinas (incluyendo máquinas de medios
industriales para capturar y reproducir medios) es la separación de hardware y software.
Es porque un sin fin de programas que realizan cosas diferentes pueden escribirse para
correr en un solo tipo de máquina (la computadora digital) que ésta máquina es tan usada
hoy en día. Y así, la invención constante de nuevos medios o sus modificaciones es un
ejemplo de esta principio general. En otras palabras, la experimentación es una
características estándar de los medios computacionales. En su mera estructura son
“vanguardistas” porque siempre está en extensión y redefinición.
Si en la cultura moderna lo “experimental” y lo “vanguardista” se oponían a lo normal y a lo
estable, esta oposición se borra en la cultural del software. Y el rol del vanguardismo de
medios ya no está representado por artistas individuales en sus estudios sino por una gran
variedad de actores, de muy grandes a muy pequeños, de Microsoft, Adobe, Apple a
programadores independientes, hackers y diseñadores.
Pero este proceso de invención continua de nuevos algoritmos no se mueve en cualquier
sentido. Si observamos el software de medios contemporáneos, veremos que la mayoría
de los principios fundamentales habían sido desarrollados por la generación de Sutherland
y Kay. De hecho, el primer editor de gráficos interactivo, Sketchpad, ya tenia los genes, por
así decirlo, de las aplicaciones de gráficas contemporáneas. Mientras más nuevas técnicas
siguen siendo inventadas, éstas se ponen encima de los fundamentos que fueron
gradualmente implementados por Sutherland, Engelbart, Kay y demás en los 60’s y 70’s.
No estamos hablando únicamente de la historia de las ideas. Varios factores sociales y
económicos también imponen posibles direcciones en la evolución del software, por
ejemplo el dominio de un mercado de software o la adopción masiva de algún formato de
archivo. Puesto de otra manera, el desarrollo de software actualmente es una industria y
como tal está equilibra constantemente entre estabilidad e innovación, estandarización y
exploración de nuevas posibilidades. Pero al mismo tiempo no es cualquier tipo de
industria. Nuevos programas pueden escribirse y programas existentes pueden extenderse
y modificarse por cualquiera que tenga conocimientos de programación, acceso a una
computadora, a un lenguaje de desarrollo y a un compilador (y si el código fuente está

80
disponible). O sea que el software de hoy es fundamentalmente “posible de hacer” en el
sentido en que los objetos producidos industrialmente no lo son.
A pesar de que Turing y Von Neumann ya había formulado en teoría esta extensibilidad del
software (cientos de miles de personas involucradas diariamente en la expansión de
posibilidades de medios computacionales), se trata del resultado de un largo desarrollo
histórico. Este desarrollo nos llevó de las computadoras del tamaño de un salón, que no
eran fáciles de reprogramar, a las computadoras baratas y programables de unas décadas
posteriores. Esta democratización del desarrollo de software era el núcleo de la visión de
Kay, quien se preocupaba en cómo estructurar herramientas que fueran posibles de
programar por usuarios comunes: “debemos proveer suficientes herramientas generales
predefinidas para que los usuarios no tengan que empezar desde cero para hacer las
cosas que quieren hacer” (Kay y Goldberg, 1977).
Si comparamos el proceso de innovación continua de los medios con software con la
historia de los medios pre-computacionales veremos que se revela una mecánica. Según
una idea general, cuando un nuevo medio es inventado, éste imita a los medios existentes
antes de descubrir su propia estética y lenguaje. Por ejemplo, las primeras biblias
impresas imitaban claramente el diseño de manuscritos; las películas producidas entre
1890 y 1900 mimetizaban el formato teatral mediante la aparición de los actores detrás
del escenario y su postura frente a la audiencia. Lenta y gradualmente, los libros impresos
desarrollaron una nueva forma para presentar la información, de la misma manera que el
cina desarrolló sus conceptos de narrativa espacial que con cambios continuos de puntos
de vista hace que los usuarios entren al espacio, encontrándose literalmente dentro de la
historia.
¿Podemos decir que esta mecánica aplica también a los medios computacionales? Tal
como lo teorizaron Turing y Von Neumann, la computadora es una máquina de simulación
multiusos. Esto es lo que la hace única y diferente de los demás máquinas y medios. Esto
significa que la idea de que un nuevo medio encuentra gradualmente su lenguaje no aplica
para los medios computacionales. Si fuera cierto iría en contra de la definición misma de
computadora digital moderna. Este argumento teórico está soportado por la práctica. La
historia de los medios computacionales no ha tratado hasta ahora de llegar a un lenguaje

81
estándar (como lo pasó al cine, por ejemplo) sino a extender los usos, técnicas y
posibilidades. En lugar de llegar a un lenguaje particular nos percatamos que la
computadora puede hablar cada vez más lenguajes.
Pero podemos encontrar un argumento más si nos detenemos un poco en la historia de los
medios computacionales. La tarea sistemática de convertir a la computadora en un medio
que simula y extiende a los demás llegó después que ya tenían diversos usos (realizar
diferentes tipos de cálculos, solucionar problemas matemáticos, controlar otras máquinas
en tiempo real, recrear simulaciones matemáticas, recrear algunos comportamientos
humanos, etc.). Así, cuando la generación de Sutherland, Nelson y Kay empezó a crear
“nuevos medios” lo hicieron sobre aquello que las computadoras saben hacer mejor.
Entonces se empezaron a añadir nuevas propiedades a los medios físicos. Si retomamos
el ejemplo de Sketchpad, Sutherland sabía que las computadoras podían resolver
problemas matemáticos y así introdujo una nueva función que no existía antes: control de
parámetros de imágenes. Parafraseando estas ideas podemos decir que en lugar de
movernos de la imitación de un media viejo para encontrar su propio lenguaje, los medios
computacionales hablaban su propio lenguaje desde su concepción.
Los pioneros de la medios computacionales no tenían la intención de hacer de la
computadora una “máquina de remediación” que simplemente representara medios
viejos. Se trató más bien de poner en avante las posibilidades de las computadoras
digitales para crear radicalmente nuevos tipos de medios para la expresión y la
comunicación. Para estos nuevos medios, su materia prima sieguen siendo los medios
precedentes (que han servido a la humanidad por cientos de años: escritura, dibujo,
sonidos, pinturas, etc.) pero esto no compromete su novedad. Los medios
computacionales se sirven de ellos como unidades básicas para crear formas, estructuras,
herramientas, opciones de comunicación que no existían antes.
Sería incorrecto decir que el trabajo de Sutherland, Engelbart, Nelson y Kay había tomado
un solo rumbo al desarrollar sus medios computacionales sobre la base de teorías
computacionales, lenguajes de programación e ingeniería de software. Es decir, de
principios existentes y generales a técnicas particulares. Estos inventores pusieron en tela
de juicio muchas de ideas de la computación. Definieron muchos nuevos conceptos y

82
técnicas fundamentales sobre hardware y software, haciendo contribuciones a la
ingeniería de software. Un ejemplo es el desarrollo de Smalltalk por Alan Kay que
estableció sistemáticamente, por primera vez, el paradigma de la programación orientada
objetos. La lógica de Kay era crear una apariencia unificada a todas las aplicaciones e
interfaces de usuario de PARC pero sobretodo también permitir a los usuarios programar
rápidamente sus propias herramientas de medios. Según Kay un programa de ilustración
hecho con Smalltalk por una talentosa niña de 12 años cabía en una página de código92.
Con el tiempo, la programación orientada objetos se ha vuelto muy popular y muchos
lenguajes siguen su paradigma, como C.
Este vistazo a los medios computacionales y a las ideas de sus inventores nos deja claro
que se trata de algo opuesto al determinismo tecnológico. Cuando Sutherland diseñó
Sketchpad, Nelson inventó el hipertexto, Kay programó un programa de dibujo, cada nueva
característica de los medios computacionales debió ser imaginada, implementada,
probada y corregida. O sea que estas nuevas propiedades no fueron el resultado inevitable
de la convergencia entre computadoras digitales y medios modernos. Más bien los medios
computacionales tuvieron que ser inventados paso a paso. Y fueron inventados que se
inspiraron del arte moderno, la literatura, la psicología educativa y cognitiva, la teoría de
medios y la tecnología. Kay recuerda que la lectura del libro Understanding Media, de
McLuhan, lo hizo darse cuenta que la computadora podía ser un medio y no solo una
herramienta93. La primera parte del artículo de Kay y Goldberg se llama “Humanos y
medios” y se lee como teoría de medios. Pero ésta no es una típica teoría que describe los
medios sino un análisis para crear un plan de acción para crear un nuevo mundo, tal como
lo hizo Marx con su análisis del capitalismo, pero esta vez permitiendo a las personas crear
sus propios nuevos medios.
Quizá el ejemplo más importante de este desarrollo no determinista es la invención de la
interfaz gráfica humano-computadora moderna. Ninguno de los conceptos clave de la
computación moderna establecidos por Von Neumann hablaba de una interfaz interactiva.
92 Alan Kay, Doing with Images Makes Symbols (University Video Communications, 1987),
conferencia grabada en video, http://archive.org/details/AlanKeyD1987/.
93 Alan Kay, “User Interface: A Personal View,” 192-193.

83
A finales de los 40s y 50s el Laboratorio Lincoln del MIT desarrollo interfaces gráficas
usadas en SAGE (los centros de control creados en EUA para recolectar información de
estaciones de radar y coordinar contraataques). Pero la interfaz de SAGE fue diseñada
para tareas muy específicas y no tuvo efecto en el desarrollo de la computacional
comercial. Sin embargo, sí orientó hacia una nueva computadora interactiva pequeña, la
TX-2, usada por estudiantes del MIT para explorar lo que podías hacerse con una
“computadora interactiva” (es decir, computadoras con una pantalla visual). Algunos
alumnos empezaron a crear juegos, entre ellos Spacewars (1960). Sutherland fue uno de
esos estudiantes que exploró las posibilidades de la computación visual interactiva usando
TX-2. Todo esto lo llevó a crear Sketchpad (su tesis de doctorado) que influenció a Kay y
demás pionera de la computación cultural de los 60s. El camino teórico que condujo de
SAGE a la GUI moderna en PARC fue muy largo.
Según Kay, el paso clave para él y su grupo fue pensar en la computadora como un medio
para aprender, experimentar y de expresión artística que fuera usada no sólo por adultos
sino también por “niños de todas las edades” 94. Un gran influencia en Kay fueron las
teorías del psicólogo Jerome Bruner que estaban basadas en las ideas de Jean Piaget. Se
había identificado que los niños pasan por distintas etapas en su desarrollo: kinésica,
visual y simbólica. Piaget pensaba que cada etapa existía sólo en un periodo particular
mientras que Bruner creía que hay mentalidades separadas que corresponden a cada
etapa y que siguen existiendo a medida que crece el niño. También sugería que las
mentalidades no se reemplazan sino que se añaden. Bruner dio nuevos nombres a estas
mentalidades: activa, icónica y simbólica. Cada mentalidad se desarrolla en diferentes
etapas de la evolución humana y siguen existiendo en la edad adulta.
La interpretación de Kay a estas teorías fue que la interfaz de usuario debía hacer
referencia a estas tres mentalidades. Al contrario de una interfaz basada en líneas de
comando, que no es accesible a niños y fuerza al adulto a usar su mentalidad simbólica, la
nueva interfaz debía usar también las mentalidades emotivas e icónicas. Kay también
tomó en cuenta numerosos estudios de creatividad en matemáticas, ciencia, música, arte,
94 Alan Kay, “A Personal Computer for Children of All Ages,” Proceedings of the ACM National
Conference, Boston, 1972, http://www.mprove.de/diplom/gui/kay72.html.

84
y demás áreas, en donde se veía que el trabajo inicia con mentalidades icónicas y
activas95. Esto inspiró en gran medida la idea que si las computadoras iban funcionar
como un medio dinámico para el aprendizaje y la creatividad, deberían permitir a sus
usuarios pensar no sólo mediante símbolos sino también con acciones e imágenes.
El grupo de PARC materializó la teoría de múltiples mentalidades de Bruner en las
tecnologías de interfaz de la siguiente manera. El mouse acciona la mentalidad activa
(identifica en dónde estás, manipula). Íconos y ventanas accionan la mentalidad icónica
(reorganiza, compara, configura). Finalmente, el lenguaje de programación Smalltalk
permite a los usuario accionar su mentalidad simbólica (combina largas cadenas de
razonamiento, haz abstracción) 96.
Actualmente, la GUI contemporáneas relacionan constantemente diferentes mentalidades.
Usas un mouse para moverte por la pantalla, como si fuera un espacio físico, y para
apuntar hacia objetos. Todos los objetos están representados por íconos visuales. Haces
doble clic en un ícono para activarlo o, si es un fólder, para examinar su contenido. Esto se
puede ver como el equivalente a tomar y examinar objetos del mundo físico. Cuando una
ventana se abre es posible alternar entre diferentes vistas, ver los datos como íconos o
listas que pueden ser ordenada de acuerdo al nombre, fecha de creación y demás
información simbólica (es decir, texto). Si no encuentras los archivos que estás buscando
puedes hacer una búsqueda en todo el disco duro de la computadora, seleccionando
palabras clave. Como lo demuestran estos ejemplos, el usuario está cambiando
contantemente entre mentalidades, usando cada vez las más adecuado según sus
necesidades.
Además de los principios generales de la interfaz, otras técnicas desarrolladas por Kay
también se pueden entender como un uso de las diferentes mentalidades. Por ejemplo, la
interfaz de usuario de PARC fue la primera que corrió en una pantalla basada en bit
mapeados lo que daba a los usuarios la posibilidad de mover el cursor y abrir varias
ventanas, pero también de escribir programas de en Smalltalk y poder ver el resultado
95 Alan Kay, “User Interface: A Personal View”, 195.
96 Ibid., 197.

85
visual inmediatamente. Cualquier modificación en el código mostraba su resultado en la
imagen producida por el programa. Hoy esta posibilidad es fundamental en el uso de las
computadoras, de las ciencias hasta la visualización de datos interactiva. Otro ejemplo que
no podemos dejar de lado son todos los editores de medios creados en PARC: programas
de dibujo, ilustración, música, etc. Estos programas hacían que el usuario cambiara de
mentalidad de una manera que no era posible de lograrse con los medios físicos. Imagina
los objetos en un programa de animación, que pueden ser dibujados a mano o escribiendo
código Smalltalk. Como lo señalan Kay y Goldberg, “el control de una animación se puede
lograr fácilmente con una simulación Smalltalk. Por ejemplo, una animación de objetos
que saltan es más fácil de hacer con unas cuantas líneas de código que expresan la clase
de objetos saltantes en términos físicos” 97.
Al definir el nuevo tipo de interfaz de usuario, Kay y sus colaboradores crearon un nuevo
tipo de medio. Si aceptamos la teoría de las múltiples mentalidades de Bruner y su
interpretación por Kay, podemos concluir que el nuevo medio que inventaron puede hacer
cosas que los medios precedentes no. Esto puede ayudar a explicar el éxito y popularidad
de la GUI que, 40 años más tarde, sigue dominando la interacción con la computadoras.
La gente la prefiere no porque sea “fácil” o “intuitiva”, sino porque les ayuda a pensar,
descubrir y crear nuevos conceptos mediante el uso de múltiples mentalidades juntas. En
pocas palabras, mientras que muchos expertos en interacción humano-computadora
siguen pensando que la interfaz ideal debe ser “invisible” y dejar a los usuario hacer su
trabajo, si volteamos hacia las teorías de Kay y Goldberg tenemos una manera muy
diferente de comprender la identidad de la interfaz. Kay y PARC pensaban en la interfaz
como un medio diseñado en cada detalle ara facilitar el aprendizaje, descubrimiento y
creatividad.
Con el énfasis que la sociedad de la información otorga a la innovación permanente, a la
educación continua y a la creatividad, es apropiado señalar que a medida que se
construía, un nuevo medio estaba siendo inventado para facilitar sus necesidades. En
1973, Daniel Bell publicó su influyente The Coming of Post-Industrial Society, al mismo
tiempo que Kay, Goldberg, Chuck Thacker, Dan Ingalls, Larry Tesler y otros miembros del
97 Kay and Goldberg, “Personal Dynamic Media,” 399.

86
The Learning Research Group crearon el paradigma de la computación moderna. O más
bien reinventaron la computadora, la transformaron de una veloz calculadora a un sistema
interactivo para el razonamiento y la experimentación. En breve, de una herramienta a un
metamedio.
Desafortunadamente cuando la GUI se volvió un éxito comercial con la Macintosh en
1984, sus origines intelectuales fueron olvidados. La razón de ser de la GUI se redujo a un
argumento simplista: como se trata de algo nuevo para el usuario, debe ser “intuitiva”,
debe recrear cosas familiares del mundo físico. Es sorprendente que esta idea siga todavía
vigente en nuestros días, aún con las generaciones de “nativos digitales”. Por ejemplo, las
recomendaciones de Apple para desarrollar interfaces humanas para iPhone (marzo,
2010) dice: “cada que sea posible, modela los objetos y acciones de tu aplicación con
base en objetos y modelos del mundo real. Esta técnica permite a los usuarios
principiantes entender rápidamente cómo funciona tu aplicación. Los folders son una
metáfora clásica de software. Como la gente archiva sus documentos en carpetas en el
mundo real, inmediatamente entienden la idea de poner datos en una carpeta de la
computadora” 98. Lo irónico es que estas recomendaciones también aplican para
desarrolladores de iPad, un producto que representa un paso más hacia la migración de
un mundo físico a un mundo totalmente digital. Los medios viejos son recordados y
valorados constantemente, pero también se nos pide que los olvidemos.
La computadora como meta-medio
Como lo hemos dicho, el desarrollo de los medios computacionales sigue una historia
opuesta a los medios precedentes. Pero en cierto sentido, la idea de que un nuevo medio
gradualmente descubre su lenguaje sí aplica a los medios computacionales después de
todo. Así como sucedió con los libros impresos y el cine, este proceso tomó unas cuantas
98
http://developer.apple.com/iphone/library/documentation/UserExperience/Conceptual/MobileHIG/
PrinciplesAndCharacteristics/PrinciplesAndCharacteristics.html#//apple_ref/doc/uid/TP40006556-
CH7-SW1, Abril 5, 2010.

87
décadas. Cuando las primeras computadoras fueron armadas en los años 40s, no se
podían usar para fines culturales, de expresión y comunicación. Lentamente, con el trabajó
de Sutherland, Engelbart, Nelson, Papert y otros en los 60s, se desarrollaron las ideas y
técnicas que hicieron de la computadora una “máquina cultural”. Uno podía crear y editar
texto, hacer dibujos, moverse alrededor de un objeto virtual, etc. Y finalmente, cuando Kay
y sus colegas en PARC sistematizaron y refinaron estas técnicas para ponerlas bajo el
techo de la GUI, haciéndolas accesibles a multitudes de usuarios, fue cuando la
computadora digital obtuvo su propio lenguaje (en términos culturales). Fue hasta que la
computadora se volvió un medio cultural y no sólo una máquina versátil.
La computadora se convirtió en algo que ninguno otro medio había sido antes. Esto que
surgió no era otro medio sino, como insisten Kay y Goldberg en su artículo, algo
cualitativamente diferente e históricamente sin precedentes. Para marcar esta diferencia,
introdujeron el término “meta-medio”.
Este meta-medio es sui generis de varias formas. Una de ellas ya ha sido discutida aquí
ampliamente (la capacidad de representar a los demás medios y de añadirles nuevas
propiedades). Kay y Goldberg también hablan de otras formas cruciales. El nuevo metamedio
es “activo (puede responder a búsquedas y experimentos) de tal medida que los
mensajes pueden involucrar al aprendiz en una conversación de ida y vuelta”. Para Kay,
quien estaba profundamente interesado en la educación y en los niños, esta forma era
particularmente importante porque “nunca se había logrado antes, excepto en el caso de
profesores individualizados” 99. Además, el nuevo meta-medio puede soportar
“virtualmente todas las necesidades de información de su usuario”. Puede servir de
“herramienta de programación y resolución de problemas” y de “memoria interactiva para
el almacenamiento y manipulación de datos” 100. Pero quizá la forma más importante
desde el punto de vista de la historia de los medios es que el meta-medio de la
computadora es simultáneamente un conjunto de diferentes medios y un sistema para
generar nuevas herramientas de medios y nuevos tipos de medios. En otras palabras, una
99 Kay & Goldberg, “Personal Dynamic Media,” 394.
100 Ibid., 393.

88
computadora puede ser usada para crear nuevas herramientas de trabajo con diferentes
tipos de medios existentes y crear otras que aún no han sido inventadas.
En la introducción de su libro Expressive processing, Noah Wardrip-Fruin articula
perfectamente esta “meta-generatividad” que es específica a las computadoras:
Una computadora puede simular una máquina de escribir (recibiendo información
de un teclado y acomodando pixeles en la pantallas que corresponden a las
teclas) pero también puede ir mucho más lejos: ofrece tipografía, corrector de
ortografía automático, reacomodo de secciones de un documento (mediante
simulaciones de “cortar” y “pegar”), transformaciones programáticas (como
“buscar” y “reemplazar”), e incluso autoría colaborativa entre grandes grupos
distribuidos (como Wikipedia). Esto es para lo que fueron diseñadas las
computadoras modernas (o en su nombre más largo “computadoras digitales
electrónicas de programas almacenados”): la creación continua de nuevas
máquinas, abriendo nuevas posibilidades, mediante la definición de nuevos
procesos computacionales101.
Haciendo analogía con la alfabetización impresa, Kay modula esta propiedad de la
siguiente forma: “la habilidad de “leer” un medio significa que se puede acceder a
materiales y herramientas generadas por otros. La habilidad de “escribir” en un medio
significa que se pueden generar materiales y herramientas para los demás. Se deben
poseer ambas para ser alfabetizado” 102. En consecuencia, los esfuerzos de Kay en PARC
estuvieron dedicados al desarrollo del lenguaje de programación Smalltalk. Todas las
aplicaciones de producción de medios y la misma GUI fueron escritos en Smalltalk. Esto
dio, como consecuencia, consistencia a todas las aplicaciones y facilitó el aprendizaje de
nuevos programas. Incluso más importante, según la visión de Kay, el lenguaje Smalltalk
permitiría a sus usuario principiantes escribir sus propias herramientas y definir sus
101 Noah Wardrip-Fruin, Expressive Processing: Digital Fictions, Computer Games, and Software
Studies (The MIT Press, 2009).
102 Alan Kay, “User Interface: A Personal View,” in The Art of Human-Computer Interface Design,
editado por Brenda Laurel (Reading, Mass,” Addison-Wesley, 1990), p. 193.

89
propios medios. Todas las aplicaciones de producción de medios, que pudieran ser
ofrecidas por la computadora, serían también ejemplos e inspiración para que los usuarios
los modificaran e escribieran nuevas aplicaciones.
Así, gran parte del artículo de Kay y Goldberg está dedicado a describir el software
desarrollo por los usuarios de su sistema: “un sistema de animación programado por
animadores”; “un sistema de dibujo y pintura programado por niños”; “una simulación de
hospital programada por un tomador de decisiones”; “un sistema de animación de audio
programado por músicos”; “un sistema de partituras musicales programado por un
músico”; “un circuito electrónico diseñado por un estudiante de secundaria”. Como se
puede ver, esta lista corresponde a la serie de ejemplos en el artículo. Kay y Goldberg
alternan deliberadamente entre tipos de usuarios (profesionales, artistas, estudiantes de
secundaria, niños) con el fin de mostrar que cualquiera puede desarrollar herramientas
con el ambiente de programación Smalltalk.
La serie de ejemplos también intercala astutamente simulaciones de medios con otros
tipos de simulaciones para enfatizar que los medios son sólo un caso particular de la
capacidad general de la computadora para simular todo tipo de proceso y sistema. Esta
yuxtaposición de ejemplos ofrece una manera interesante de pensar los medios
computacionales. Así como un científico una simulaciones para probar condiciones
diferentes y escenarios posibles, también los diseñadores, escritores, músicos, cineastas,
arquitectos pueden “probar” diferentes horizontes creativos en donde se pueden ver cómo
diferentes parámetros pueden “afectar” al proyecto. Esto último es especialmente fácil
porque hoy muchas de las interfaces de producción de medios no sólo presentan
explícitamente dichos parámetros pero además permiten al usuario modificarlos. Por
ejemplo, cuando el panel de formato de Microsoft Word muestra la tipografía usada en
determinado texto seleccionado, ésta se presentan en una columna al lado de todas las
demás tipografías disponibles. Para aplicar diferentes fuentes sólo basta con seleccionar
el nombre del menú desplegable.
El hecho de que los usuarios pudieran escribir sus propios programas era crucial para la
visión de “meta-medio” que Kay estaba inventando en PARC. Según Wardrip-Fruin, la
investigación de Engelbart seguía un objetivo similar: “la visión de Engelbart veía a los

90
usuarios creando, compartiendo y modificando sus herramientas y las de los demás” 103.
Lamentablemente, cuando se lanzó al mercado Macintosh en 1984, ésta no incluía un
ambiente de programación fácil de usar. HyperCard, escrito para Macintosh en 1987 por
Bill Atkinson (quien era un alumno de PARC), dio a los usuario la posibilidad de crear
rápidamente cierto tipo de aplicaciones, pero no tenía la versatilidad y amplitud imaginada
por Kay. Sólo en fechas recientes, con la alfabetización computacional y el desarrollo de
varios lenguajes de script (Perl, PHP, Python, ActionScript, Vbscript, JavaScript, etc.), más
gente se ha puesto a crear sus propias mediante escribiendo software. Un buen ejemplo
de ambiente de programación contemporáneo que, a mi parecer, está apegado a la visión
de Kay es Processing104. Processing está construido sobre el lenguaje Java y se ha vuelto
muy popular entre artistas y diseñadores por su estilo simple y su extensa cantidad de
librerías de medios. Se puede usar para bocetar rápidamente ideas pero también para
desarrollar programas de medios complejos. De hecho, atinadamente, los documentos de
Processing se llaman “bocetos” 105. En palabras de sus inventores y principales
desarrolladores, Ben Fry y Casey Reas, el lenguaje de Processing se enfoca “en el proceso
y creación, más que en los resultados” 106. Mencionemos otro ambiente de programación
que igualmente se vuelto muy popular: MAX/MSP y su sucesor PureData, ambos
desarrollados por Miller Puckette.
Al final del artículo de 1977, que nos ha servido como base de discusión en este capítulo,
Kay y Goldberg resumen sus argumentos en una frase, que para mí es la mejor
formulación que existe sobre lo que son, cultural y artísticamente, los medios
computacionales. Ellos llaman a la computadora un “meta-medio”, cuyo contenido es “una
amplia gama de medios existentes y aún no inventados”. En otro artículo, publicado en
1984, Kay amplía esta definición. Para cerrar este capítulo, me gustaría citarlo:
103 Noah Wardrip-Fruin, introducción a Douglas Engelbart y William English, “A Research Center for
Augmenting Human Intellect” (1968), New Media Reader, 232.
104 www.processing.org.
105 http://www.processing.org/reference/environment/.
106 http://wiki.processing.org/w/FAQ.

91
“[la computadora] es un medio que puede simular dinámicamente los detalles de
cualquier otro medio, incluyendo medios que no existen físicamente. No es una
herramienta, aunque puede actuar como muchas de ellas. Se trata del primer metamedio
y, como tal, tiene niveles de libertad para la representación y expresión nunca antes vistos
y apenas investigados” 107.
107 Alan Kay, “Computer Software,” Scientific American (September 1984), 52. Citado en Jean-Louis
Gassee, “The Evolution of Thinking Tools,” in The Art of Human-Computer Interface Design, p. 225.

92
Capítulo 2. Para entender los metamedios
“[el libro electrónico] no debe ser visto como una simulación del libro de papel porque se
trata de una nuevo medio con nuevas propiedades”.
Kay & Goldberg, “Personal Dynamic Media,” 1977.
Hoy, Popular Science, editada por Bonnier, la más grande revista de ciencia y tecnología
del mundo, lanzó Popular Science+ (la primera revista en la plataforma Mag+, disponible
para el iPad a partir de mañana)… Lo que me emociona es que no se siente como si
usáramos un sitio Web, o un lector de las nuevas tableta portátiles sino que, técnicamente,
es lo que es. Se siente como leer una revista.
Popular Science+, publicado el 2 de abril, 2010.
http://berglondon.com/blog/2010/04/02/popularscienceplus/
Los componentes básicos
Empecé a construir este libro en 2007. Hoy es 3 de abril de 2010 y estoy escribiendo este
capítulo. Hoy es también un día importante para la historia de la computación de medios
(que empieza exactamente 40 años antes con Sketchpad, de Ivan Sutherland): Apple lanzó
a la venta en EUA su nueva tableta iPad. Durante el tiempo en que he estado trabajando
en este libro, muchos desarrollos han hecho más real la visión de Kay, la computadora
como primer metamedio, pero también la han hecho más distante.
La dramática reducción de precios de laptops y el surgimiento de tabletas baratas ha
acercado la computación de medios a más y más personas (aunado al continuo
incremento de capacidades y bajo costo de dispositivos electrónicos: cámaras digitales,
video cámaras, pantallas, discos duros, etc.). Con el precio de una notebook 10 ó 20
menor que una TV digital, el argumento de la “brecha digital” de los 90s se ha vuelto
menos relevante. Se ha vuelto más económico crear tus propios medio que consumir
programas profesionales de TV vía los modos estándares de la industria de masas. Cada
vez hay más estudiantes, diseñadores y artistas que aprenden Processing y otros
lenguajes de script y programación adaptados a sus necesidades (lo que hace del arte y

93
diseño con software algo más familiar). Quizá, aún más importante, muchos teléfonos
portátiles se volvieron “teléfonos inteligentes” o “smart phones”, con acceso a Internet,
navegación Web, email, cámara de foto y video y demás capacidades de creación de
medios (incluyendo nuevas plataformas para el desarrollo de software). Por ejemplo, el
iPhone de Apple salió a la venta el 29 de junio de 2007; el 10 de julio, cuando abrió la App
Store, ya habían casi 500 aplicaciones desarrolladas por terceras personas. Según las
estadísticas de Apple, el 20 de marzo de 2010 la tienda llegó a 150 mil aplicaciones
diferentes y el número total de apps descargadas ascendía a 3 mil millones. En febrero
2012, el número de apps iOS llegó a 500 mil (sin considerar todas aquellas que Apple
rechaza diariamente), y las descargas totales fueron 25 mil millones108.
Al mismo tiempo, algunos de los mismos desarrollos fortalecieron una visión diferente de
la computación de medios (la computadora como aparato para comprar y consumir medios
profesionales, organizar archivos de medios personales y uso de GUI para creación y
producción de medios) pero no para crear e imaginar “medio aún no inventados”. Las
primer computadora Mac introducida en 1984 no permitía escribir nuevos programas para
sacar provecho de sus capacidades mediáticas. La adopción de la interfaz GUI en todas las
aplicaciones PC, por parte de la industria del software, hizo que las computadoras fueran
fáciles de usar pero también dejó sin sentido el hecho de aprender a programar. En los
2000, el nuevo paradigma de Apple de la computadora como “centro de medios” (una
plataforma para gestionar todos los medios creados personalmente) borró la parte
“computacional” de la PC. En la siguiente década, el surgimiento gradual de canales de
distribución profesionales basados en el Web, como la Apple iTunes Store (2003), TV por
Internet (en EUA, Hulu salió al mercado el 12 de marzo de 2008), tiendas de e-book
(Random House y Harper Collins empezaron a vender libros digitales en 2002) y la Apple
iBook Store (3 de abril de 2010), de la mano con lectores especializados como el Kindle de
Amazon (noviembre 2007) han ampliado una parte crucial de este paradigma. Una
computadora se volvió una “máquina universal de medios” como nunca antes (con acento
en el consumo de medios creados por otros).
108 http://www.apple.com/itunes/25-billion-app-countdown/, consultado el 5 de marzo, 2012.

94
Así que si en 1984, la primera computadora de Apple fue criticada por sus aplicaciones
GUI y la falta de herramientas de programación para sus usuarios, el iPad de 2010 fue
criticado por no incluir suficientes herramientas GUI para el trabajo pesado de creación y
producción de medios (lo que es un retroceso en la visión del Dynabook de Kay). La
siguiente reseña del iPad, del periodista Walter S. Mossberg del Wall Street Journal, era
típica de las reacciones del nuevo aparato: “si usted navega de Web, toma notas, usa
redes sociales, manda correos electrónicos y consume fotos, videos, libros, revistas,
periódicos y música, entonces éste aparato puede ser para usted” 109. David Pogue, del
New York Times, hace eco: “el iPad no es una laptop. No es muy buena para crear cosas.
Pero, por otro lado, es infinitamente mejor para consumir contenidos (libros, música, video,
fotos, Web, correo electrónico y demás)” 110.
Sin importar qué tanto las contemporáneas “máquinas universales de medios” cumplen o
traicionan la visión original de Kay, sólo son posibles gracias a ellos. Kay y Xerox PARC
construyeron la primera máquina de medios al crear un número de aplicaciones para
producirlos con una interfaz unificada y la tecnología para que fueron amplificadas por los
usuarios. Este capítulo retoma la noción de la computadora como “metamedio” para
investigar de qué manera la ha redefinido. Profundizaremos en la pregunta: ¿qué son
exactamente los medios después del software?
Desde el punto de vista de la historia de los medios, se pueden distinguir dos diferentes
tipos de medios en el metamedio computacional. El primero es la simulación antes medios
físicos existentes con nuevas propiedades, por ejemplo el “papel electrónico”. El segundo
tipo es un número de nuevos medios computacionales sin precedentes físicos. Aquí caben
los ejemplos de aquellos “verdaderos nuevos medios” enlistados con nombre y/o lugar de
invención: hipertexto e hipermedios (Ted Nelson); espacios 3D navegables (Ivan
Sutherland); multimedios interactivos ( “Aspen Movie Map” del Architecture Machine
Group).
109 http://ptech.allthingsd.com/20100331/apple-ipad-review/, consultado el 3 de abril, 2010.
110 http://gizmodo.com/5506824/first-ipad-reviews-are-in, consultado el 3 de abril, 2010.

95
Esta taxonomía es consistente con la definición de metamedio computacional anotada al
final del artículo de Kay y Goldberg. Pero hagámonos otra pregunta: ¿cuáles son los
componentes básicos de estas simulaciones de medios existentes y de nuevos medios?
De hecho, ya hemos encontrado algunos de estos componentes pero no me he referido a
ellos explícitamente.
Los componentes básicos usados en el metamedio computacional son los diferentes tipos
de datos de medios y las técnicas para generar, modificar y visualizar estos datos.
Actualmente, los tipos de datos que son más usados son los textos, las imágenes
vectoriales, las imágenes en secuencia (animaciones vectoriales), imágenes de tonos
continuos y secuencias de dichas imágenes (es decir, fotografías y video digital), modelos
3D y audio. Estoy seguro que algunos lectores preferirían una lista diferente, y no los culpo.
Lo que es importante en este punto de la discusión es establecer que hay muchos tipos de
datos y no sólo uno.
Este punto nos lleva al siguiente: las técnicas de manipulación de datos pueden ser, a su
vez, divididas en dos tipos, según el tipo de datos que pueden manejar:
A) el primer tipo son las técnicas de creación, manipulación y acceso especificas a
un tipo de datos. O sea, estas técnicas sólo se pueden usar en un tipo de datos o
tipo de “contenido de medios” en particular. Me referiré a estas técnicas como
“específicas a un medio” (aquí, la palabra “medio” significa en realidad “tipo de
dato”). Por ejemplo, la técnica de “satisfacción de restricciones geométrica”
inventada por Sutherland sólo puede actuar en data gráficos definidos por puntos
y líneas. Sin embargo, no tendría sentido aplicar esta técnica al texto. Otro
ejemplo, los programas de edición de imágenes incluyen generalmente varios
filtros como “enfoque” y “desenfoque” que pueden ejecutarse en imágenes de
tonos continuos. Pero normalmente no podríamos enfocar o desenfocar un
modelo 3D. Y los ejemplos siguen: no tendría sentido la “extrusión” de un texto o
su “interpolación”, así como tampoco lo sería definir un cierto número de
columnas en una imagen o una composición de audio.

96
Parece que algunas de estas técnicas de manipulación de datos no tienen
precedente histórico en otros medios (la “satisfacción de restricciones
geométricas” puede ser una). Otro caso son los algoritmos evolutivos usados para
generar imágenes estáticas, animaciones y formas 3D. Pero existen técnicas
específicas de medios que sí hacen referencia a herramientas y máquinas
precedentes (por ejemplo, los pinceles en aplicaciones de edición de imágenes,
comandos de zoom en software gráficos o comandos de recorte/alargamiento en
software de edición de video). En otras palabras, la misma división entre
simulaciones y “verdaderamente nuevos” medios también aplica para las técnicas
individuales detrás del “metamedio computacional”.
B) el segundo tipo son las técnicas del nuevo software que funcionan con datos
digitales en general (es decir que no son particulares a un medio). Entre los
ejemplos tenemos: “control de vistas”, hipervínculos, ordenar, buscar, protocolos
de red como HTTP, las técnicas de áreas como Inteligencia Artificial, Aprendizaje
de Máquinas, Descubrimiento de Información y demás sub-ramas de las ciencias
computacionales. De hecho, gran parte de las ciencias computacionales, ciencias
de la información e ingeniería computacional tratan sobre estas técnicas (debido
a que se enfocan en el diseño algoritmos para procesar información en general).
Estas técnicas son formas generales para manipular datos, sin importar su
contenido (valores de pixeles, caracteres de texto, sonidos, etc.). Me referiré a
estas técnicas como independientes de los medios. Por ejemplo, como ya lo
vimos, el “control de vistas” de Engelbart (la idea que una misma información
pueda ser vista de diferentes maneras) ha sido implementada en casi todos los
editores, por consiguiente actúa sobre imágenes, modelos 3D, archivos de video,
proyectos de animación, diseño gráfico y composiciones sonoras. El “control de
vistas” también ya es parte integral de los modernos sistemas operativos (Mac
OSX, Microsoft Windows, Google Chrome OS). Diario usamos control de vistas de
nuestros archivos: cambiando entre “listas” a “íconos” y “columnas”. Las técnicas
generales que son independientes de los medios también incluyen comandos de
interfaz como cortar, copiar y pegar. Por ejemplo, puedes seleccionar el nombre
de un archivo de algún directorio, un grupo de pixeles en una imagen o un

97
conjunto de polígonos en un modelo 3D, para después cortarlos, copiarlos y
pegarlos.
Muy bien. Tenemos dos formas diferentes de “dividir” un metamedio computacional. Si
queremos seguir usando el concepto de “medio”, diremos que la computadora simula
medios precedentes y permite definir nuevos. Alternativamente, podemos pensar en el
metamedio computacional como una colección de tipos de datos, técnicas específicas de
medios que sólo funcionan en tipos particulares, y en técnicas independientes a los
medios que pueden funcionar con cualquier tipo de dato. Cada uno de los medios del
metamedio computacional está hecho de estos componentes básicos. Por ejemplo, los
elementos del medio “espacio navegable 3D” son los modelos 3D y las técnicas para
representarlo en perspectiva, el mapeo de texturas, la simulación de varios efectos de
iluminación, mapeo de sombras, etc. Otro ejemplo: los elementos de la “fotografía digital”
son las imágenes de tonos continuos capturadas por sensores del lente y una variedad de
técnicas para manipular dichas imágenes: cambiar contraste y saturación, tamaño,
composición, etc.
Un comentario más sobre la distinción entre técnicas específicas y técnicas
independientes de los medios. Todo esto funciona muy bien en teoría pero en la práctica,
sin embargo, es muy difícil decir en cuál categoría debe situarse un medio o una técnica.
Por ejemplo, ¿se puede decir que la posibilidad de Sketchpad para trabajar en cualquiera
de sus 2,000 niveles de acercamiento es una extensión de técnicas precedentes (como lo
es acercar el cuerpo al lienzo o usar lupas) o más bien algo nuevo? ¿o qué podemos decir
del espacio navegable 3D que usamos como ejemplo de nuevo medio sólo posible con la
computadora (que se remonta a la realidad virtual de Sutherland en 1966)? ¿es algo
nuevo o es una extensión de medios físicos como la arquitectura, que permiten caminar al
interior de sus estructuras?
Las fronteras en “medios simulados” y “nuevos medios”, o entre técnicas “específicas a un
medio” e “independientes a un medio” no debe tomarse de forma estricta. En lugar de
pensar en ellas como categorías sólidas, es mejor imaginarlas como coordenadas del
mapa del metamedio computacional. Como cualquier primer boceto, sin importar lo

98
impreciso, este mapa es útil porque nos sirve como base para irlo modificando a medida
que avanzamos el estudio.
Técnicas independientes y específicas a los medios
Ahora que tenemos nuestro primer mapa del metamedio computacional, veamos si
podemos detectar cosas que no hayamos notado antes. Primero, observamos cierto
número de medios, viejos y nuevos, que corresponden a nuestro entendimiento de la
historia de medios (por ejemplo, si echamos un vistazo al sumario de Understanding Media
de McLuhan, encontraremos un par de docenas de capítulos dedicados a un tipo particular
de medio, de la escritura y los caminos a los carros y la TV). Segundo, vemos varias
técnicas específicas a ciertos medios, algo que también nos es familiar: pensemos en
técnicas de edición de cine, formas de bocetar un dibujo, creación de rimas en poesía, o
formas de dar narrativa a eventos cronológicos en la literatura. Pero una zona del mapa se
ve nueva y diferente con relación a nuestra previa historia cultural. Ésta es el área en
donde se hayan las “técnicas independientes a los medios”. ¿Qué son estás técnicas y
cómo pueden funcionar de manera transversal en los medios, es decir, a lo largo de
diferentes tipos de datos?
Argumentaré que la “independencia de medios” no sucede porque sí. Para que una técnica
funcione en varios tipos, los programadores deben implementar distintos métodos
apropiados para cada medio pero de forma separada. Entonces, las técnicas
independientes de los medios son conceptos generales traducidos en algoritmos que, en
efecto, funcionan en tipos de datos particulares. Exploremos algunos ejemplos.
Pensemos en el omnipresente copiar-pegar. El algoritmo para seleccionar una palabra en
un documento es diferente al algoritmo para seleccionar una curva en un dibujo vectorial,
o del algoritmo para seleccionar una parte de un imagen de tonos continuos. En otros
palabras, “copiar-pegar” es un concepto general que se implementa de forma diferente en
el software de medios, según el tipo de datos que dicho software está diseñado a soportar
(la implementación original de Larry Tesler, entre 1974 y 1975 en PARC, estaba dedicada

99
únicamente al texto). Aunque copiar-cortar-pegar sean comandos comunes a muchas
aplicaciones, éstos son implementados de formas diferente.
Buscar funciona de manera similar. El algoritmo para buscar una frase en un documento
de texto es diferentes al algoritmo que busca un rostro en una foto o clip de video (aquí
estoy hablando de “búsquedas basadas en el contenido”, es decir, búsquedas en el
contenido mismo y no en los metadatos, como los títulos, autores, etc.). Sin embargo, a
pesar de estas diferencias el concepto general de “búsqueda” es el mismo: localizar
cualquier elemento, dentro de uno o varios objetos mediáticos, que cumpla con cierto
criterio definido por el usuario. Entonces, podemos pedir a los motores de búsqueda que
no localicen aquellas páginas Web en donde existe cierta palabra, o conjunto de palabras
clave, o imágenes similares a cierta composición dada.
Como consecuencia del popular paradigma de búsqueda en el Web, podemos asumir que
podremos, en principio, hacer búsquedas en cualquier medio. En realidad es mucho más
fácil buscar datos con cierta organización modular (como texto o modelos 3D) que en los
demás (video, imágenes bitmap, audio). Pero, para al usuario, estas diferencias no son
importantes. Lo que importa es que todo tipo de medio haya adquirido la nueva propiedad
de “rastreabilidad” o “buscabilidad”.
Igualmente, a mediados de los 2000s, la foto y el video empezaron a mostrar otras
propiedades de su “encontrabilidad” (aquí tomo prestado el término “encontrabilidad” del
libro Ambient findability: what we find changes who we become111, 2005, de Peter
Morville). La apariencia para los consumidores de un medio con soporte GPS incluye: geoetiquetas,
geo-búsqueda, servicios de localización, intercambio de contenidos geográficos
(como Flickr en 2006) y aplicaciones para su gestión (como lo introdujo iPhoto en 2009).
Esto ha añadido la propiedad de “localizabilidad”.
Otra noción que se convertido, gracias al esfuerzo de muchas personas, en una técnica
“independiente de los medios” es la visualización de la información (que algunos abrevian
111 Peter Morville. Ambient Findability: What We Find Changes Who We Become. O'Reilly Media, Inc.,
2005.

100
infovis). El mismo nombre “infovis” connota que no se trata de una técnicas específica,
sino un método general que puede aplicar potencialmente a cualquier tipo de dato. Su
nombre implica que podemos usar cualquier dato de entrada (números, texto, redes,
sonido, video, etc.) y mapearlo u componerlo en una organización espacial para identificar
patrones y relaciones. Algo similar puede ser la sonificación de datos, que hace lo mismo
pero nos da como resultado sonidos.
Sin embargo, hay que decir que tomó décadas inventar técnicas que hicieron real esta
posibilidad. En los 80’s, el campo de la visualización científica se enfocaba en la
visualización 3D de datos numéricos. En la segunda parte de los 90’s, las crecientes
capacidades gráficas de las PC facilitaron que más personas experimentaran con la
visualización (llevando, al final, a diversas técnicas para visualizar medios). Las primeras
visualizaciones exitosas de grandes cuerpos textuales fueron alrededor de 1998 (Text Arc,
de W. Bradford Paley en 2002; Valence, de Ben Fry en 1999; Rethinking the Book, de
David Small en1998112), de estructuras musicales en el 2001 (The Shape of Song, de
Martin Wattenberg), y de películas en 2000 (The Top Grossing Film of All Time, 1 x 1 de
Jason Salavon).
La infovis es un ejemplo particularmente interesante técnicas independiente de los nuevos
medios debido a la variedad de algoritmos y a las estrategias de qué y cómo visualizar. Por
ejemplo, Martin Wattenberg, cuyo trabajo “se enfoca en exploraciones visuales de datos
culturalmente significativos” 113, ha creado visualizaciones de la historia del net art, de
composiciones de Bach o de Philip Glass y de procesos de razonamiento de un programa
computacional de ajedrez. Para cada caso ha debido tomar decisiones sobre qué
dimensiones de los datos debe elegir y cómo traducirlas en forma visual. Y a pesar de esas
diferencias, reconocemos dichos proyectos como visualizaciones de información. Todas
son realizaciones de un mismo concepto general: elegir ciertas dimensiones y
112 W. Bradford Paley, TextArc, 2002, http://www.textarc.org/; Ben Fry, Valence, 1999,
http://benfry.com/valence/; David Small, Rethinking the Book, tesis doctoral, 1999,
http://acg.media.mit.edu/projects/thesis/DSThesis.pdf.
113 http://www.bewitched.com/about.html, 23 de julio, 2006.

101
representarlas con elementos gráficos114. Todas están basadas en las capacidades del
software para manipular datos numéricos y mapearlos de un formato a otro. Finalmente,
todas pueden ser consideradas aplicaciones de los gráficos computacionales de los
nuevos medios (piensa en la afinidad que existe entre un modelo 3D de un rostro
escaneado y una visualización vectorial de un rostro sobre datos temporales dinámicos de
un video).
Como resultado del trabajo de Wattenberg en infovis, y de muchos otros más en las
últimas dos décadas, los datos han adquirido una nueva propiedad común: su estructura
puede ser visualizada. Esta nueva propiedad de los medios está presente en diferentes
aplicaciones, librerías de software, proyectos de arte y diseño, prototipos y artículos de
investigación. Hoy en día, algunas herramientas de visualización vienen incluidas en
software de producción de medios, por ejemplo, editores como Photoshop pueden mostrar
el histograma de una imagen, Final Cut puede visualizar el contenido cromático de un clip
de video y, algunos reproductores de música como iTunes tienen la opción de visualizar
música. El servicio Google Trends visualiza patrones de búsqueda; YouTube y Flickr
visualizan estadísticas de visitas por video y foto. Un recorrido a los miles de proyectos
recolectados por infosthetics.com, visualcomplexity.com, entre otros blogs, nos deja ver
una amplia variedad de experimentos en visualización de medios como canciones, poemas
y novelas, y todo tipo de datos, desde los movimientos en un cuarto, durante una hora, de
los hijos y el gato de un artista (1hr in front of the TV de umblebee, 2008) hasta la red de
citas de una revista científica (Eigenfactor.org de Moritz Stefaner, 2009115). Además, es
posible encontrar esos mismos proyectos en exhibiciones de arte como Info-Aesthetics116
en SIGGRAPH 2009 o Design and Elastic Mind (2008)117 y Talk to Me (2011) en el MOMA.
114 Para un estudio detallado sobre infovis, sus principios generales y nuevos desarrollos, consultar
mi artículo: “What is Visualization?” (2010), Visual Studies 26, no. 1 (Marzo, 2011).
115 http://well-formed.eigenfactor.org/;
http://www.flickr.com/photos/the_bumblebee/2229041742/in/pool-datavisualization.
116 http://www.siggraph.org/s2009/galleries_experiences/information_aesthetics/.
117 http://www.moma.org/interactives/exhibitions/2008/elasticmind/.

102
Visualización, rastreabilidad y localizabilidad: éstas y muchas otras nuevas técnicas de
independientes de los medios destacan en el mapa de los medios computacionales
precisamente porque van contra nuestra comprensión habitual de los medios (entendidos
en plural, como medios separados). Si podemos usar las mismas técnicas en diferentes
tipos de medios, ¿qué pasa con las distinciones entre medios?
La idea que las obras de arte usan distintos medios, cada uno con sus propias técnicas y
formas de representación, fue central para la estética y el arte moderno. En su Laokoon
oder Über die Grenzen der Malerei und Poesie (Laocoon: ensayo sobre los límites de la
pintura y la poesía), el filósofo alemán Gotthold Ephraim Lessing recalcaba la diferencia
radical entre pintura y poesía debido a que una se “extiende” en el tiempo y, la otra, en el
espacio. Esta idea llega a su clímax a inicios del siglo XX, cuando los modernistas
dedicaron sus fuerzas al descubrimiento de un lenguaje único a cada medio artístico. El
siguiente postulado, hecho en 1924 por Jean Epstein, un cineasta y teórico francés de la
vanguardia, es un típico ejemplo de la retórica modernista de la pureza (innumerables
postulados como ese aparecieron en publicaciones vanguardistas de ese tiempo):
Cada arte construye su ciudad prohibida, su propio territorio exclusivo, autónomo,
específico y hostil a cualquier cosa que no le pertenezca. Tan sorprendente como
parezca, la literatura debe ser primero y sobretodo, literaria; el teatro, teatral; la
pintura, pictórica; y, el cine, fílmico. Hoy la pintura se está liberando de muchas de
sus preocupaciones narrativas y representacionales… el cine debe buscar
convertirse, gradualmente y, al final, únicamente, en fílmico, usando solamente
elementos fotogénicos118.
En relación a la pintura, la doctrina de la pureza de medios alcanza su expresión máxima
con el famoso argumento de Clement Greenberg: “debido a que la superficie plana era la
única condición que la pintura no compartía con nadie más, la pintura modernista se
118 Jean Epstein, "On Certain Characteristics of Photogénie," in French Film Theory and Criticism,
Volumen 1: 1907-1929, ed. Richard Abel (Princeton: University of Princeton Press, 1988).

103
orientó hacia lo plano como a ninguna otra cosa” 119. Notemos que Greenberg no abogaba
esta postura como justificación del arte abstracto de sus contemporáneos, sólo hacía un
análisis del modernismo emergente.
No fue hasta después de los 60s, cuando la instalación (una nueva forma de arte que
combina medios y materiales) es aceptada y se vuelve popular, que la obsesión con la
especificidad de los medios pierde importancia.
Sin embargo, aún en su apogeo, la especificidad de medios siempre tuvo su contraparte.
Durante el periodo moderno también encontramos intentos “locales” (es decir, momentos
históricos particulares y escuelas artísticas) para formular principios estéticos que puedan
relacionar diferentes medios entre sí. Pensemos en los esfuerzos de muchos modernistas
para establecer líneas paralelas entre composiciones visuales y musicales. Esta trabajo ha
estado comúnmente asociado con la idea de sinestesia y Gesamtkunstwerk, con teorías,
composiciones y tecnologías de Scriabin, The Whitneys (que fueron pioneros en la
animación computacional) y varios artistas más.
Aunque los paradigmas artísticos modernistas (clasicismo, romanticismo, naturalismo,
impresionismos, realismo social, suprematismo, cubismo, surrealismo, etc.) no teorizaron
de forma explícita la estética de los medios transversales, sí se les puede considerar como
sistemas que dieron “propiedades comunes” a varios trabajos de medios. Así, novelas de
Émile Zola y pinturas de Manet fueron agrupadas en una representación “naturalista” y
científica de la vida cotidiana; el constructivismo en la pintura, la gráfica, el diseño
industrial, el diseño teatral, la arquitectura y la moda, compartieron la estética de la
“estructura expresada” (el énfasis en la composición estructural mediante su exageración);
la estética de De Stijl, con formas rectangulares, sin intersecciones, en colores primarios,
fue aplicada en la pintura, los muebles, la arquitectura y la tipografía.
¿Cuál es la diferencia entre estos trabajos artísticos, que establecieron correspondencias
mediáticas, y las técnicas de software, que funcionan en diferentes medios? Está claro que
119 Clement Greenberg, “Modern Painting,” Forum Lectures (Washington, D.C.: Voice of America:
1960), http://www.sharecom.ca/greenberg/modernism.html.

104
los sistemas artísticos y la producción, edición e interacción de técnicas de medios con
software funcionan en niveles diferentes. Los primeros son responsables del contenido y el
estilo de las obras creadas (esto es, aquello que será creado en primera instancia). Los
segundos no sólo son usados para crear, sino también para interactuar con lo ha sido
previamente creado, por ejemplo blogs, fotos y videos en el Web creados por otros.
Dicho de otra forma, los esfuerzos de los modernistas por crear similitudes entre medios
fueron prescriptivos y especulativos120. Las “propiedades comunes de los medios” podían
ser aplicadas a determinadas obras creadas por ciertos artistas, o grupos. Por el contrario,
el software impone las “propiedades” comunes de los medios en cualquier medio.
Entonces, el software determina nuestra comprensión de los medios en general. Por
ejemplo, las aplicaciones y servicios Web incluyen métodos de navegación, lectura,
escucha o visión de objetos mediáticos: añadiendo informaciones (comentarios, etiquetas,
geo-etiquetas) o encontrándolos en conjuntos más grandes (o sea, motores y comandos de
búsqueda). Esto aplica a videos, imágenes, textos, mapas, etc. De esta manera, podemos
decir que el software de medios “interpreta” cualquier medio que toca, y dichas
“interpretaciones” siempre vienen con algunos postulados.
Por supuesto que los sistemas estéticos “independientes de los medios” propuestos por
los modernistas no sólo fueron generativos, pero también interpretativos. Los artistas y
teóricos modernistas siempre querían cambiar la comprensión del arte antiguo y
contemporáneo (usualmente de forma crítica o negativa: cada nuevo grupo quería
desacreditar a sus predecesores o a sus competidores). Sin embargo, como sus programas
eran teorías y no software, no tenían ningún efecto material en la interacción de los
usuarios con las obras. En contraste, las técnicas de software afectan nuestra
comprensión de los medios mediante operaciones para producir, crear, interactuar y
compartir artefactos mediáticos.
120 Entiéndase que aquí, en lo especulativo, sucede que en muchos casos los sistemas estéticos
propuestos no siempre vieron el día en la práctica. Por ejemplo, ninguna arquitectura suprematista
de Kasimir Malevich fue construida; lo mismo para la arquitectura futurista de Antonio Sant'Elia, tal
como la presentó en sus bocetos para La Città Nuova, 1912-1914.

105
Adicionalmente, los paradigmas artísticos y estéticos, en la práctica, se podían hacer con
dos, tres o quizá cuatro medios (pero definitivamente no todos). El naturalismo se podía
ver en la literatura y artes visuales, pero no en la arquitectura; el constructivismo, por su
parte, no se expandió a la música o a la literatura. Sin embargo, los comandos cortar,
copiar y pegar se hallan en todas las aplicaciones de medios; cualquier objeto de medios
puede ser geo-etiquetado; el principio de control de vistas puede ser implementado en
cualquier tipo de medios. Para citar más ejemplos, recordemos cómo los medios
adquieren nuevas propiedades, como la “localizabilidad” y la “buscabilidad”. Cualquier
texto puede ser buscado, sin importar si fuiste tú quien lo escribió o si está en el repertorio
de las novelas clásicas del Proyecto Gutenberg. De igual forma, una parte de una imagen
puede ser cortada y pegada en otra imagen, sin importar el estilo de ellas. O sea que el
software de medios afecta de igual forma todos los contenidos de medios, sin importar la
estética, la semántica, la autoría o el origen histórico.
Para resumir esta discusión diremos que, en contraste con los programas artísticos
modernos que creaban diferentes medios con principios similares, las técnicas de
software, independientes de los medios, son ubicuas y “universales”. Por ejemplo, ya
vimos que cortar y pegar está presentes en todos los software de producción (ya sean
profesionales o para el público masivo). Además, éstas técnicas se pueden usar en
cualquier obra sin importar su estética o su autoría, es decir, sin importar que haya sido
hecha por un usuario especializado o cualquier persona. De hecho, la posibilidad técnica
de samplear un trabajo mediático creado por otros se ha vuelto la clave de la estética de
nuestra época: el remix.
Claro que no todas las aplicaciones y dispositivos de medios nos ofrecen todas estas
posibilidades de la misma manera. Por ejemplo, Google Books no permite seleccionar y
pegar texto de sus páginas. Así, aunque nuestro análisis aplique a principios técnicos y
conceptuales del software y sus implicaciones culturales, es necesario tener en mente
que, en la práctica, estos principios se borran debido a limitantes comerciales o de
derechos de autor, principalmente. Pero hasta el software más restrictivo incluye algunas
operaciones básicas de medios. Al estar presentes en todos los software diseñados para
funcionar en diferentes tipos de medios, estas operaciones establecen un entendimiento

106
compartido de los medios de hoy. Estas operaciones dan la experiencia al usuario de
“principios comunes a todos los contenidos de los medios”.
Para concluir, recordemos lo que decía hace unos párrafos: la digitalización de los medios
no pone en riesgo la diferencia entre ellos, pero sí los une de diferentes formas. Además
de mis ejemplos inspirados de Kay y Goldberg, ahora puedo nombrar unos de los
desarrollos clave al centro de esta “atracción de medios” (o sea, técnicas comunes de
software que operan a través de diferentes tipos de medios). Si recordamos que la
definición de metamedio incluye una variedad de medios existentes y nuevos, podemos
parafrasearle así: en el metamedio computacional, todos los medios, precedentes y
nuevos, comparten propiedades comunes, es decir, se basan en ciertas técnicas de
software para la gestión, producción y comunicación de datos.
Es difícil sobrevaluar la importancia histórica del desarrollo de estas técnicas mediáticas.
Los humanos siempre hemos usado estrategias generales para organizar nuestras
representaciones, experiencias y acciones culturales. Hemos usado la narrativa, la
simetría, el ritmo, la repetición de estructuras (decoraciones), los colores
complementarios, entre otras. Estas estrategias fueron muy importantes para la
percepción, cognición y memoria humana, y es por eso que las encontramos en toda
cultura y todo medio, de la poesía a la arquitectura, de la música a la poesía. Sin embargo,
estas estrategias no siempre estuvieron presentes en materiales y herramientas
tecnológicas, sino en la mente y cuerpo de artesanos que pasaban mensajes de una
generación a otra. Los medios modernos de representación y comunicación nos llevan a
otro estado. Generalmente, traen consigo ciertas técnicas que aplican a cualquier medio
que puede ser generado o capturado con ellos mismos (pensemos en la perspectiva lineal
de un solo punto, impuesta por las tecnologías de captura basadas en lentes, como la
fotografía, el cine y el video analógico). Pero esta técnicas sólo podían aplicar a un cierto
tipo de medios. Frente a estos desarrollos, la innovación en el software de medios
claramente se impone. Éstos traen un nuevo conjunto de técnicas que se usan en todos
los medios. Localizabilidad, buscabilidad, conectividad, compartir y “enviar por mensajes”
contenidos multimedia, producción, control de vistas, zoom y demás técnicas
“independientes de los medios” son virus que infectan todo lo que toca el software. Por

107
esta simple razón los podemos comprar con los principios básicos de organización de
medios y artefactos que han existido durante miles de años.
Adentro de Photoshop
Los medios contemporáneos son creados, editados, combinados, vividos y organizados
con software. Este software incluye aplicaciones profesionales de diseño, producción y
gestión de medios (Photoshop, Illustrator, Flash, Dreamweaver, Final Cut, After Effects,
Aperture, Maya); aplicaciones para el público masivo (iPhoto, iMovie, Picasa); herramientas
de los medios sociales compartir, comentar y editar (Facebook, YouTube, Video,
Photobucket); y, las numerosas apps de los dispositivos móviles. Para entender a los
medios de hoy necesitamos entender el software de los medios: su genealogía (de dónde
viene); su anatomía (interfaces y operaciones); y sus efectos teórico-prácticos. ¿De qué
manera el software de producción de medios modela los medios que crea? Tomar ciertas
decisiones de diseño parece natural y fácil de ejecutar, peo al mismo tiempo otras más
quedan ocultas. ¿De qué manera el software para visualizar, gestionar y combinar medios
afecta nuestra experiencia con el medio y con las acciones que podemos ejercer sobre
ellos? ¿De qué manera el software cambia conceptualmente a los “medios”?
Esta sección continúa la investigación de estas cuestiones mediante el análisis de una
aplicación de software que se ha vuelto sinónimo de “medios digitales”: Adobe Photoshop.
Como otros programas profesionales para la producción y diseño de medios, los menús de
Photoshop vienen con una docena de comandos separados. Si consideramos que cada
comando tiene múltiples opciones que le permiten hacer diferentes cosas, el número
completo puede oscilar fácilmente alrededor de los miles.
Esta multiplicidad de operaciones del software contemporáneo es un reto para los
Estudios de Software. Si queremos entender cómo participa el software en la modelación
de nuestros mundos e imaginaciones (todo lo que la gente imagina puede ser hecho con
software), necesitamos alguna manera de organizar estas operaciones según pequeñas
categorías para empezar a construir una teoría del software de aplicaciones. Esto no se
puede lograr si sólo nos enfocamos en las categorías de los menús superiores de las

108
aplicaciones; éstas son, en Photoshop CS6, Archivo, Edición, Imagen, Capa, Tipo,
Selección, Filtro, 3D, Vista, Ventana y Ayuda. Como cada aplicación tiene sus propias
categorías, cualquier lista que combine todas las opciones sería muy larga. Lo que más
bien necesitamos es un sistema más general.
El mapa provisional del metamedio computacional que desarrollamos en secciones previas
nos da una pista de dicho sistema. En esta sección probaremos la eficacidad de este
mapa mediante el análisis de un subconjunto de comandos de Photoshop que, de cierto
modo, definen esta aplicación en nuestro imaginario cultural: los filtros. Igualmente,
también revisaremos otra propiedad de Photoshop: las capas.
Nuestro mapa organiza las técnicas del software según dos esquemas. El primer esquema
divide estas técnicas en dos, dependiendo del tipo de datos que puede leer: 1) técnicas de
creación, manipulación y acceso a medios que son específicas a tipos particulares de
datos; 2) técnicas del nuevo software que funcionan en datos digitales en general (es decir
que no son específicas a tipos particulares de datos). El segundo esquema también divide
las técnicas en dos tipos pero bajo un criterio diferente. Lo que importa aquí son las
relaciones entre técnicas de software y las tecnologías de medios pre-digitales. En esta
taxonomía, algunas técnicas son las simulaciones de técnicas de medios precedentes pero
aumentados con nuevas propiedades y funciones. Otras técnicas, por el contrario, no tiene
equivalente previo en los medios físicos o electrónicos.
Puede ser que para un historiador de medios el segundo esquema sea más significativo,
pero ¿qué hay acerca de los usuarios que son “nativos digitales”? Estos usuarios de
software puede que nunca hayan usado otro medio que tablets, laptops o dispositivos
móviles (celulares, cámaras, reproductores MP3). También se puede que estos usuarios
no conozcan los detalles de la animación con acetatos del siglo XX, o el equipo de edición
de cine o cualquier otra tecnología de medios pre-digitales. ¿Significa esto que la distinción
entre simulaciones de herramientas de medios precedentes y las nuevas técnicas
“nacidas en lo digital” no tiene sentido para los nativos digitales y que sólo son
importantes para los historiadores de medios como yo?

109
Pienso que, mientras la semántica de esta distinción (es decir, la referencia a tecnologías y
prácticas previas) puede que no sea significativa para los nativos digitales, la distinción, en
sí, es algo que los usuarios viven en la práctica. Para entender este caso, preguntemos si
todas las técnicas de medios “nacidas en lo digital” disponibles en el software tienen algo
en común (además del hecho de haber existido antes del software, claro está).
Unos de los principales usos de las computadoras digitales, desde sus inicios, ha sido la
automatización. A partir del momento en que un proceso puede ser definido como un
conjunto finito de pasos simples (es decir, un algoritmo), una computadora puede ser
programada para ejecutar dichos pasos sin la intervención de un humano. En el caso del
software, la ejecución de cualquier comando implica una automatización de “bajo nivel”
(debido a que la computadora ejecuta automáticamente una secuencia de pasos del
algoritmo detrás del comando). No obstante, lo que es importante para el usuario es el
nivel de automatización que le ofrece la interface del comando.
Muchas técnicas de software que simulan herramientas físicas comparten una propiedad
fundamental con ellas: necesitan que el usuario las controle “manualmente”. El usuario
debe micro-manejar la herramienta, por así decirlo, dirigiéndola, paso por paso, hasta
producir el efecto deseado. Por ejemplo, debemos explícitamente mover el cursor de cierta
manera para producir una determinada pincelada con la herramienta Brocha; también, es
necesario teclear cada letra para producir una oración determinada. Por el contrario,
muchas de las técnicas que no simulan nada que existía previamente (por lo menos no de
manera obvia), ofrecen un nivel mayor de automatización del proceso creativo. Por
ejemplo, en lugar de controlar cada detalle, un usuario especifica parámetros y controles y
pone la herramienta en acción. Todas las técnicas generativas, también llamadas
“procedurales”, disponibles en el software de medio caen en esta categoría. En lugar de
crear una cuadrícula de rectángulos a mano, trazando miles de líneas, un usuario puede
especificar el alto y ancho de la cuadrícula y el tamaño de la celda, entonces el programa
se encarga de generar el resultado deseado. Otro ejemplo de alto nivel de automatización
es la interpolación de valores clave en software de animación. En una producción de
animación del siglo XX, un animador experimentado dibujaba los fotogramas clave, y
después los asistentes completaban los cuadros de en medio. El software de animación

110
automatiza este proceso con la interpolación automática de valores entre dos fotogramas
clave.
De esta manera, aunque sea de poca importancia para los usuarios que existan
herramientas que hagan cosas que antes no eran posibles u otras que simulen medios
físicos previos, la distinción es en sí algo que los usuarios viven diario. Las herramientas
que pertenecen al primer grupo muestran la posibilidad de las computadoras para
automatizar procesos; las herramientas del otro grupo usan automatización invisible de
bajo nivel tras bambalinas, pero piden al usuario una manipulación directa.
Filtro > Estilizar > Viento…
Una vez establecidos nuestros dos conjuntos de categorías de las técnicas del software
(independientes vs. específicas; simulación de lo previo vs. lo nuevo), intentemos ahora
aplicarlas a los comandos de Photoshop. Pero antes de empezar, es importante resaltar
que ambos esquemas tienen la intención de ser únicamente categorías provisionales. Sólo
nos dan un camino posible, como lo sería el norte, sur, este y oeste en un mapa en donde
podemos ubicar múltiples operaciones del diseño de software de medios. Como todo
primer boceto, no importa qué tan impreciso sea este mapa, su utilidad es que podemos
irlo modificando conforme vayamos avanzando. No se trata de que todo quepa en las
categorías de este mapa, sino de descubrir sus límites lo más pronto posible para ajustar
cambios.
Empezaremos con los filtros de Photoshop, esos comandos que están bajo el menú Filtro.
Es interesante hacer notar que una gran parte de los filtros de Photoshop también existe
en otros programas de edición de imágenes, animaciones y video, aunque a veces con
nombres diferentes. Para evitar cualquier malentendido, aquí haré referencia a los
comandos de la versión CS4 de Photoshop121.
121
Para una historia de las versiones de Photoshop, consultar:
http://en.wikipedia.org/wiki/Adobe_Photoshop_release_history.

111
La primer cosa que sobresale es que los nombres de muchos filtros de Photoshop hacen
referencia a técnicas de manipulación y creación de imágenes, así como a materiales que
existían antes del desarrollo de la aplicaciones de software de los 90s (pintura, dibujo,
bocetos, fotografía, vidrio, neón, fotocopia). Cada filtro tiene sus opciones particulares, que
pueden ser controladas con barras de progresión interactivas o con un campo de texto, en
donde insertamos el valor deseado. Estas opciones permiten controlar los efectos visuales
del filtro con un grado de precisión tal que sería muy difícil de lograr con su herramienta
equivalente física.
Este es un buen ejemplo de mi argumento anterior: las simulaciones de medios físicos
precedentes son aumentadas con nuevas propiedades. En este caso, la nueva propiedad
son los controles de filtro. Por ejemplo, el filtro Espátula ofrece tres opciones: Tamaño del
trazo, Detalle del trazo y Suavidad. Los tres toman valores del 1 al 50. Al mismo tiempo, es
importante señalar que los usuarios expertos de muchas herramientas físicas, como los
aerosoles, pueden lograr efectos imposibles para la simulación con software. Entonces, las
simulaciones de software no se deben ver como mejoras lineales sobre tecnologías de
medios precedentes.
Es posible relaciones algunos de estos filtros con medios físicos y mecánicos, como la
pintura y la fotografía, pero otros hacen referencia a acciones y fenómenos del mundo
físico que, en primera instancia, parecería que no tienen nada que ver con los medios. Por
ejemplo, el filtro Extrusión genera conjuntos de bloques o pirámides 3D, con partes de la
imagen mapeadas en sus caras. Y el filtro Onda crea un efecto de arrugas en la superficie
de una imagen.
Sin embargo, si examinamos a detalles cualquiera de estos filtros nos daremos cuenta que
las cosas no son tan simples. Tomemos, como ejemplo, el filtro Viento (en el submenú
Estilizar). Así es como la ayuda de Photoshop lo describe: “pone pequeñas líneas
horizontales en la imagen para dar el efecto de soplo de viento. Sus métodos incluyen
Viento; Ráfaga, para un efecto más dramático, y Escalonar, que compensa las líneas en la
imagen”. Todos conocemos el efecto visual de un viento fuerte sobre un ambiente (por
ejemplo, sobre un árbol o un césped), pero antes de hallar este filtro probablemente no
nos imaginábamos que podíamos “ventiscar” una imagen. ¿Debemos entender el nombre

112
de este filtro como una metáfora? ¿o más bien debemos pensar en él como un ejemplo de
“doblez” conceptual (que es la forma en como varios conceptos del lenguaje natural se
forman, según la teoría de la Flexión de conceptos122) en donde “viento” más “imagen”
dan como resultado un nuevo concepto actualizado en las operaciones dl filtro Viento?
Esta situación se complica aún más con el hecho de que, al aplicar un filtro Viento a una
imagen, el resultado se vea muy diferentes a lo que realmente pasa con el viento sobre un
árbol o césped. Sin embargo, sí es similar a la fotografía de una escena ventosa, a
condición que haya tenido una larga exposición. Entonces, podemos pensar en el nombre
de Viento tanto como metáfora (para imaginar lo que un algoritmo de transformación
puede hacer a una imagen) como simulación de una técnica fotográfica particular (la larga
exposición). O sea que, aunque su nombre haga referencia al mundo físico, sus
operaciones actuales pueden referirse a tecnologías de medios pre-digitales.
¿Existen filtros “nacidos en lo digital”?
Sigamos con la exploración de los filtros de Photoshop. La gran mayoría de ellos hace
referencia a medios físicos previos o a experiencias del mundo físico (por lo menos en lo
que respecta a su nombre). Sólo algunos no lo son. Entre otros, estos filtros serían: Paso
alto, Mediana, Reducir ruido, Enfocar y Ecualizar. ¿Estamos frente filtros “nacidos en lo
digital”? En otras palabras, hemos llegado a ejemplos de “nuevas” técnicas de medios? La
respuesta es: no. Como resulta, todos estos filtros son también simulaciones de software
que se refieren a cosas que ya existían antes de las computadoras digitales.
A pesar de que se trata de un conjunto relativamente pequeño, comparado con la extensa
colección de filtros de Photoshop, se trata de filtros clave para las tecnologías electrónicas,
informáticas y de telecomunicaciones. Además, no sólo se pueden usar en imágenes
digitales sino también en cualquier tipo de datos: sonidos, transmisiones de TV, datos
capturados por un sensor ambiental, datos de un aparatos de imagen médica, etc.
122
Consultar Mark Turner y Gilles Fauconnier, The Way We Think: Conceptual Blending and the
Mind's Hidden Complexities (New York: Basic Books 2002).

113
En su versión Photoshop, estos filtros trabajan en imágenes de tonos continuos (bitmap),
pero como pueden ser usados en otros tipos de señales, en realidad pertenecen a la
categoría técnicas “independientes de los medios”. Son técnicas generales que se
desarrollaron primero en la ingeniería y luego en las ciencias computacionales para el
procesamiento de información. La aplicación de estas técnicas en las imágenes forma
parte del campo del procesamiento de imágenes, definido como “cualquier forma de
procesamiento de información que tiene como entrada una imagen, como las fotografías y
el video” 123. Esta relación conceptual entre procesamiento de “información” y de
“imágenes” ejemplifica un argumento de este libro: en la cultura del software, los “medios
digitales” son un conjunto particular de una categoría más amplia, que es la “información”.
Como estos filtros, muchas de las “nuevas” técnicas para la creación, producción y
análisis de medios implementadas en el software no fueron hechas para funcionar en
datos de medios. Más bien, fueron hechas para el procesamiento de información y de
señales en general, y después fueron adaptadas o heredadas en los medios. Entonces, el
desarrollo de software acerca diferentes tipos de medios debido a que las mismas
técnicas son usadas por todos ellos. Al mismo tiempo, hoy los “medios” están relacionados
con todos los demás tipos de información, ya sean datos financieros, patentes, resultados
de experimentos científicos, etc.
Ésta es una de las dimensiones teóricas más importantes en el paso de los tecnologías de
medios físicos y mecánicos a los medios electrónicos y al software digital. Antes, los
herramientas de medios físicos y mecánicos eran usadas para crear contenidos accesibles
directamente mediante los sentidos humanos (con excepciones notables, como el código
Morse) y por ello las posibilidades de cada herramienta estaban determinadas por aquello
que era significativo a un sentido dado. Un pincel podía hacer trazos con color, grosor y
forma (propiedades directamente asociadas con la vista y el tacto). De forma similar, los
ajustes de una cámara fotográfica afectaban el enfoque y contraste de las fotos
capturadas (visión humana). Una forma diferente de expresar esto era decir que el
“mensaje” no estaba codificado de ninguna manera: era creado, almacenado y accedido
123
Ibid.

114
en su forma nativa. Así que, si tuviéramos que redibujar el famoso diagrama del sistema
de comunicación de Claude Shannon (1948) para la era pre-electrónica, deberíamos
borrar las etapas de codificación y decodificación124.
Las tecnologías de medios basadas en la electrónica, como el telégrafo, el teléfono, la
radio, la televisión y, más tarde, las computadoras digitales, usan la codificación de
mensajes (o “contenido”). Y esto hace posible la idea de “información”: una dimensión
abstracta y universal de cualquier mensaje, separado de su contenido. En lugar de
aplicarse directamente en sonidos, imágenes, cine o texto, los aparatos electrónicos y
digitales operan en el continuo de señales y datos numéricos discretos. Algunos ejemplos
son: modulación, suavidad (reducción de diferencias en los datos) y enfoque (exageración
de las diferencias). Si los datos son discretos, se pueden hacer otras operaciones como
búsqueda y clasificación.
Con la llegada de la codificación, tenemos un nuevo nivel de eficiencia y velocidad en el
procesamiento, transmisión e interacción con los datos de medios y el contenido de la
comunicación. Así se explica en parte porque los medios electrónicos y digitales
remplazaron otras herramientas específicas a ciertos medios y máquinas. Éstas
operaciones automatizan procesos que van desde la reducción de tamaño y la banda
pasante, la mejora de la calidad y la distribución en redes de comunicación.
El campos del procesamiento de imágenes digitales se empezó a desarrollar en la segunda
parte de los 50’s, cuando científicos y militares se dieron cuenta que las computadoras
podían analizar e mejorar automáticamente la calidad de imágenes aéreas y satelitales.
Otros usos pioneros incluyen el reconocimiento de caracteres y la conversiones estándares
de telefotografías125. Como parte de este desarrollo, ésta área tomó filtros básicos que
124
C.E. Shannon, "A Mathematical Theory of Communication", Bell System Technical Journal 27
(Julio, 1948): 379-423; (Octubre, 1948): 623-656.
125
A pesar de que el procesamiento de imágenes es un área muy activa de las ciencias
computacionales, no conozco algún libro o artículo que trace su historia. El primer libro sobre
procesamiento de imágenes fue Azriel Rosenfeld, Picture Processing by Computer (New York:
Academic Press, 1969).

115
eran comunes en las electrónica y los adaptó en las imágenes digitales. Los filtros
Photoshop que automáticamente mejoran la apariencia de una imagen (por ejemplo,
aumentar el contraste o reducir ruido), vienen directamente de este periodo (finales de los
50’s e inicios de los 60’s).
En resumen, los filtros de Photoshop que parecen “nacidos en lo digital” (o “nativos del
software”) tienen sus predecesores físicos en los filtros análogos, que fueron usados en el
teléfono, la radio, el telégrafo, los instrumentos de música electrónica y demás tecnologías
de la primera mitad del siglo XX. Y todo esto ya estaba siendo estudiado en el
procesamiento de señales antes de pasar a la imagen digital.
Filtro > Distorsionar > Onda…
El reto que implica decidir en qué categoría debemos ubicar los filtros de Photoshop sigue
vivo si continuamos explorando dicho menú. Como lo dijimos anteriormente, los esquemas
para clasificar técnicas de software son un mapa inicial para empezar la discusión.
Las dificultades para categorizar tal o tal técnica están directamente relacionadas con la
historia de las computadoras digitales, vistas como máquinas de simulación. Cada
componente de los medios computacionales viene de algún lugar fuera de las
computadoras. Esto es cierto no sólo para una parte significante de técnicas de producción
de medios (filtros, lápices y pinceles digitales, Diseño Asistido por Computadora,
instrumentos musicales virtuales, teclados, etc.), sino también para las operaciones
computacionales más básicas (clasificar, buscar, organizar). Cada una de estas
operaciones y estructuras puede, conceptual e históricamente, remontarse a operaciones
físicas y mecánicas de datos, conocimiento y gestión de memoria, que ya existían antes de
los años 40. Por ejemplo, los “archivos” y “fólder” de computadora hacen referencia a sus
predecesores de papel, que ya eran un estándar en las oficinas. Las primeras
computadoras comerciales de IBM fueron promocionadas como equivalentes más rápidos
de otros equipos de IBM vendían desde hace décadas: calculadoras electro-mecánicas,
tabuladoras, clasificadores, etc. Sin embargo, cuando cualquier operación o estructura fue
simulada por la computadora, también fue mejorada y aumentada. Este proceso de

116
transferencia, del mundo físico al computacional, sigue existiendo, pensemos en la interfaz
táctil popularizada por el iPhone (2007). Entonces, mientras Alan Turing definió la
computadora como una máquina general de simulación, si consideramos sus usos y
desarrollos subsecuentes, es más apropiado pensar en la computadora como una
máquina de simulación y de aumento. La dificultad de clasificar diferentes técnicas del
software de medios es resultado directo de este paradigma subyacente al desarrollo de
software desde sus inicios.
El cambio de herramientas y materiales de medios físicos en algoritmos diseñados para
simular sus efectos tiene otra consecuencia importante. Como hemos visto, algunos filtros
de Photoshop hacen referencia explícita a medios artísticos previos; otros hace referencia
a diversas efectos, acciones y objetos físicos (Molinete, Extrusión, Viento, Resplandor
difuso, Rizo, Vidriera, Onda, Grano, Retazos, Encoger, etc.). Pero en ambos casos, al
cambiar los valores de los controles de cada filtro, podemos alterar significativamente el
efecto visual, yendo a una dimensión que ya no es familiar. Podemos usar un mismo filtro
para lograr un efecto parecido a su contraparte física, pero también para lograr algo
completamente diferente a lo que existe en la naturaleza o en los medios viejos, sólo
posible con algoritmos de manipulación de pixeles. Lo que empieza con una referencia al
mundo físico, usando los ajuste por default, se vuelve en algo extraño si alteramos los
ajustes de un simple parámetro. En otras palabras, muchos algoritmos simulan los efectos
de herramientas, máquinas, materiales y fenómenos físicos con sus ajustes predefinidos,
pero cuando los cambiamos, ya no funcionan como simulaciones.
Por ejemplo, analicemos el comportamiento del filtro Onda (en el submenú Distorsionar).
Este filtro hace referencia a un fenómeno físico conocido y, de hecho, puede producir
efectos visuales que podemos llamar “ondas”. Esto no quiere decir que el efecto de este
filtro deba tener una similitud exacta con el significado literal de la palabra onda, definida
según el diccionario como: “un disturbio en la superficie de un cuerpo líquido, como el mar
o los lagos, en forma de cadena en incremento” 126. En nuestro lenguaje coloquial, usamos
la palabra “ondear” metafóricamente para referirnos a algún movimiento periódico
(“ondear la bandera”) o a alguna forma estática que asemeje una onda o turbulencia
126 http://dictionary.reference.com/browse/wave, Agosto 1, 2012.

117
(“hacer ondas”). Según la influyente teoría del lingüista cognitivo George Lakoff, éste uso
metafórico no es una excepción sino la norma en el lenguaje y pensamiento humano.
Lakoff propuso que la mayoría de nuestros conceptos abstractos son proyecciones
metafóricas de experiencias sensomotoras de nuestro cuerpo y el mundo físico127. “Hacer
ondas”, y otras metáforas derivadas de nuestra experiencia perceptiva de ver ondas
reales, ejemplifica este mecanismo general del lenguaje.
De acuerdo con la teoría de metáforas de Lakoff, algunos detalles del filtro Onda (así como
demás filtros inspirados del mundo físico) se pueden entender como proyecciones
metafóricas similares. Dependiendo de la selección de valores en los parámetros, este
filtro puede producir efectos que asemejan nuestra experiencia perceptiva de ondas física
reales o nuevos efectos relacionados con las ondas, metafóricamente.
El filtro genera ondas según la función del seno (y = seno x); luego las añade y usa el
resultado para distorsionar una imagen. El usuario puede controlar el número de ondas
con el parámetro Número de generadores. Si este número es 1, el filtro genera una sola
onda seno. Al aplicar esta función a una imagen, la distorsiona siguiendo un patrón de
variación periódico. O sea que el filtro genera un efecto que, efectivamente, parece una
onda.
Sin embargo, en algún punto, la conexión metafórica con el mundo real se rompe y en este
caso la teoría de Lakoff ya no nos ayuda. Si aumentamos el número de generadores
(podemos ir hasta 999), el patrón que produce el filtro ya no parece periódico y ya no se
puede relacionar con las ondas reales, ni siquiera metafóricamente.
La razón del comportamiento de esta filtro está en su implementación. Como ya lo hemos
explicado, cuando el generador está puesto en 1, el algoritmo genera una sola función
seno. Si está en 2, se generan dos funciones y así sucesivamente. Los parámetros de cada
función son seleccionados de forma aleatoria dentro de la selección del usuario.
127
George Lakoff y Mark Johnson, Metaphors We Live By (Chicago: University of Chicago Press,
1980).

118
Si seleccionamos un número bajo de generadores (del 2 al 5, por ejemplo), a veces los
valores aleatorios que se añaden al resultado siguen haciendo que parezca una onda, pero
en otros casos no. Y si incrementamos el número de funciones, el resultado nunca parece
una onda.
El filtro Onda puede crear prácticamente una variedad infinita de patrones abstractos, y
muchos de ellos no son periódicos de forma obvia, es decir que ya no son reconocidos
visualmente como “ondas”. Lo que son es el resultado de un algoritmo computacional que
usa fórmulas y operaciones matemáticas (generar y añadir funciones seno) para crear un
vasto espacio de posibilidades visuales. Así, aunque el filtro se llama “Onda”, sólo una
pequeña parte de su espacio corresponde a efectos semejantes al mundo físico.
Lo mismo puede decirse de muchos otros filtros que remiten a medios físicos. Los filtros
del submenú Artístico y del submenú Textura producen simulaciones bastante precisas
con sus ajustes pre-definidos pero, cuando los alteramos, el resultado puede ser patrones
muy abstractos.
La operación de filtros de Photoshop tiene consecuencias teóricas importantes.
Mencionamos antes que las herramientas de software que simulan instrumentos físicos
(pinceles, plumas, reglas, gomas, etc.) necesitan un control manual, mientras que las
otras, que no tienen referente previo, permiten un nivel mayor de automatización. Un
usuario introduce los valores del parámetro y el algoritmo crea el resultado deseado.
El mismo “alto nivel” de automatización se halla en las técnicas de software “generativo”
(o “procedural”) que se usan actualmente. Estos trabajos generados con algoritmos van de
los gráficos fijos y animados a la arquitectura y los juegos (de las visualizaciones y
animaciones en tiempo real de la artista de software Lia128 a los mundos masivos
generados proceduralmente en el videojuego Minecraft). Otros proyectos generativos usan
algoritmos ara crear automáticamente formas complejas, animaciones, formas espaciales,
música, planos arquitectónicos, etc. (una buena selección de trabajos generativos e
interactivos puede verse aquí: http://www.processing.org/exhibition/). Debido a que
128
http://www.liaworks.com/category/theprojects/.

119
muchas obras creadas con algoritmos generativos son abstractas, los artistas y teóricos
las oponen a aquellas hechas con Photoshop y Painter, usados por ilustradores y
fotógrafos comerciales para lograr realismo y figuración. Además, como éstas últimas
aplicaciones simulan modelos de creación que son más manuales, también son vistas
como menos “específicas a los nuevos medios” que el software generativo. Al final, ambas
críticas llevan a decir que el software que simula “medios viejos” es más conservador,
mientras que los algoritmos y obras generativas son vistas como progresistas debido a que
son única a los “nuevos medios”. Cuando la gente dice que las obras que requieren
escribir código computacional califican como “arte digital” y aquellas que sólo usan
Photoshop, evocan los mismos argumentos anteriores.
Sin embargo, las técnicas de software que simulan medios previos y aquellas que son
explícitamente procedurales son parte del mismo continuo al momento en que se
implementan en aplicaciones como Photoshop. Como vimos con el filtro Onda, el mismo
algoritmo puede generar una imagen abstracta o realista. De igual forma, los algoritmos de
sistemas de partículas son usados ampliamente la producción de películas para generar
explosiones realistas, fuegos artificiales, parvadas de pájaros y demás fenómenos físicos.
Otro ejemplo: técnicas procedurales usadas en diseño arquitectónico para crear
estructuras espaciales abstractas también son usadas en videojuegos para generar
ambientes 3D realistas.
Menús Historia y Acciones
Esta discusión sobre filtros Photoshop empezó con la intención de comprobar la utilidad de
los dos esquemas de clasificación de técnicas de software en software de medios: 1)
técnicas independientes vs específicas de los medios; 2) las simulaciones de herramientas
previas vs técnicas que no simulan explícitamente medios previos. El primer esquema
llama nuestra atención porque las aplicaciones de medios comparten algunos géneros, por
así decirlo, mientras que también ofrecen técnicas que funcionan en tipos particulares de
datos. El segundo esquema es útil si queremos entender las técnicas de software en
términos de su genealogía y su relación con medios previos, físicos, mecánicos y
electrónicos.

120
La discusión anterior resaltó casos difíciles de distinguir y otros en donde las divisiones
son claras. Para recordar, el filtro Trazos de pincel se inspira claramente de herramientas
de medios físicas, mientras que Añadir ruido no. Los comandos Copiar y Pegar son
ejemplos de técnicas independientes de los medios. Auto-contraste y Reemplazar color son
ejemplos de técnicas específicas a ciertos medios.
Pero a pesar de éstas distinciones, todas las técnicas de creación, producción e
interacción de medios comparten rasgos adicionales que no hemos discutido.
Conceptualmente, estos rasgos son diferentes a las técnicas independientes de los
medios, como copiar y pegar. ¿Qué son entonces?
Sin importar si hacen referencia o no a instrumentos, acciones o fenómenos previos, las
técnicas de medios disponibles en el software son implementadas como funciones y
programas de software. En consecuencia, siguen los principios generales de la ingeniería
de software moderna. Adicionalmente, sus interfaces siguen convenciones establecidas en
todos los demás software de aplicación (ya sean hojas de cálculo, gestión de inventarios,
análisis financieros o diseño Web). Se les dan amplios controles numéricos, sus
preferencias se pueden guardar y volver a cargar después, su uso queda grabado en la
ventana de Historia, se pueden usar automáticamente mediante la grabación y
reproducción de Acciones. Dicho de otra forma, adquieren completa funcionalidad del
ambiente de software moderno, que es muy diferente a los medios y máquinas físicas
previas. Debido a estos principios compartidos de implementación, todas las aplicaciones
de software son como especies que pertenecen a la misma familia evolutiva, siendo el
software de medios una rama del árbol129.
Los pioneros del software de medios querían extender las propiedades de las tecnologías y
herramientas de medios que estaban simulando en la computadora. Se trataba de crear
un “un nuevo medio con nuevas propiedades”, como lo dijeron Kay y Goldberg. Así, las
técnicas de software que se refieren a procesos y técnicas previas son también “nuevos
129
La biología contemporánea ya no emplea la idea un árbol evolutivo. El concepto de “especie” se
ha vuelto también problemático. Aquí sólo usamos estos términos como metáfora.

121
medios” porque se comportan de forma muy diferente a sus predecesores. Hoy tenemos
una razón de más para apoyar esta conclusión. Las nuevas funcionalidades (múltiples
niveles de zoom, copiar, pegar, buscar, etc.) y las convenciones estándar de la interfaz
(controles, pre-visualización, historia) distinguen entre la herramienta de simulación de
medios más “realista” de medios de su predecesor.
Esto significa que usar cualquier software de creación y producción de medios es usar
“nuevos medios”. O, para desarrollar mejor esta frase: todas las técnicas y herramientas
de medios disponibles en el software de aplicación son “nuevos medios”, sin importar que
alguna de ellas haga referencia a medios, fenómenos o tareas comunes que existían
antes de convertirse en software. Escribir con Microsoft Word es usar nuevos medios.
Tomar fotos con una cámara digital es usar nuevos medios. Aplicar el filtro Nubes (en el
submenú Interpretar), que usa un algoritmo automático para generar texturas similares a
las nubes, es usar nuevos medios. Dibujar trazos con la herramienta Pincel es usar nuevos
medios.
En otras palabras, no importa en dónde cataloguemos las técnicas, todas ellas son
instancias de un tipo de tecnología: el software de aplicaciones interactivas. Como ya lo
dijeron Kay y Goldberg en 1977, el software interactivo es cualitativamente diferente de
todos los medios previos. En los últimos 30 años, éstas diferencias se han vuelto más
claras. La interactividad; la personalización; la posibilidad de simular y crear nuevos
medios y tecnologías de información; el procesamiento en tiempo real de grandes
cantidades de datos; el control e interacción con sensores y otras máquinas; el soporte de
colaboración asincrónica en tiempo real… éstas, y muchas más, son las posibilidades que
abre el software moderno (de la mano con el middleware, hardware y las redes) y que lo
separan de todos los medios precedentes.
La paleta de las Capas
Para nuestro análisis final, nos moveremos fuera del menú de Filtro y examinaremos una
de las características clave de Photoshop, que originalmente lo diferenció de otros editores
de medios para “consumidores”: la paleta de Capas. Las propiedad de las Capas fue

122
introducida en Photoshop 3.0, en 1994130. Citando la Ayuda de Photoshop, “las Capas
permiten trabajar con un elemento de una imagen si interrumpir los demás” 131. Desde la
perspectiva de las teoría de medios, sin embargo, las Capas son mucho más que eso. Han
redefinido cómo se crean las imágenes e incluso el significado mismo de “imagen”. Lo que
antes era un todo indivisible se ha vuelto una composición hecha de partes separadas.
Esto es un apunte teórico y, al mismo tiempo, la realidad del diseño profesional y edición
de imágenes en nuestra sociedad de software. Cualquier diseño profesional creado con
Photoshop usa múltiples capas (en Photoshop CS4, una imagen puede tener
potencialmente miles de capas). Las capas pueden ser invisibles, pueden jugar el rol de
contenedores de otros elementos y pueden ser diferentes versiones de un mismo
elemento. Un diseñador puede controlar la transparencia de cada capa, agruparlas,
cambiar su orden, etc.
Las capas modifican la manera en que un diseñador o ilustrador piensan las imágenes. En
lugar de trabajar en un solo diseño (que se altera inmediatamente de manera irreversible),
ahora trabaja con una colección de elementos independientes. Puede jugar con ellos,
borrarlos, editarlos, importar otros, etc., hasta que obtenga un composición final que le
satisfaga (o un grupo de versiones ordenadas en grupos de capas). El contenido y las
preferencias de todas las capas queda guardado en un archivo de imagen, así puede
cerrarlo y regresar más tarde para seguir trabajando.
Las capas también tienen otras funciones. Citando una vez más la Ayuda en línea de
Photoshop: “En ocasiones, las capas no tienen un contenido aparente. Por ejemplo, una
capa de ajustes tiene preferencias de color y tono que afectan a las capas que se
encuentran abajo. En lugar de modificar directamente los pixeles, es posible editar una
capa de ajuste y dejar los pixeles intactos” 132. En otras palabras, las capas pueden tener
operaciones de edición, que se pueden activar o desactivar, o reorganizar en otro orden.
Una imagen queda entonces redefinida como una composición provisional de elementos y
operaciones de modificación, conceptualmente separadas entre sí.
130
http://en.wikipedia.org/wiki/Adobe_Photoshop_release_history.
131 http://help.adobe.com/en_US/Photoshop/11.0/, Octubre 9, 2011.
132
Ibid.

123
Podemos comparar este cambio fundamental en el concepto y práctica de la creación de
imágenes con un cambio similar que tuvo lugar en los mapas; de los mapas en papel los
Sistemas de Información Geográfica (GIS). Así como todos los profesionales de los medios
usan Photoshop, hoy la mayoría de los profesionales en contacto con espacios físicos
(oficiales, compañías de petróleo, mercadólogos, equipos de emergencia, geólogos,
oceanógrafos, agencias de seguridad, policía, etc.) usa sistemas GIS. El software de mapas
disponible al público, como Google Maps, MapQuest y Google Earth, puede ser visto como
una versión simplificada de los GIS ya que no ofrecen opciones cruciales para el análisis
profesional de espacios (por ejemplo, cruzar datos demográficos, de viaje, de gastos, para
determinar la mejor ubicación de un nuevo supermercado).
Los GIS “capturan, almacenan, analizan, gestionan y presentan datos relacionados con la
ubicación” 133. El concepto central de los GIS son paquetes de capas de datos unidos por
coordenadas espaciales. Queda obvia la conexión conceptual entre el uso de capas en
Photoshop y el de otras aplicaciones de software (aunque los GIS funcionan con cualquier
dato que tenga coordenadas geoespaciales, en lugar de imágenes exclusivamente). Las
coordenadas geoespaciales alinean diferentes conjuntos de datos. Los profesionales
construyen “mapas” con software GIS que tienen cientos o incluso miles de capas. La
representación de capas también existe en aplicaciones para el público como Google
Earth, pero la diferencia es que las aplicaciones profesionales, como ArcGIS, el usuario
puede crear su propio mapa de capas a partir de cualquier fuente de datos, mientras que
en Google Earth los usuarios sólo pueden añadir sus propios datos a la representación de
base de la Tierra, que Google provee y que no puede ser modificada.
Para los GIS, el espacio funciona como una plataforma de medios que puede contener
todo tipo de datos al mismo tiempo: puntos, líneas 2D, mapas, imágenes, video, datos
numéricos, texto, ligas. Otros tipos de estas plataformas también incluyen bases de datos,
páginas Web y espacios digitales 3D. En Photoshop, las capas siguen subordinadas
conceptualmente a la imagen final (cuando usas la aplicación, continuamente interpreta
todos las capas visibles para mostrar ésta imagen). Así que, a pesar de que podemos usar
133 http://en.wikipedia.org/wiki/GIS, Octubre 9, 2011.

124
un documento Photoshop como una especia de base de datos (una manera de coleccionar
diferentes elementos de imágenes), éste no es el uso ideal (se supone que debes usar
otros programas como Adobe Bridge o Aperture). Los GIS llevan la idea de la
representación basada en capas un paso más allá. Puede ocurrir que las aplicaciones GIS
profesionales no produzcan un mapa con todos los datos. Más bien, un usuario elige los
datos que necesita para trabajar y realiza diversas operaciones con ellos (prácticamente
esto significa seleccionar un subconjunto de capas de datos disponibles). Si un mapa
tradicional ofrece una representación fija, los GIS son un sistema de información: una
manera de manejar y trabajar con grandes cantidades de datos separados pero vinculados
(vía un sistema compartido de coordenadas).
De las técnicas de programación a la composición digital
¿Cuál es el origen conceptual de las capas en Photoshop? ¿a cuál categoría pertenecen
las capas, según nuestra taxonomía de técnicas de medios basadas en software? Al
pensar en los orígenes de este concepto y la manera en que se relaciona con otras
técnicas de producción de medios, me lleva a varias direcciones. Primero, las capas no son
específicas a editores de imágenes bitmap, como Photoshop; ésta técnica también se usa
en software de imágenes vectoriales (Illustrator), gráficos animados y composición (After
Effects), editores de video (Final Cut), y editores de audio (Pro Tools). En programas
orientados a datos de tiempo (audio, video, animación), las capas son conocidas como
“canales” o “pistas”. Dichos términos indican medios físicos y electrónicos que han sido
implementados en software (switcher análogos de video, grabadoras multi-pista de audio).
A pesar del cambio de vocabulario, las funcionalidad de la técnicas es la misma: una
composición final es el resultado de “añadir” datos (componerlos, técnicamente)
almacenados en diferentes capas, canales o pistas.
La Ayuda de Photoshop explica las Capas de la siguiente manera: “son como hojas de
acetato apiladas. Se puede, ver a través de las áreas transparentes de una capa, las otras
capas que están abajo” 134. Aunque no se haga referencia explícita, se trata de la técnica
134
http://help.adobe.com/en_US/Photoshop/11.0/, Octubre 9, 2011.

125
estándar de la animación comercial del siglo XX conocida como animación de celdas (o
tradicional, o a mano). De la misma manera que una cámara montada sobre su trípode,
Photoshop está “grabando” continuamente la imagen a través de la yuxtaposición de los
elementos visuales de las capas individuales.
No es asombroso que las Capas de Photoshop estén relacionadas con las técnicas de
animación tradicional de inicios del siglo XX, así como con otras prácticas tales como
exposición, proyección de fondo, mates de cine, perforación de video135. De la misma
forma, hay un fuerte vínculo entre las Capas y la tecnología musical, como son las pistas y
la grabación en múltiples de ellas. El inventor de esta técnicas fue el guitarrista Les Paul,
que en 1953 fue comisionado por Ampex para construir el primer grabador de 8 canales.
En los 60’s, las grabadoras multi-pista ya eran usadas por Frank Zappa, los Beach Boys y
The Beatles. Desde ese momento, el uso de múltiples pistas se volvió un estándar en la
grabación y arreglo musical136. Cuando las grabadoras se simularon en software dejaron
de ser caras y espaciosas pero sobretodo se volvieron disponibles en muchas aplicaciones.
Por ejemplo, desde 2004, todas las computadoras Apple vienen con el grabador y editor
multi-pista Garage Band. Otras implementaciones populares son Audacity (libre) y Pro Tools
(comercial y profesional).
Finalmente, hay un indicio que une las Capas con el principio general de la programación
de computadoras modernas. En 1984, dos computólogos, Thomas Porter y Thomas Duff,
que trabajan para Industrial Light and Magic (ILM), la filial de efectos especiales de
Lucasfilm, definieron formalmente el concepto de composición digital en un artículo
presentado en SIGGRAPH137. El concepto surgió del trabajo en curso en Star Trek II: The
Wrath of Khan (1982). La idea clave era interpretar cada elemento independiente con un
135
El capítulo “Compositing”, de mi libro anterior, presenta una “arqueología” de la composición
digital y discute los vínculos entre ellas. Lev Manovich, The Language of New Media (The MIT Press,
2001).
136
Ver http://en.wikipedia.org/wiki/Multitrack_tape_recorder y
http://en.wikipedia.org/wiki/History_of_multitrack_recording.
137 Thomas Porter y Tom Duff, “Compositing Digital Images,” Computer Graphics vol. 18, no. 3 (Julio,
1984): 253-259.

126
canal mate que tuviera información de valores de transparencia. Esto permitía a los
cineastas trabajar en cada elemento de forma separada y luego combinarlos en una
escena 3D realista.
El artículo de Porter y Duff hace la analogía entre crear una escena final mediante la
composición de elementos 3D y ensamblar módulos de códigos independientes para
completar un programa computacional. Como Porter y Duff lo explican, la experiencia de
escribir software de esta manera los llevó a considerar la misma estrategia para hacer
imágenes y animaciones. En ambos casos, las partes pueden ser re-usadas para hacer
nuevas producciones:
La experiencia nos ha enseñado a descomponer grandes cuerpos de código fuente en
módulos separados con el fin de ahorrar tiempo de compilación. Un error en alguna rutina
obliga únicamente a recompilar ese módulo y rápidamente volver a cargar el programa
completo. Igualmente, pequeños errores en coloración o diseño en algún objeto no deben
obligar a “recompilar” la imagen completa138.
La misma idea de tratar una imagen como una colección de elementos que pueden ser
cambiados independientemente y re-ensamblados en nuevas imágenes está al fondo de
las Capas de Photoshop. Es importante ver que Photoshop fue desarrollado en el mismo
lugar en que había sido definida la composición digital años antes. Los hermanos Thomas
y John Knoll escribieron la primera versión del programa cuando Thomas hizo una pausa
de seis meses de sus estudios doctorales en la Universidad de Michigan en 1988 y se unió
a hermano que estaba trabajando en ILM.
Este vínculo entre una popular técnica de software para la manipulación de imágenes y un
principio general de la programación es muy notable. Es un ejemplo perfecto de cómo los
elementos de un ecosistema de software de medios modernos (aplicaciones, formatos de
archivos, interfaces, técnicas, herramientas, algoritmos usados para crear, visualizar y
compartir contenido mediático) no tienen un solo padre sino dos, cada uno su propio DNA.
Por un lado las prácticas culturales y mediáticas. Por otro, el desarrollo de software.
138
Ibid.

127
En resumen, a través del trabajo de muchas personas, de Ivan Sutherland a inicios de los
60’s a los equipos de ILM, Macromedia, Adobe, Apple y otras compañías de los 80’s y
90’s, los medios se vuelven software (con todas las implicaciones teórico-prácticas que
esta transición implica). Aquí hemos hablado de los filtros de Photoshop y el menú de
Capas para discutir algunas consecuencias, pero hay mucho más que debe ser
descubierto.
Solamente hay software
¿Qué son exactamente los “nuevos medios” y qué medida son diferentes de los “viejos
medios”? Académicos, artistas y periodistas han escrito ampliamente sobre el tema desde
los 90’s. En muchas discusiones, un solo término ha representado todo el rango de las
nuevas tecnologías, de las nuevas posibilidades de expresión y comunicación, de las
nuevas formas de comunidad y socialización que surgen con las computadoras e Internet.
Éste término ha sido “lo digital”, que recibió su aprobación en 1996, cuando el entonces
director del MediaLab del MIT , Nicholas Negroponte, compiló sus columnas en la revista
Wired en un libro titulado Being Digital139. Hoy, “lo digital” sigue dominando nuestra
comprensión sobre lo que son los nuevos medios.
Cuando hice búsquedas en Google sobre “digital”, “interactividad” y “multimedia”, el 28 de
agosto de 2009, los resultados fueron entre 230 y 240 millones de resultados para cada
uno. Una búsqueda en Google Scholar produce resultados similares: 10,800,00 para
“digital”; 4,150,000 para “web”; 3,920.00 para “software”; 2,760,000 para
“interactividad”; y, 1,870,000 para “multimedia”. Claramente, Negroponte estaba en lo
cierto, nos hemos vuelto digitales.
Hoy, no es necesario convencer a nadie sobre los efectos transformadores de Internet, el
Web y demás redes tecnológicas en la cultura y sociedad. Sin embargo, sí pretendo
convencernos de otro factor crucial en la revolución computacional y que ha sido menos
139 Nicholas Negroponte, Being Digital (Vintage, 1996).

128
discutido. Esto es importante para entender las formas de los medios contemporáneos y lo
que son los “medios” actualmente. Ésta es la cuestión del software.
Ninguna de las técnicas de creación y producción de medios que están asociadas con la
computadora es el resultado directo de los medios “se volvieron digitales”. Todas las
nuevas formas de acceso, distribución, análisis, generación y manipulación vienen del
software. Lo que también significa que son el resultado de decisiones particulares de
individuos, compañías y consorcios que desarrollan software (aplicaciones, codecs de
compresión, formatos de archivo, lenguajes de programación). Algunas de estas decisiones
tienen que ver con principios básicos y protocolos que rigen los ambientes de software
modernos: por ejemplo, los comandos “copiar” y “pegar” que vienen con software basado
en GUI, y sus nuevas versiones móviles; o los hipervínculos direccionales implementados
por las tecnologías Web. Otras decisiones son específicas a tipos particulares de software.
Si alguna técnica particular del software, o una metáfora de interfaz, que se implementa
en alguna aplicación (de escritorio, Web o móvil) se vuelve popular entre los usuarios, ésta
aparece rápidamente en otras aplicaciones. Por ejemplo, después de que Flickr añadió su
interfaz de nube de etiquetas, muchos otros sitios la empezaron a añadir también. La
apariencia de ciertas técnicas en las aplicaciones está, además, relacionada con la
economía de la industria del software. Esto se observa cuando una compañía de software
compra otra; se tienden a unificar los paquetes de ambas compañías. Por ejemplo, en
1995 Silicon Graphics compró dos programas de gráficos 3D (Wavefront y Alias), y unificó
ambas en un nuevo producto Alias|Wavefront. Grandes compañías como Google compran
constantemente software pequeño para añadirlo a la oferta de sus productos. Una de sus
aplicaciones más populares, Google Earth, está basada en software original desarrollado
por Keyhole, Inc. que Google compró en 2004.
A menudo, algunas técnicas desarrolladas con cierto fin, emigran a otras áreas. Las
técnicas de procesamiento de imágenes desarrolladas en segunda parte de los 50’s, para
el análisis de reconocimiento de fotografías, llegó a Photoshop a finales de los 80’s,
usadas hoy de manera más creativa para dar un aspecto más “artístico” a las fotografías.

129
Todas estas mutaciones de software y “nuevas especies” de software son profundamente
sociales, en el sentido que no surgen de mentes individuales o de algunas cuantas
propiedades “esenciales” de las computadoras digitales. Se trata de software desarrollado
por grupos de personas, comercializados a un gran número de usuarios y, por lo tanto, en
mantenimiento constante para mantenerse competitivos respecto de otros productos del
mismo mercado.
En suma: las técnicas, herramientas y convenciones del software de medios no son el
resultado de un cambio tecnológico que va de los medios “análogos” a los “digitales”. El
cambio a lo digital permite el desarrollo posterior de software de medios, pero no
determina las direcciones de su evolución. Esto es el resultado de las ideas intelectuales
de diseñadores (Sutherland, Engelbart, etc.); de los productos reales creados por las
empresas de software y de las comunidades open source; de los procesos culturales y
sociales que se establecen cuando la gente las empieza a usar; y, de las fuerzas del
mercado de software.
Esto nos lleva a pensar que los términos “medios digitales” y “nuevos medios” no abarcan
de manera suficiente la unicidad de la “revolución digital”. (Yo prefiero el término “medios
computacionales”, aunque no es muy usado fuera de algunas comunidades de
computólogos europeos) ¿Por qué no funcionan estos términos? Porque todas las nuevas
cualidades de los “medios digitales” no están “adentro” de los objetos media. Más bien,
todas ellas existen “afuera” (como comandos y técnicas de visualizadores de medios,
software de producción, animación, composición y edición, motores de juegos, software de
wikis y demás “especies” de software). Mientras la representación digital permite a las
computadoras trabajar con imágenes, texto, formas, sonidos y otros medios, en principio,
es el software el que determina lo que podemos hacer con ellos. Así que en realidad
“estamos siendo digitales”, pero las formas vienen del software.
Al aceptar la centralidad del software debemos cuestionar otro concepto fundamental de
la teoría estética y de medios: las “propiedades de un medio”. ¿Qué significa hacer
referencia a un “medio digital” que tiene “propiedades”? Por ejemplo, ¿es significativo
hablar de propiedades únicas en las fotografías digitales, los textos electrónicos o los sitios
Web? En el artículo de Kay y Goldberg, se establece una similitud entre las palabras

130
“propiedad” y “medio”: “el (texto electrónico) no debe ser considerado como una
simulación de libro de papel debido a que es un nuevo medios con nuevas propiedades”.
Yo también he usado esta combinación de palabras pero es el momento de indagar sobre
conceptos más precisos.
En sentido estricto, es pertinente, pero no exacto, hablar de propiedades de sitios Web,
imágenes digitales, modelos 3D, representaciones GIS. Los diferentes tipos de contenido
digital no tienen propiedades en sí. Lo que experimentamos como propiedades del
contenido de los medios viene del software usado para acceder, crear, editar y presentar
este contenido.
Esto incluye todo el software para crear y visualizar medios, tanto profesional como para el
público en general. El software en cuestión también puede ser el código que hace correr
una aplicación en donde el usuario interactúa o genera directamente el contenido a través
de la interfaz (un menú DVD, un kiosco interactivo, un proyecto de art basado en software).
Así, seguiré usando el término “propiedades” de forma amplia, pero mentalmente se
deberá interpretar como “técnicas de software definidas para trabajar en tipos particulares
de ecologías de medios, contenido y datos de medios” (un ejemplo de “ecología de
medios” es el sistema Flickr que incluye subir, etiquetar, organizar, comentar y compartir
imágenes. Un ejemplo de “datos de medios” es una imagen bitmap de 24 bits almacenada
en formato JPG).
Es importante dejar claro que no estamos diciendo que las diferencias entre los distintos
tipos de medios (imágenes bitmap, vectoriales, modelos 3D, texto sin y con formato,
animaciones, video, mapas, sonidos, olores, etc.) quedan completamente determinadas
por el software. Obviamente, éstos tipos de datos de medios tienen varias posibilidades de
expresión y representación; pueden producir diferentes efectos emocionales; son
procesados por diferentes redes neuronales en el cerebro; y corresponden a diferentes
procesos y representaciones mentales. Estas diferencias han sido discutidas durante miles
de años (de la filosofía antigua a la teoría estética clásica, al arte moderno y a la
neurociencia contemporánea). Lo que quiero decir en esta libro es algo otra cosa. Por un
lado, el software interactivo añade un nuevo conjunto de operaciones que pueden
aplicarse a todos estos tipos de medios (que, como usuarios, experimentamos como sus

131
nuevas “propiedades” (editar mediante la selección de partes discretas, distinción entre
estructura de datos y su representación, hipervínculos, visualización, búsqueda, etc.)). Por
otro lado, las “propiedades” de un tipo particular de medio pueden variar drásticamente
dependiendo de la aplicación que usamos para interactuar con él.
Veamos un ejemplo en detalle. Tomemos a la fotografía como tipo de medio. En la era
análoga, una vez que la fotografía quedaba impresa, la información quedaba “fija”. El que
viéramos esta fotografía en casa o en una exhibición, no era gran diferencia. Claro que un
fotógrafo podría hacer otra impresión con mayor o menor contraste o con un papel
diferente, pero esto resultaría en un objeto físico diferente, es decir, una nueva impresión
fotográfica con diferente información (por ejemplo, algunos detalles se perderían con un
mayor contraste).
¿Entonces qué pasa con una fotografía digital? Podemos tomar una foto con una cámara
digital compacta o con una DSLR profesional, o capturarla con un teléfono móvil, o
escanearla de un libro. En cada caso, acabamos con un archivo que contienen un arreglo
de pixeles con información de color (o escala de grises) y metadatos que especifican las
dimensiones, el modelo de cámara y demás datos sobre el disparo. En otras palabras, al
final, terminamos con algo que se llama normalmente “medio digital”: un archivo que
contiene números que representan detalles de una escena o de un objeto.
Sin embargo, a menos que seamos programadores, en realidad nunca interactuamos con
estos números. La mayoría de los usuarios interactúa con archivos de medios digitales
usando software. Y con base en el software, el uso que podemos hacer del medio cambia
dramáticamente. El software de mensajes multimedia (MMS) en los teléfonos portátiles
permite visualizar la foto que nos envió un amigo e incluso reenviarla a otra persona, pero
nada más. Los visualizadores gratuitos que tenemos en las computadoras de escritorios
nos permiten más funciones. Por ejemplo, Google Picasa 3.9 (2013) nos deja recortar,
colorear automáticamente, reducir ojos rojos, usar varios filtros, etc. En Picasa también
podemos ver la foto en blanco y negro, sin modificar el archivo original, y hacer
acercamientos para examinar detalles. Finalmente, si abrimos la misma foto en Photoshop
CS5, tenemos aún más funciones. Por ejemplo, podemos reemplazar automáticamente los
colores o hacer visible su estructura linear mediante la detección de bordes, etc.

132
Para resumir, haré un comentario directo. No existe tal cosa como “medios digitales”.
Solamente hay software, aplicado a los medios (o al “contenido”). Dicho de otra manera:
para los usuarios que interactúan con el contenido de medios solamente a través del
software, todas las “propiedades de los medios digitales” quedan definidas por el
software, y no están incluidas en el contenido mismo, es decir, en los archivos digitales. El
hecho que los medios sean usados y entendidos de forma diferente tiene que ver con el
desarrollo de un gran número de técnicas, algoritmos, estructuras de datos, convenciones
y metáforas del software. Estas técnicas existen a diferentes niveles: yendo de un reducido
número que son muy generales (“independientes de los medios”), a miles de ellas que son
muy particulares (por ejemplo, algoritmos usados para generar paisajes realistas o
software que extraen la posición de la cámara en secuencias de video para alinear
correctamente la composición de un modelo 3D).
Debido a la multiplicidad y variedad de técnicas de software, no es muy productivo reducir
los “medios digitales” a un pequeño número de nuevas propiedades. Esta reducción
solamente sería posible si pudiéramos organizar estas técnicas jerárquicamente, como
diferentes aplicaciones de unos cuantos principios generales. Después de trabajar en la
cuestión durante diez años (en 1999, redacté un artículo llamado Software como
vanguardia en donde intenté dar una primera taxonomía de estas nuevas técnicas) me doy
cuenta que cualquier jerarquía nos llevaría a una dirección equivocada. Todas las técnicas
cambian la identidad del “medio” (o “dato”), o conjunto de ellos, en donde se puedan
aplicar.
No es porque una técnica aparece en diversos software (diseñados para funcionar en
diferentes tipos de medios) o que otra es específica a un tipo particular, que se vuelve más
o menos importante que las demás. Por ejemplo, ¿se podría decir que la operación de
zoom, que existe en procesadores de texto, visualizadores de medios, software de
animación, modelado 3D, navegadores Web, etc. es más radical que un algoritmo para
“esferizar” un modelo 3D, es decir que solo se aplica dicho tipo con cierto software
especial? No creo que podamos medir cualitativamente los efectos prácticos de ambos
tipos de operación en la producción cultural y concluir si una es más importante que otra.
Ambas operaciones alteran cualitativamente el medio sobre el que actúan, y no

133
cuantitativamente. Es decir, ambas añaden a los medios nuevas cualidades (o
“potencialidades”) que no existían previamente. Un documento de texto en donde nos
podemos alejar al grado de ver diferentes páginas al mismo tiempo tiene una “identidad
mediática” diferente de otro en donde no podemos. De forma similar, la posibilidad,
precisamente, de esferizar un modelo virtual 3D es una nueva manera de trabajar con
formas espaciales.
En Software como vanguardia, agrupé las nuevas funciones de los medios digitales en
cuatro tipos según sus funciones: acceso, generación, manipulación y análisis. Hoy, incluso
esta simple clasificación parece problemática, en parte debido a la evolución del software
desde 1999 que ha llevado a una integración gradual de estas funciones. Por ejemplo,
cuando un usuario selecciona un archivo que contiene medios (imágenes, video, música,
texto, etc.) el archivo automáticamente se abre en un programa de
visualización/reproducción de medios. Y hoy, todos los estos programas (Windows Media
Player, Apple Quicktime, etc.) vienen con otras funciones básicas de edición. Entonces, en
términos prácticos, no hablamos simplemente de “acceder” a los medios, sin que
tengamos a la mano algunas maneras de “modificarlo (para aclarar las cosas, estoy
haciendo referencia a computadoras personales y aparatos portátiles que no están
especializados en la oferta de contenido digital, como los lectores DVD y los reproductores
MP3). Este desvanecimiento de fronteras entre tipos de funciones es una característica
muy importante de los medios basados en software.
¿Cómo fue que llegamos a esta nueva situación en donde, en lugar de ver/leer el
contenido, la mayoría experimentamos con las capas de las aplicaciones? La respuesta
más obvia sería la adopción de formato numérico como el intermediario universal. Lo llamo
intermediario porque al final del día, los medios deben ser accesibles a nuestros sentidos,
es decir, análogos (la onda de presión oscilatoria que percibimos como sonido, los niveles
de voltaje aplicados a pixeles del LCD que los hacen aparecer como diferentes tonos y
colores, las diferentes cantidades de teñido depositadas en el papel por las impresoras,
etc.). Por esto, los medios de software son una conversión constante de A a D (análogo a
digital) y de D a A (digital a análogo): por ejemplo, de las ondas luminosas a los números
guardados en un archivo, que representa una imagen, y después de vuelta a los niveles de
voltaje que controlan el monitor.

134
Hay dos niveles de codificación, primero la cuantificación de una señal análoga continua,
que resulta en su representación usando una escala discreta de números (por ejemplo, los
255 niveles usados comúnmente para representan tonos grises en las imágenes).
Después, viene la traducción de esta representación discreta en un sistema numérico
binario (que hace a los medios “incomprensibles” a la observación directa). La principal
razón no es el código binario per se (inventado por el académico hindú Pingala alrededor
de los siglos 5 al 2 AC), ya que es posible aprender cómo convertir una notación binaria en
decimal mentalmente. El problema es la descripción de una imagen, por simple que sea,
conlleva muchos números. Por ejemplo, una imagen 1024 x 768 contiene 786,432
pixeles, o 2,359,296 valores RGB, lo que hace muy difícil de comprender lo que estos
números representan si los examinamos directamente (diciendo de paso, en parte por
estas consideraciones, cualquier imagen digital puede ser entendida como visualización de
información, mediante la visualización de estos patrones representados numéricamente).
Así, usamos constantemente tecnologías para traducir estos conjuntos de números en
representaciones significativas para nuestros sentidos, ya sea la imagen original que
codifican o cualquier otra señal análoga (por ejemplo los valores visuales que controlan el
sonido en performances audiovisuales contemporáneos). Es interesante notar que el
precursor del fonógrafo de Edison en 1877 (el primer aparato para grabar y reproducir
sonido) fue el fonautógrafo de Édouard-Léon Scott de Matinville en 1857 que transcribía
sonido en un medio visual. O sea: la visualización de sonido fue inventada antes que su
grabación y reproducción.
Desde sus inicios, las tecnologías que generan y transmiten señales electromagnéticas
(como el gramófono) incluían por lo menos algunos controles para modificarlas (por
ejemplo, modificar la amplitud de una señal). El primer instrumento electrónico popular,
inventado por Leon Theremin en 1920, convirtió esos controles en un nuevo paradigma
para la ejecución musical. El artista controlaba la amplitud (el volumen) y la frecuencia
(tono) de un sonido acercando o alejando sus manos de las antenas del aparato.
El software extiende significativamente este principio, añadiendo más controles y más
formas de representar los datos. Por ejemplo, con Word, es posible ver el texto que estoy
escribiendo en este momento como bosquejo o como versión preliminar a su impresión,

135
etc., puedo ver u ocultar notas de pié de página, puedo hacer un resumen automático,
puedo cambiar tamaño y fuente de la tipografía. Mientras los datos, tal como son
representados y guardados en la computadora, no pueden ser accedidos directamente con
nuestros sentidos, el nuevo modelo tiene ventajas significativas debido a que los datos
pueden ser formateados de diferentes maneras. Este formato puede ser modificado
interactivamente, puede ser guardado junto con los datos e invocado más tarde.
Es posible articular las relaciones entre tecnologías pioneras de grabación y reproducción
electromagnética, desarrolladas a finales del siglo XIX, y el software de medios
desarrollado 100 años después (teléfono: Bell, 1875; fonógrafo: Edison, 1878; televisión:
1884; radio: Fesseden: 1900). Las primeras tecnologías mantenían la forma original de la
representación de los medios: impresión en bloques de madera, la impresión con
tipografía móvil, la imprenta, la litografía, la fotografía. Las tecnologías del siglo XIX
favorecieron la forma de la señal eléctrica, introduciendo una fundamental nueva capa de
medios: la interfaz, es decir, formas de representar (“dar formato”) y controlar la señal. Y
esto cambia la manera en que los medios funcionan: sus “propiedades” ya no están
únicamente en los datos sino también en la interfaz disponible.
El cambio a datos digitales y software de medios, 100 años después, generaliza este
principio a todos los medios. Con todos los tipos de datos codificados como números, sólo
pueden ser accedidos eficientemente por los usuarios mediante aplicaciones. La
consecuencia de esto ya la hemos discutido: todas las “propiedades de los medios
digitales” están ahora definidas por el software en lugar de estar únicamente en el
contenido mismo, es decir, los archivos digitales. De esta manera, lo que es cierto para la
grabación de audio, la radio, la televisión, el video, también aplica al texto, las imágenes y
la 3D.
En pocas palabras: los medios se vuelven software.
Podríamos concluir este capítulo aquí, pero es necesario hacer una cosa más. Sería injusto
poner toda la atención en el término “medios digitales” sin tomar en cuenta otro término

136
relacionado. Actualmente, éste otro término se usa ampliamente y se estudia críticamente:
me refiero a “nuevos medios”.
Ahora que hemos entendido que los “medios” de hoy son en realidad un conjunto de
técnicas de software en constante desarrollo, esto da un nuevo significado al segundo
término. No hay un límite lógico al número de algoritmos que pueden ser inventados; las
personas siempre pueden desarrollar nuevas técnicas de software para trabajar con los
medios. Desde esta perspectiva, el término “nuevos medios” captura adecuadamente esta
lógica fundamental del “metamedio computacional”. Los medios basados en software
siempre serán “nuevos”, siempre y cuando se sigan inventando y añadiendo nuevas
técnicas a las que ya existen. Quizá se pueda pensar que no todas las técnicas cambiarán
profundamente las forma o función de los medios, pero algunas definitivamente sí lo
harán.
Esta lógica de “expansión permanente” del software de medios sigue la lógica de la
ingeniería del software en su conjunto. Los computólogos de la academia y las empresas
de software trabajan con algoritmos para modificarlos constantemente, llevarlos de un
área a otra y diseñar nuevos. Esta lógica, que también puede llamarse “innovación
permanente”, es diferente de la lógica que gobernó el desarrollo de tecnologías de medios
en la era industrial. Tomemos el cine como ejemplo, entre 1890-1900 y la adopción de
herramientas de software por la industria cinematográfica. A pesar de que hubo cambios
dramáticos durante el siglo XX en lo que respecta a la construcción de lentes, a las
propiedades y formas de almacenar el celuloide, a las operaciones la cámara y demás
elementos de tecnología cinematográfica, la “capacidad básica de representación” de
éstas imágenes fue la misma. De Edison a George Lucas, las imágenes del cine se han
hecho de la misma manera: mediante el reflejo de la luz de los objetos, vía un lente, sobre
una superficie plana. Este proceso de captura crea tipos particulares de imágenes de la
realidad visible. Los objetos son interpretados en una perspectiva lineal con un punto de
fuga; o sea que las propiedades geométricas de cualquier escena, sin importar su
contenido, están sujetas al mismo tipo de transformación: el mismo mapeo que preserva
algunas propiedades de los visible (distorsión de la perspectiva con base en la distancia
entre objetos y observador), en oposición a otras perspectivas (por ejemplo, las

137
proporciones reales del tamaño de los objetos). Adicionalmente, una vez que la grabación
era hecha, la geometría de la imagen ya no se podía modificar.
Cuando el software llega a la imagen, estas constantes del medio fílmico se vuelven
variables. Aunque parezca menos “real”, la imagen del cine ahora tiene múltiples
relaciones con el mundo que está siendo visualizado. La composición digital permite la
inserción de modelos 3D generados a computadora que no estaba presentes en la escena
original. Igualmente, los objetos que estaban presentes pueden ser borrados. Los
panoramas virtuales interactivos, que permiten a los usuarios moverse en el espacio, se
pueden construir automáticamente. En algunos casos, es incluso posible re-interpretar una
secuencia del film como si fuera vista de diferentes perspectivas. Y éstas son sólo algunas
de las nuevas formas en que el software cambian la identidad del cine (estas nuevas
identidades aplican al video también).

138
SEGUNDA PARTE: Hibridación y evolución
Capítulo 3: Hibridación
Hibridación vs. multimedia
“El primer metamedio”, imaginado por Kay y Goldberg en 1977, se volvió gradualmente
una realidad en los 90’s. Empezando con Sketchpad y hasta las recientes versiones del
software de medios, la mayoría de los medios físicos han sido simulados a detalle y
muchas nuevas propiedades han sido añadidas. En paralelo, un nuevo medio
computacional sin precedentes físicos se estaba gestando: por ejemplo, los espacios 3D
navegables (Ivan Sutherland); multimedios interactivos (“Aspen Movie Map” de
Architecture Machine Group); hipertexto e hipermedia (Ted Nelson); cine con narrativa
interactiva (Graham Weinbren); Internet (Licklider, Bob Taylor, Larry Roberts, entre otros);
World Wide Web (Tim Berners-Lee); medios sociales como servicio (SixDegrees.com) 140,
plataformas masivas de colaboración como Wikipedia (Jimmy Wales y Larry Sanger).
Nuevas técnicas de generación, manipulación y presentación de medios, sin precedentes
en medios físicos, también se estaba desarrollando. Por ejemplo: generación algorítmica
de imágenes, interpretación foto-realista 3D, parámetros y comandos, llegados con
Sketchpad. También surgieron técnicas especificas y generales a los nuevos medios como
la gestión de datos. Pero aún más importante, a mediados de los 90’s, las computadoras
se volvieron suficientemente rápidas para “correr” estos medios. O sea que la visión de
Kay de la computadora como metamedio (una plataforma que alberga muchos medios
existentes y nuevos) se realizó.
140 d. boyd & N. B. Ellison, “Social network sites: Definition, history, and scholarship,” Journal of
Computer-Mediated Communication, 13 (1) (2007),
http://jcmc.indiana.edu/vol13/issue1/boyd.ellison.html.

139
Entonces, ¿qué pasa después? ¿cuál es la siguiente etapa en el desarrollo del
metamedio? (uso aquí la palabra “etapa” en un sentido lógico, más que histórico). Esto es
algo que, según yo, no escribieron los inventores de los medios computacionales:
Sutherland, Nelson, Engelbart, Kay y sus colaboradores. Pero como ellos prepararon las
condiciones, son indirectamente responsables de ello.
La discusión del metamedio computacional, al final del famoso artículo de Kay y Goldberg,
da la impresión que se desarrollaría con “adiciones”, a medida que los usuarios
construyeran nuevos tipos de medios para satisfacer sus necesidades, usando las
herramientas de la computadora personal Dynabook. Al ver el desarrollo del metamedio
computacional de los últimos 30 años, parece que se confirma esta conclusión.
Por ejemplo, si vemos el uso de los medios computacionales en el arte, que empezó en los
50’s (adopción de computadoras para la composición musical y la generación algorítmica
de imágenes), para el 2003, el influyente libro Digital Art (de Christiane Paul) lista ya una
docena de diferentes áreas. El artículo de Wikipedia sobre “software colaborativo”
contiene también una lista de docenas de tipos de programas (y claro, dentro de cada uno
hay cientos de productos) 141. El artículo sobre “software social” enlista 20 grandes tipos
de medios sociales y ninguno de ellos existía prácticamente en los 60’s (mensajes
instantáneos, chats, blogs, etc.).
Para continuar con ejemplos de estas adiciones, un típico diseño visual hecho hoy con
aplicaciones de software puede parecer la suma de medios previos: un dibujo + pintura al
óleo + fotografía + collage. Si ponemos atención en las interfaces de edición de medios,
también parece que se confirma nuestra impresión. Aquí vemos infinitas opciones para
modificar un documento, acomodadas una con otra en múltiples menús. Mientras las
nuevas opciones se van haciendo disponibles, debido a que la manufactura de software
lanza una nueva versión o porque compramos plug-ins por separado, se muestran como
adiciones en los mismos menús. Ciertamente, el número de comandos en aplicaciones de
medios populares se incrementa con cada nueva versión: se añaden más técnicas de
creación, edición, remix y creación, con salida hacia diferentes plataformas de distribución.
141 http://en.wikipedia.org/wiki/Collaborative_software, consultado el 4 de febrero, 2012.

140
Sin embargo, estos procesos de adición y acumulación no son los únicos que definen la
evolución del metamedio computacional. Aunque efectivamente funcionan, no creo que
estén al centro de la transformación (o, si queremos, mutación) de este metamedio (y por
extensión de toda la cultura moderna vía software) en las tres décadas que han seguido el
artículo de Kay y Goldberg. De hecho, tomo 1977 como la fecha simbólica que marca la
primera etapa de la “invención de medios”.
Creo que el periodo que va de finales de los 70’s a mediados del los 2000’s, representa
una segunda etapa fundamentalmente distinta en el desarrollo del metamedio
computacional. Esta etapa continúa la implementación práctica inicial y la podemos llamar
hibridación de medios.
Una vez que la computadora se convirtió en un cómodo hogar para un gran número de
medios simulados y nuevos, se vuelve lógico esperar que se empiecen a crear híbridos. Y
esto es exactamente lo que ha estado pasando en esta nueva etapa de la evolución de
medios. Tanto los medios simulados como los nuevos (texto, hipertexto, fotografía fija,
video digital, animación 2D, animación 3D, espacios navegables, mapas, informaciones
geográficas) operan como los componentes básicos para nuevas combinaciones de
medios.
He aquí algunos ejemplos de medios híbridos computacionales: Google Earth combina
fotografía aérea, imágenes satelitales, gráficas computacionales 3D, fotografía fija y
demás medios pata crear una nueva representación híbrida que los ingenieros de Google
llaman “interfaz 3D del planeta”. Una secuencia de gráficos animados (motion graphics)
combina contenido y técnicas de diferentes medios como el video de acción viva,
animación 3D por computadora, animación 2D, pintura y dibujo. Los motion graphics son
visuales animados que nos rodean diario; los vemos en los títulos de películas y programas
de TV, gráficos de la TV, gráficas para contenido de medios móviles, spots publicitarios y
videoclips musicales. El diseño de un sitio Web puede mezclar fotos, tipografía, gráficos
vectoriales, elementos interactivos y animación. Las instalaciones físicas que se integran
en espacios culturales y comerciales (por ejemplo el Nobel Field en el Centro Nobel de la
Paz en Oslo, hecho por Small Design, las pantallas interactivas en tiendas Nokia y Diesel

141
hechas por Nanika, o el Memory Wall del Hotel Puerta América en Madrid, hecho por Karen
Finlay y Jason Bruges, etc.) combinan animaciones, video, control computacional y varias
interfaces físicas de sensores táctiles para crear el ambiente espacial interactivo142.
El diccionario que vienen incluido en la versión Microsoft Word que estoy usando para
escribir este texto tiene unas definiciones para “híbrido”: “una planta producida de la cruza
de plantas con diferentes componentes genéticos”; “un animal que resulta del apareo
entre distintas especies o subespecies”. Estos significados biológicos de “híbrido” captan
muy bien la esencia de lo que está pasando en los medios con su “softwareización” en los
80’s y 90’s. O sea, la translación sistemática de numerosas técnicas para la creación y
edición de medios, procedentes de tecnologías físicas, mecánicas y electrónicas, en las
aplicaciones de software. Una vez traducidas en software, las técnicas empiezan a actuar
como especies en una ecología común, en este caso un ambiente de software compartido.
Una vez que se “liberan” en este ambiente, empiezan a interactuar, mutar y hacer híbridos.
Si queremos relacionar el inicio de esta nueva etapa de hibridación con algunos proyectos
y tecnologías importantes en la historia de los medios computacionales, el famoso sistema
interactivo hipermedia Aspen Movie Map, desarrollado en el MIT entre 1978-1979, sería
un buen punto de partida143. Precursor del Google Street View (2007), este sistema
combinaba películas de las calles de Aspen, fotografías fijas, un mapa de navegación con
fotografías aéreas y diagramas de dibujo, y audio. El segundo evento sería el lanzamiento
del software multimedia QuickTime (Apple, 2 de diciembre, 1991) como adición a su
System Software 6. Como lo explicó Apple en su artículo técnico “QuickTime 1.0: You
Oughta be in Pictures” (verano de 1991): “el nuevo QuickTime 1.0 facilita añadir medios
dinámicos, como video y sonido, en tus aplicaciones. Y esto es sólo el inicio” 144. En los
siguientes diez años desarrolladores comerciales, ingenieros, diseñadores y artistas
142 http://www.davidsmall.com/articles/2006/06/01/nobel-field/; http://www.nanikawa.com/;
http://www.jasonbruges.com/projects/international-projects/memory-wall.
143 “The Interactive Movie Map: A Surrogate Travel System,” video, (The Architecture Machine,
1981), http://www.media.mit.edu/speech/videos/.
144 Apple, “QuickTime 1.0: ‘You Oughta be in Pctures” (verano, 1991),
http://www.mactech.com/articles/develop/issue_07/Ortiz_Text.html.

142
independientes invirtieron mucha energía creativa para explorar la nueva habilidad de la
computadora para presentar múltiples medios. Entonces, pienso en los 90’s como en el
periodo en que se fundan las bases y formas de combinar medios en una solo plataforma
computacional. A esto sigue el periodo de comercialización de dichas invenciones en los
2000’s y la adopción de plataformas móviles.
Para empezar el estudio de la hibridación de medios, es importante aclarar que no sólo
nos estamos refiriendo a algo que ya tiene un nombre, la “multimedia computacional” o,
simplemente, la “multimedia”. Éste término se hizo popular en los 90’s para describir las
aplicaciones y los documentos electrónicos en los cuáles diferentes tipos de medios
coexisten.
Es común que estos medios (típicamente texto, gráficos, fotografías, video, escenas 3D y
sonido) estén situados en lo que parece visualmente un espacio bidimensional. O sea que
una típica página Web de los 2000’s es un ejemplo de multimedia, lo mismo una
presentación PowerPoint. Hoy, éstas siguen siendo las formas más comunes de
estructurar documentos multimedia. De hecho, incluso vienen predefinidas en muchas
aplicaciones de creación de medios. Cuando un usuario de Word, PowerPoint o
Dreamweaver crea un “nuevo documento”, se le muestran una página en blanco lista para
empezar a teclear texto. Otros tipos de medios deben ser “insertados” mediante comandos
especiales. Pero las interfaces para crear “multimedios” no deben seguir forzosamente
esta convención. El e-mail y los mensajes instantáneos siguen otro paradigma para
agregar elementos de distintos tipos: “los archivos adjuntos”. Entonces, el usuario de un
teléfono con MMS (Multimedia Messaging Service) puede enviar mensajes de texto con
archivos adjuntos como imágenes, sonidos, videos y textos enriquecidos. Otro paradigma
más, igual de persistente en la cultura digital, de Aspen Movie Map (1978) a Second Life
(2003), pasando por VRML (1994), son los espacios 3D como plataforma: aquí, otros
medios son anexados o insertados directamente en el espacio.
La “multimedia” fue un término muy importante cuando las aplicaciones culturales
interactivas empezaron a aparecer a inicios de los 90’s. El desarrollo de estas aplicaciones
estuvo apoyado en la introducción de medios de almacenamiento apropiados, por ejemplo

143
los CD-ROM para grabar en 1991, las arquitecturas de computación y los formatos de
archivo diseñados para soportar diversos formatos y el software de producción de
multimedios (la primera versión de un software que más tarde se Macromedia Director
salió en 1987). Para mediados de los 90’s, las exhibiciones de arte digital tenían ya una
variedad de proyectos multimedia; los programas educativos en arte digital empezaron a
introducir cursos de “narrativa multimedia”; y, los museos de arte, como el Louvre,
empezaron a publicar CD-ROM’s multimedia con visitas virtuales a sus colecciones. En la
segunda parte de la década de los 90’s, la multimedia tomó el Web, a medida que más y
más sitios empezaban a incorporar diferentes tipos de medios. A finales de la misma
década, la multimedia se había vuelto el estándar de las aplicaciones computacionales
interactivas. Los CD-ROM’s, sitios Web, kioscos interactivos y comunicación multimedia vía
dispositivos móviles se volvieron tan comunes que el término perdió relevancia. De esta
manera, hoy vemos multimedia todos los días y no nos asombrarnos de las capacidades
de los nuevos aparatos para mostrar múltiples medios al mismo tiempo.
Desde el punto de vista de la historia de medios, la “multimedia computacional” es un
desarrollo de gran importancia. Los “documentos multimedia” de antes combinaban
múltiples tipos de medios (los manuscritos medievales ilustrados, la arquitectura sagrada,
o el cine y la TV del siglo XX) pero no eran interactivos (en el sentido de las potencialidades
particulares que proveen las computadoras interactivas, en lugar de otras tecnologías
igualmente interactivas como los libros de papel) ni en red. Pero la coexistencia de
múltiples tipos de medios en un mismo documento, o aplicación, es sólo uno de los
desarrollos de la simulación de varios tipos de medios en la computadora. Al poner el
acento en el término medios híbridos, quiero llamar la atención a otro desarrollo, igual de
esencial y que, contrario a la multimedia, no ha recibido un nombre formal.
Es cierto que la multimedia puede ser vista como un caso particular de medios híbridos.
Sin embargo, prefiero pensar en ellos como superpuestos uno sobre otro; como dos
desarrollos diferentes. Y es que sólo algunas de las clásicas aplicaciones multimedia de
los 90’s eran medios híbridos, pero la mayoría no. Al contrario, mientras los medio híbridos
incluyen contenido de diferentes medios, éste es solamente un aspecto de su apariencia.
Entonces, ¿cuál es la diferencia entre los dos? En los documentos multimedia y las
aplicaciones interactivas, los contenidos de múltiples medios aparecen juntos. En una

144
página Web, las imágenes y el video están junto al texto; un blog muestra texto, seguido de
imágenes, seguido de más texto. En un mundo 3D, una superficie plana puede ser usada
para mostrar un video. Y en una aplicación MMS, cada elemento de un mensaje se
despliega en una ventana diferente o en su propio pre-visualizador (por lo menos eso es lo
que sucede en los teléfonos móviles de los 2000’s). Por el contrario, en el caso de medios
híbridos, las interfaces, las técnicas y las formas y tradiciones de los medios se unen y dan
como resultado una nueva Gestalt de los medios. Esto quiere decir que se fusionan para
ofrecer una nueva experiencia, diferente a la que se vive con elementos separados.
Otra manera de resaltar esta diferencia es con la metáfora de la reproducción sexual. El
resultado de la reproducción sexual son nuevos individuos que combinan material genético
de ambos padres (no son ensambles mecánicos de las partes físicas de los padres, lo que
sería analógico a la multimedia). Usando esta metáfora, podemos decir que los nuevos
medios son hijos que combinan el DNA de sus padres mediáticos.
Un modelo relacionado que ayuda a comprender otros aspectos de este proceso es el de la
evolución biológica. Este proceso trae como resultado nuevos organismos, nuevas
especies y nuevos componentes básicos (moléculas como el DNA y las proteínas). De
forma similar, algunas veces los nuevos medios son ligeramente diferentes a los
existentes; en otras ocasiones, las combinaciones con DNA del software producen nuevas
“especies”. El proceso de la evolución de medios también produce nuevas técnicas para
su creación, edición, colaboración e intercambio, nuevas convenciones de interfaz y
nuevos algoritmos. Estos son los equivalentes a los componentes básicos de la evolución
biológica.
Una metáfora más, que nos puede ayudar a entender la nueva etapa del desarrollo de
medios, es el remix. En el proceso de desarrollo del metamedio computacional, diferentes
tipos de medios se mezclaron y formaron nuevas combinaciones. Partes de estas
combinaciones entraban en nuevos remix y así consecutivamente ad infinitum.
Cada metáfora resalta algunos aspectos de nuestro fenómeno de estudio, pero también
oculta otros. Las metáforas de la reproducción sexual, de la evolución biológica y del remix
musical funcionan de forma similar. Cada una de ellas tiene ventajas y desventajas que

145
explican esta segunda fase del desarrollo de medios computacionales. En este capítulo,
usaré ampliamente los conceptos de hibridación y evolución para describir la nueva etapa
del desarrollo de medios. Y en los capítulos de la tercera parte usaré la metáfora del remix.
Es probable que, debido a que usaré estas metáfora en diferentes partes de mi narración,
dé la impresión de que son partes complementarias de una misma descripción. Pero éste
no es el caso de la hibridación y de la evolución biológica. Además del significado cotidiano
de “hibridación”, también se usa de una manera muy particular en la teoría evolutiva. Así
que, si pensamos la hibridación en ese sentido, no podemos usar el concepto en conjunto
con el modelo de la evolución biológica.
Las teorías contemporáneas de la evolución biológica comparten la definición básica que
las especies son un conjunto de organismos que pueden tener relaciones sexuales entre
ellos, pero no con otras especies. A través del proceso de evolución basado en la
separación geográfica de grupos de una misma especie, estos grupos evolucionan y
eventualmente ya no pueden tener relaciones entre grupos. Estos nuevos grupos se
llaman nuevas especies.
En contraste, un híbrido animal es resultado de la cruza entre especies. La mayoría de los
híbridos son producidos artificialmente, aunque se han registrado unos cuanto
naturalmente145. Por eso, os híbridos son excepciones en el proceso evolutivo normal.
Cuando uso el término “híbrido”, estoy haciendo referencia a un significado más general,
más allá del ámbito de la biología.
También debo hacer una observación sobre mi uso del modelo evolutivo biológico. No
estoy diciendo que los medios computacionales (o el desarrollo de la tecno-cultura en
general) “evolucionan” como los mecanismos biológicos, ni que los mecanismos de dicha
evolución son los mismos de la evolución biológica, como lo formula la biología
contemporánea. (Franco Moretti, en si libro Graphs, Maps, Trees: Abstract Models for a
Literary History, da explicaciones convincentes de porqué algunas de las ideas de la
145 M.L. Arnold (1996). Natural Hybridization and Evolution (New York: Oxford University Press,
1996).

146
evolución biológica no son compatibles con la historia cultural146). En su lugar, pretendo
usar la teoría evolutiva como una caja de herramientas conceptual, que nos puede ayudar
a pensar sobre cualquier tipo de proceso temporal. Si así la entendemos, la teoría
evolutiva complementa otras sobre procesos físicos, sociales y psicológicos (cada una de
ellas con sus conceptos únicos, que permiten conceptualizar el desarrollo). Algunos
ejemplos son: la teoría del desarrollo social de Marx, con conceptos como modo de
producción, base, superestructura, etc.; la teoría del “trabajo del sueño”, formulada por
Freud en su La interpretación de los sueños (1899), con conceptos como condensación y
desplazamiento; la teoría de sistemas complejos, con conceptos como emergencia y autoregulación;
el modelo de cambio de fases de la termodinámica, y muchas otras.
Como yo lo percibo, la hibridación de medios es una reconfiguración del universo de
medios más profunda que la multimedia. En ambos casos se nota la convergencia de
múltiples tipos de medios. Sin embargo, la multimedia no pone riesgo la autonomía de los
diferentes medios, quienes mantienen sus propios lenguajes, es decir, formas de
organizar, acceder y modificar los datos de medios. Los ejemplos típicos de múltiples
medios en el Web y en presentaciones PowerPoint ilustran claramente este punto. Imagina
una clásica página Web de los 2000’s, que tiene texto y video. Ambos medios están
separados en todos sus niveles. Sus lenguajes mediáticos no se riegan entre ellos. Cada
medio usa su propia interfaz. Con el texto podemos deslizarnos hacia arriba y abajo,
podemos también hacer zoom con los controles del navegador o incluso cambiar la fuente
o quitar el estilo visual. Con el video, podemos su interfaz para reproducirlo, detenerlo o
retrocederlo, o también podemos subir o bajar el volumen. Así, ambos están juntos pero
sus interfaces no interactúan. Esto es, según yo, un caso típico de la multimedia.
Por su parte, en los medios híbridos, los lenguajes de distintos medios se unen:
intercambian propiedades, crean nuevas estructuras e interactúan en sus niveles más
profundos. Por ejemplo, en los gráficos animados (motion graphics), el texto adquiere
propiedades que eran únicas al cine, la animación y al diseño gráfico. En otras palabras, el
texto mantiene sus dimensiones tipográficas tradicionales, fuente, tamaño, espaciamiento,
pero se enriquece de otras posibilidades expresivas de otros medios. Cuando una palabra
146 Franco Moretti, Graphs, Maps, Trees: Abstract Models for Literary History (Verso, 2007).

147
se va acercando a nosotros se puede ir desenfocando, como si fuera un objeto físico
filmado con cámaras y lentes del siglo XX. Al mismo tiempo, puede volar en un espacio
virtual, hacer movimientos físicos imposibles (como cualquier otro objeto 3D). Sus
proporciones cambian según el lente virtual que use el diseñador. Las letras individuales
pueden explotar en cientos de partículas, etc. Así, en el proceso de hibridación, el lenguaje
de la tipografía no es estático, fijo o fiel a sus orígenes. Se genera un nuevo metalenguaje
que combina las técnicas de distintos lenguajes previos.
Otra manera de distinguir entre “multimedia” y “medios híbridos” es verificando si la
estructura original de los datos es afectada, o no, cuando los medios se combinan. Por
ejemplo, en los videos en documentos multimedia, como los mensajes MMS, los e-mails
en formato HTML, las páginas Web convencionales y las presentaciones PowerPoint, la
estructura de los datos del video no cambian de ninguna manera. Un archivo de video
digital es una secuencia de fotogramas individuales, que tienen el mismo tamaño y
proporción, justo como lo fueron el cine y el video del siglo XX. En consecuencia, los
métodos estándar para interactuar con estos tipos de datos no desestabilizan nuestra idea
de lo que es un “video”. De la misma manera que con una videocasetera de los 80’s, el
usuario oprime el botón de “play” y los cuadros rápidamente se van sustituyendo, unos por
otros de forma secuencial, para producir el efecto de movimiento. El video sigue siendo
video.
Esto es clásico en la multimedia. Un ejemplo de cómo sería reconfigurada (la
reconfiguración es una habilidad clave de los medios híbridos) ésta misma estructura se ve
en The Invisible Shape of Things Past, una famoso proyecto de patrimonio cultural digital
sobre la historia de Berlín, desarrollado por la compañía de medios Art+Com entre 1995 y
2007147. Aquí, los clips del video se convierten en objetos sólidos, ubicados en un espacio
virtual 3D. Cada objeto está construido a partir de los fotogramas individuales, que se van
ordenando uno tras otro, en un apilamiento 3D. El ángulo entre los fotogramas y el tamaño
de cada fotograma está dado por los parámetros de la cámara que originalmente tomó la
película. Es posible interactuar con estos nuevos “objetos fílmicos”, como cualquier otro
espacio 3D, mediante una cámara virtual.
147 http://www.artcom.de/en/projects/project/detail/the-invisible-shape-of-things-past/.

148
Al mismo tiempo, también es posible “ver la película”, usando el apilamiento de
fotogramas como reproductor de video. Pero esta operación ha sido repensada. Cuando un
usuario hace clic sobre el fotograma inicial de un apilamiento, los fotogramas
subsecuentes detrás de él se van borrando rápidamente. Esto da la ilusión de movimiento,
como en el siglo XX, y la contracción del objeto virtual 3D, al mismo tiempo.
En este ejemplo de reestructuración de medios, que caracteriza a los medios híbridos, los
elementos que componen la “estructura de datos” de la película original (los fotogramas
individuales) se componen en una nueva configuración. La antigua estructura se vuelve
una nueva estructura que mantiene los datos y relaciones originales (los fotogramas
siguen ordenados secuencialmente) pero también tiene nuevas dimensiones (el tamaño y
ángulo de los fotogramas). La nueva estructura permite un nuevo tipo de interfaz para
acceder a la película, que combina atributos del espacio virtual y del cine.
Espero que esta discusión haya dejado porqué los medios híbridos no son lo mismo que la
multimedia y porqué necesitamos éste nuevo término. El término “multimedia” capturó el
fenómeno de juntar el contenido de diferentes medios, pero no sus lenguajes. Hay otro
término que se usa frecuentemente en estudios de medios computacionales pero que
tampoco nos conviene usarlo: “convergencia”. El significado de diccionario incluye
“alcanzar un mismo punto” y “volverse cada vez menos diferentes y, eventualmente, lo
mismo”. Pero esto no es lo que sucede cuando los lenguajes de los medios se hibridan. Lo
que sucede en este caso es que adquieren nuevas propiedades, se enriquecen. Ya hemos
citado el ejemplo de los motion graphics, en donde el texto adquiere propiedades del cine
y de la animación por computadora. En gráficas 3D, la interpretación de los objetos 3D
puede usar las mismas técnicas que la pintura tradicional. En globos terráqueos virtuales
como Google Earth y Microsoft Virtual Earth, las fotografías se combinan para crear nuevas
representaciones híbridas y con nuevas interfaces enriquecidas.
En breve, la “softwareización” de los medios pasados no nos llevó a su “convergencia”.
Más bien, lo que vino después de la representación de formatos de medios viejos fueron
las técnicas para crear contenidos y las interfaces para accederlos. Todo esto se

149
independizó de sus bases físicas y se tradujo a software, y las partes empezaron a
interactuar creando nuevos híbridos.
Esta es, para mí, la esencia de la nueva etapa del metamedio computacional en donde
vivimos actualmente. Las propiedades y técnicas únicas de diferentes medios se volvieron
elementos de software que se pueden combinar de formas imposibles previamente.
Entonces, si en 1977 Kay y Goldberg previeron que el nuevo metamedio computacional
contendría “una variedad de medios existentes y aún-no-inventados”, ahora podemos
describir uno de los mecanismos clave detrás de la invención de estos nuevos medios.
Este mecanismo es la hibridación. Las técnicas y formatos de representación de medios
físicos y electrónicos se unen a las nuevas técnicas de manipulación de información y
formato de datos que son únicas ala computadora.
En retrospectiva, quizá no es accidental que la publicación del artículo de Kay y Goldberg
haya estado directamente seguida de un proyecto decisivo en esta nueva etapa de
hibridación de medios simulados en el software. En 1978-1979, un grupo de jóvenes
investigadores, en el grupo Architecture Machine en el MIT (dirigido por Nicholas
Negroponte y precursor del Media Lab), crearon Aspen Movie Map, un nuevo tipo de
aplicación que combina varios medios: video, mapas, gráficas y su nueva forma híbrida
similar a lo que ofrece Google Street View actualmente (lanzada en 2007, casi 30 años
después). Esta variedad de medios se integró en una nueva forma de interfaz hipermedia.
El nombre de la aplicación, Aspen Movie Map, dejaba ver que no se trataba ni de un mapa
ni de una películas sino de un híbrido de ambos. Este proyecto inaugura una etapa
fundamental en la evolución de medios basada en la “softwareización”: su etapa de
hibridación.
La evolución de un metamedio computacional
Sigamos con la metáfora de la evolución biológica. El título de Darwin lo deja claro: El
origen de la especies (1859). El objetivo central de esta teoría evolutiva era explicar el
desarrollo de diferentes especies. Darwin propuso que el mecanismo básico era la

150
selección natural y, en el siglo XX, otros biólogos supusieron ideas derivadas (desvío
genético, mutación, etc. 148). Nos queda claro que los mecanismos que rigen el desarrollo
del metamedio computacional son diferentes, pero es posible usar las ideas básicas de la
teoría evolutiva: el surgimiento, con el tiempo, de nuevas especies y el incremento de su
número. Pero, aún con esta idea básica, quedan otras diferencias importantes entre la
evolución biológica y de medios.
Si comparamos el desarrollo del metamedio computacional con la evolución biológica
podemos considerar las nuevas combinaciones de tipos de medios como nuevas
especies149. En la evolución biológica, el surgimiento de nuevas especies es un proceso
lento y gradual, que requiere muchas generaciones150. Pequeños cambios genéticos se
acumulan durante largos periodos antes de que surja una nueva especie. Sin embargo,
nuevas “especies de medios” pueden aparecer de la noche a la mañana: sólo hace falta
una idea novedosa y algo de programación. Los programadores y diseñadores de hoy
pueden usar diversas librerías de software para manipular medios pero también lenguajes
especializados creados específicamente para crear ideas y experimentos (por ejemplo
PureData Extended o Processing, entre otros). Una persona con talento puede inventar una
nueva especie mediática en una cuantas horas.
En la biología evolutiva, las especies se definen como grupos de organismos pero, en la
evolución de medios, las cosas son diferentes. Algunas combinaciones innovadoras de
medios sólo aparecen una o dos veces. Por ejemplo, un artículo en ciencias
computacionales puede introducir el diseño de una nueva interfaz; o un diseñador puede
148 http://en.wikipedia.org/wiki/Evolution#Mechanisms, Febrero 6, 2012.
149 Estoy consciente de que no sólo algunos detalles, sino también postulados fundamentales, son
tema de debate en la comunidad científica. En mis alusiones a la teoría evolutiva, tomo en cuenta
algunas ideas que son generalmente aceptadas. A pesar de estas ideas sigan siendo cuestionadas y
que puedan ser desaprobadas, actualmente forman parte del “sentido común”, del conjunto de
ideales acerca del mundo.
150 “La selección natural es el proceso gradual, no aleatorio, mediante el cuál los rasgos biológicos
se vuelven más o menos comunes en una población, como resultado de una diferencia reproductiva
de sus pares. Es un mecanismo clave de la evolución“.
http://en.wikipedia.org/wiki/Natural_selection, consultado el 7 de febrero, 2012.

151
crear una combinación única, que sólo sirve para un proyecto en particular; o una película
puede combinar diferentes técnicas de medios de forma creativa. Imaginemos que, para
casa caso, ningún híbrido se vuelve a recrear. Esto sólo pasa de vez en cuando.
Entonces, algunas combinaciones de medios que van surgiendo en la evolución no son
“selectivas”. Otras combinaciones, por su lado, puede que sobrevivan y que se “reduzcan”.
(Insisto: recuerda que sólo estoy evocando el modelo evolutivo como metáfora. No intento
aseverar que los mecanismos de la evolución de medios son similares a los biológicos).
Eventualmente, estos híbridos exitosos se pueden volver convenciones en el diseño de
medios; u opciones de software en programas de producción; u opciones en sitios de
medios sociales; u patrones de diseños; y así. En otras palabras, se vuelven nuevos
componentes básicos del metamedio computacional que se pueden combinar con otros
componentes.
Un ejemplo de una combinación exitosa de “genes” mediáticos es la técnica de “mapa de
imágenes” en el diseño Web. Esta técnica surgió a mediado de los 90’s y rápidamente fue
adoptada en varios proyectos de medios interactivos, juegos y sitios Web. ¿Cómo
funciona? Una imagen bitmap continua (una fotografía, un dibujo, un fondo blanco o
cualquier otra parte de una pantalla) es dividida en cierto número de partes invisibles.
Cuando un usuario hace clic sobre una de esas partes, ésta activa un hipervínculo
conectado su parte gráfica correspondiente.
Como medio híbrido, el “mapa de imágenes” combina la técnica de hipervínculos con las
técnicas de creación y edición de imágenes fijas. Antes, los hipervínculos sólo se añadían
en palabras o frases, y se hacían explícitos con alguna estrategia visual (casi siempre
subrayados). Cuando los diseñadores empezaron a usar hipervínculos en las diferentes
partes de un mapa de imágenes, o de otras partes de una página Web, sin hacerlos
explícitos, una nueva “especie” de medio fue creada.
Como nueva especie, ésta define nuevos comportamientos de usuario y genera una nueva
experiencia de medios. En lugar de que se presente de forma clara e inmediata, un usuario
debe explorar la pantalla, posicionar el mouse y haciendo clic de manera intuitiva. En lugar
de pensar en los hipervínculos como ubicaciones discretas dentro de una pantalla

152
“muerta”, un usuario la considera como una superficie viva e interactiva. A nivel
experimental, en lugar de imaginar un hipervínculo como algo presente o ausente, un
usuario lo vive como una dimensión continua, con algunas partes de su superficies “más”
vinculadas que otras.
Otros exitoso ejemplo de híbrido, que ha sobrevivido y se ha reproducido en la reciente
evolución de medios, es el modelo de la cámara virtual usada en la animación 3D por
computadora. Éste fue desarrollado en los 80’s para crear secuencias animadas para
películas y se ha vuelto uno de los elementos más usados en el desarrollo de videojuegos,
ambientes virtuales, interfaces de programa, películas, gráficas animados, etc.151.
Como veremos a detalle en la próxima sección de este libro, el nuevo lenguaje del diseño
visual (una categoría que abarca el diseño gráfico, diseño Web, gráficos animados, diseño
de cine, y demás áreas) que surgió en la segunda parte de los 90’s ofrece un ejemplo
prominente de hibridación de medios. Al trabajar con software, un diseñador puede
acceder y combinar cualquiera de las técnicas del diseño gráfico, tipografía, pintura,
cinematografía, animación, animación por computadora, dibujo vectorial y modelado 3D. Al
mismo tiempo, también puede usar muchas técnicas algorítmicas para la generación de
nuevas imágenes y formas (como sistemas de partículas y modelado procedural) y su
transformación (mediante el uso de filtros y demás técnicas de procesamiento de
imágenes), que no tienen equivalente en medios físicos, mecánicos o electrónicos previos.
Todas estas técnicas se encuentran disponibles en cierto número de programas para la
producción de medios (Photoshop, Illustrator, Flash, Maya, Final Cut, After Effects, varios
editores HTML, etc.) y se pueden fácilmente combinar entre ellas en un mismo diseño.
El resultado es el nuevo lenguaje de diseño usado actualmente en casi todas partes del
mundo. La nueva “estética global” celebra la hibridación de los medios y los emplea para
hacer una ingeniería de reacciones emocionales, contar historias y modelar experiencias
de usuario. O sea, todo es hibridación. O, para decirlo diferente, la combinación de
técnicas previamente incompatibles entre los diferentes medios es la característica común
151 Para un análisis del uso de la cámara virtual, consulta Mike Jones, “Vanishing Point: Spatial
Composition and the Virtual Camera,” Animation 3, no. 2 (2007): 225-243.

153
de millones de diseños creados diariamente, tanto por profesionales como por
estudiantes, que se ve en el Web y en medios impresos, en pantallas grandes y pequeñas,
en cualquier ambiente y plataforma.
Como sucedió con el post-modernismo de los 80’s y con la revolución Web de los 90’s, la
“softwareización” de los medios (la conversión en software de todas las técnicas e
interfaces de las tecnologías de medios precedentes) ha aplanado la historia… y
específicamente la historia de los medios modernos. Esto es, mientras que los orígenes
históricos de los componentes básicos que constituyen al metamedio computacional
pueden ser importantes en ciertos casos, ahora son la excepción y no la regla. Está claro,
para un historiador de medios como yo, que los orígenes de cualquier técnica disponible
en el software de producción de medios de hoy es sumamente interesante. De la misma
manera, también puede ser importante para personas a cargo de un proyecto de diseño de
medios específico, pero sólo si el diseñador decide resaltarlas. Por ejemplo, en la
secuencia animada del logotipo de DC Comics (Imaginary Forces, 2005), los diseñadores
usan de forma exagerada artefactos impresos y fílmicos para evocar periodos históricos
del siglo XX. Pero cuando uno se pregunta sobre el proceso real de diseño de medios,
estos orígenes históricos no son importantes. Cuando un diseñador prende su
computadora y empieza a trabajar, no le importa si la técnica que emplea fue
originalmente desarrollada en un medio físico o electrónico o ninguno de ellos. Así, ya sean
los pinceles digitales, o los filtros que simulan texturas, o un paneo de cámara, o una
perspectiva aérea, o modelos poligonales, o filtros en enfoque y desenfoque, o sistemas de
partículas, y demás, todos tienen el mismo estatus si los vemos como componentes
básicos de los nuevos híbridos.
Treinta años después de que Kay y Goldberg predijeron que el nuevo metamedio
computacional contendría “una amplia gama de medios existentes y aún no inventados”,
podemos darnos cuenta que su predicción fue correcta. El metamedio computacional se
ha en realidad expandido sistemáticamente. Sin embargo, como podemos ver ahora, esta
expansión no debe ser entendida como una simple adición de más y más nuevos
“medios”. A pesar de que algunos nuevos medios de software han sido inventados en los
cincuenta años que siguieron a Sketchpad, su número se puede contar en menos de dos
docenas. El proceso clave en la evolución del metamedio computacional va de la mano

154
con la innovación a un nivel más local: los tipos de medios precedentes simulados en
software (texto, sonido, dibujo, etc.), las técnicas para su manipulación y las nuevas
técnicas nativas a la computadora se combinan y crean un gran número de nuevas
“especies”.
Parafraseando: después de la primera etapa, en donde la mayoría de los medios
existentes fueron simulados en el software y que un número considerable de técnicas
computacionales para generar y editar medios fueron inventadas (una etapa que se
completó conceptual y prácticamente a finales de los 80’s), entramos en un nuevo periodo
regido por la hibridación. Los medios simulados empiezan a intercambiar propiedades y
técnicas. Como resultado, el metamedio computacional llegó a contener infinitas nuevas
especies. En paralelo, vemos un proceso continuo de invención de lo nuevo: pero lo que se
inventa no son nuevos tipos de medios sino nuevos elementos (nuevas técnicas para
crear, modificar y compartir datos de los medios). Tan pronto como son inventados, dichos
elementos empiezan a interactuar con otros elementes ya existentes. Así, los procesos de
invención e hibridación están íntimamente ligados y funcionan juntos.
En mi opinión, este es el mecanismo clave, responsable de la evolución y expansión del
metamedio computacional, desde finales de los 80’s hasta nuestros días (y no veo alguna
razón por la cuál el mecanismo se vuelva menos importante en el futuro). Y aunque en el
tiempo en que Kay y Goldberg escribieron su artículo el proceso de hibridación apenas
empezaba (recordemos que el primer medio híbrido significativo fue Aspen Movie Map
creado por el grupo Architecture Machine del MIT en 1978-79), hoy es de lo que se trata el
diseño de medios. Entonces, desde un punto de vista contemporáneo, el metamedio
computacional es de hecho un paraguas para muchas cosas, pero en lugar de solamente
abarcar medios diferentes, también abraca un gran número de pequeños componentes
básicos que se unen para crear híbridos. Estos componentes básicos incluyen algoritmos
para la creación y edición de medios, formatos de datos, metáforas de interfaces, técnicas
de navegación, técnicas de interacción física y demás tipos elementales. Con el tiempo, se
inventan nuevos elementos y se vuelven parte del metamedio computacional.
Periódicamente, la gente encuentra nuevas formas en que los elementos pueden
funcionar juntos, produciendo nuevas especies. Algunas de estas especies podrán
sobrevivir. Otras se podrán volver nuevas convenciones que serán tan omnipresentes que

155
no ya serán percibidas como combinación de elementos que pueden separarse. Otras se
olvidarán pero ser reinventadas posteriormente.
Evidentemente, no todos los componentes básicos que forman el metamedio
computacional tienen la misma importancia y las mismas posibilidades de “conectividad”.
Algunos son usados con más frecuencia que otros, entrando en muchas más
combinaciones. Por ejemplo, un modelo de cámara virtual 3D es más popular que una
técnicas de interpretación realista para cabello y pelo. La cámara viene incluida en toda
aplicación de animación 3D y, por consecuencia, es usada en más animaciones y efectos
visuales, en spots para TV, gráficos animados, noticias y películas. Es una parte de la
interfaz de usuario de los juegos 3D y también una interfaz popular de globos virtuales 3D
como Google Earth152. En contraste, los algoritmos de cabello y pelo no están disponibles
en cada paquete de animación y sus aplicaciones son más ilimitadas debido a que sólo
caracteres humanos o animales las usan.
Algunas innovaciones se pueden volver tan importantes e influyentes que pareciera
inadecuado referirse a ellas simplemente como elementos. Más bien, sería conveniente
usar la apelación plataformas de medios o, simplemente nuevos medios.
Las plataformas de medios móviles que surgieron a finales de los 2000’s (iOS y Android en
tabletas y teléfonos) son un ejemplo perfecto. También lo son: un espacio virtual 3D, el
World Wide Web, y los geo-medios (medios que incluyen coordenadas GPS). Estos son tres
ejemplos de plataformas de nuevos medios que se hicieron populares en los 80’s, 90’s y
2000’s, respectivamente. Estas plataformas reconfiguran profundamente la forma en que
se conciben los demás medios y cómo se pueden usar. De esta manera, cuando añadimos
coordenadas espaciales a objetos mediáticos, los ponemos en un hipertexto global en red
(el Web), o usamos espacios virtuales 3D como plataforma de diseño (no sólo de edificios
u objetos industriales sino de películas y caricaturas), la identidad de lo que creemos que
son los “medios” cambia fundamentalmente. De hecho, podemos decir que estos cambios
son tan radicales como los efectos de la “softwareización” de medios.
152 Una lista de otras aplicaciones de globos virtuales y herramientas de software puede verse en:
http://en.wikipedia.org/wiki/Virtual_globe (consultado el 7 de febrero, 2012).

156
¿Pero qué tan cierto es esto? No hay una manera simple de responder a esta pregunta. Al
final, se trata de un problema de perspectiva. Por ejemplo, si vemos la estética visual y
espacial contemporánea del diseño, notaremos que la simulación de medios existentes
con software, y su subsecuente periodo de hibridación de medios, ha tenido efectos más
substanciales que la invención del Web y los navegadores del Web gráfico.
Alternativamente, si nos interesamos en la comunicación visual, su historia, sus técnicas
de representación y su memoria cultural, sí creo que la adopción universal del software en
las industrias culturales globales es tan importante como la invención de la imprenta, la
fotografía o el cine. Pero si nos enfocamos en aspectos políticos y sociales de la cultura de
medios e ignoramos cómo se ven y qué pueden representar (preguntar quién crea y
distribuye los medios, cómo la gente se ve ella misma y al mundo y cómo crean y
mantienen relaciones sociales), sería mejor ubicar a las redes computacionales en el
centro.
Y aún así, no está de más recordar que sin el software, las redes contemporáneas no
existirían. Lógica y prácticamente, el software está en la epidermis de todo lo que viene
después.
Imaginemos que desconecto mi laptop de la red inalámbrica ahora mismo. Podría seguir
usando la mayor parte de mis aplicaciones (incluyendo Word para escribir este enunciado).
También puedo editar imágenes y video, crear una animación digital, diseñar un sitio Web
funcional y redactar entradas de blog. (Para cuando leas esto, Microsoft estará ofreciendo
Word como servicio en línea, pero otros procesadores de texto que funcionen localmente
debe seguir existiendo).
Pero si alguien deshabilita el software que permite funcionar a la red, ésta perecería153. En
otras palabras, sin las capas fundamentales de software, La Galaxia Internet (citando el
153 Desde finales del 2010, hay un movimiento gradual que tiende a ofrecer cada vez más funciones
en aplicaciones Web. Sin embargo, todavía hoy (en 2012), a menos que esté en Singapur o Tallinn,

157
título del libro de Manuel Castells de 2001154) no existiría. El software que estuvo a cargo
de la computadora en red de ARPANET, que vinculó dos máquinas remotas el 29 de
octubre de 1969 (una en el Centro de Network Measurement en la Escuela de Ingeniería y
Ciencias Aplicadas de la UCLA y la otra el sistema NLS de Douglas Engelbart en el SRI
International de Menlo Park, California), sólo se ha vuelto más variado e importante a
medida que las redes sean desarrollado. Así, una innumerable cantidad de tecnologías de
software es lo que permite la existencia de los medios en el Web: imágenes y videos en
páginas Web, blogs, Facebook y Twitter, servicios de intercambio de medios como YouTube
y Flickr, las fotografías aéreas y los edificios 3D de Google Earth, etc. De manera similar, el
uso de un espacio virtual 3D como plataforma de diseño (que será estudiado
posteriormente con más detalle), en realidad significa usar un número de algoritmos que
controlan la cámara virtual, la posición de los objetos en el espacio y su vista en
perspectiva, la simulación espacial de la difusión de la luz sobre las superficies, etc.
Hibridación: ejemplos
Ejemplos de medios híbridos están por todos lados: los podemos ver en interfaces de
usuario, aplicaciones Web, apps móviles, diseño visual, diseño interactivo, efectos
visuales, medios de localización, ambientes interactivos, arte digital y demás áreas de la
cultura digital. He aquí algunos ejemplos que he elegido de diferentes áreas. Mappr!,
creado en 2005 por Stamen Design (San Francisco), fue uno de los primeros mashups
populares155. Combinaba mapas geográficos y fotos de Flickr156. Usando información
suministrada por los usuario de Flickr, la aplicación adivinaba y mostrabas las ubicaciones
geográficas en donde las fotos habían sido tomadas. Desde 2007, Google Maps ofrece
que tienen cobertura total de red inalámbrica gratis, cortesía del gobierno, no sabré si tendré
conexión o no, así que no me confío ciegamente en el webware.
154 Manuel Castells, The Internet Galaxy: Reflections on the Internet, Business, and Society (Oxford
University Press, 2001).
155 http://en.wikipedia.org/wiki/Web_mashup, febrero 7, 2012.
156 www.mappr.com, enero 27, 2006.

158
Street Views, que añade vistas panorámicas (fotografías) de las calles a los demás medios
que ya usa Google Maps157. Street Views es un híbrido de fotografía e interfaces de
navegación espacial que permite a sus usuarios explorar las calles mediante flechas
superpuestas a las fotos panorámicas158.
En 1991, el artista de medios japonés Masaki Fujihata creó una serie de proyectos
llamados Field Studies159. Estos proyectos ponían grabaciones de video, tomadas en
lugares particulares, sobre espacios virtuales 3D altamente abstractos que representaban
dichos lugares. Fujihata empezó a trabajar en Field Studies una década antes de que el
término “medios de localización” apareciera. Debido a que las cámaras comerciales aún
no tenían GPS incluido, el artista construyó una cámara especial que capturaba las
coordenadas geográficas de cada entrevista (así como el ángulo, dirección y movimiento
de la cámara mientras grababa). El artista usaba estas informaciones para crear una
interfaz única que combinaba especio navegable 3D y video.
Para crear la instalación Alsace (2000) 160, Fujihata grabó entrevistas a personas que
vivían y cruzaban la frontera entre Francia y Alemania. El proyecto enfrenta al usuario con
una pantalla negra y unas líneas blancas tridimensionales, que representan el movimiento
del artista mientras grababa las entrevistas. A medida que navegamos su espacio, los
cambios de perspectiva de estas líneas sugieren las formas del terreno alsaciano. También
vemos unos rectángulos aplanados, ubicados en los puntos donde se grabó la entrevista.
Cada uno de ellos está puesto en el ángulo que corresponde a su grabación original.
Cuando hacemos clic sobre un rectángulo, el video de la correspondiente a la entrevista se
reproduce.
Para mí, Alsace representa un tipo de medio híbrido muy interesante. Fusiona fotografía
(las imágenes fijas dentro de los rectángulos), video documental (el video que se
reproduce cuando un usuario hace clic), medios de localización (las trayectorias de
157 http://maps.a9.com, enero 27, 2006.
158 http://en.wikipedia.org/wiki/Google_Street_View, julio 17, 2008.
159 www.field-works.net/, enero 27, 2006.
160 http://www.medienkunstnetz.de/works/field-work/, febrero 11, 2012.

159
movimiento) y espacio virtual 3D. Además, Alsace emplea una nueva técnica de medios
desarrollada por Fujihata: la grabación no sólo de ubicación 2D sino también de la
orientación 3D de la cámara.
El resultado es una nueva forma de representar experiencias colectivas usando los
espacios 3D como sistema general de coordenadas, en lugar de, por ejemplo, una historia
o una base de datos. Al mismo tiempo, Fujihata inició una forma sencilla y elegante de
interpretar las propiedades subjetivas y únicas de cada entrevista de video, al situar cada
rectángulo en el ángulo particular que refleja la posición de la cámara durante la
grabación. Adicionalmente, al definir el espacio 3D como un hueco vacío, mostrando
únicamente las trayectorias de los movimientos de Fujihata en la región geográfica, el
artista introdujo una nueva dimensión de subjetividad. Incluso hoy, después de que Google
Earth popularizó la navegación de espacios 3D con fotos y video, Alsace, y otros proyectos
de Fujihata, continúan imponiéndose. Estos proyectos muestran que para crear un nuevo
tipo de representación no basta con “añadir” diferentes formatos y técnicas de medios.
Más bien, se trata de cuestionar sistemáticamente la convenciones de los diferentes tipos
de medios que dan forma a un híbrido; cambiando su estructura en el proceso.
Otro proyecto célebre de arte de medios, que ya evocamos antes, es Invisible Shape of
Things Past de Joachim Sauter y su estudio Art+Com, también usa espacios 3D como
contenedor de otros tipos de medios. Este proyecto mapea clips de video históricos de
Berlín, grabados a lo largo del siglo XX, en nuevas formas espaciales, integradas en una
reconstrucción 3D de la ciudad161. Las formas están construidas mediante el
posicionamiento sucesivo de fotogramas del film, uno tras otro. Además de que es posible
movernos en el espacio y reproducir los videos, el usuario puede combinar y empatar
partes de Berlín, según diversos mapas, y crear un representación del desarrollo de la
ciudad en distintos periodos del siglo XX. Así como Alsace, Invisible Shape recombina tipos
de medios y cambia sus estructuras. Un video se vuelve un objeto 3D con una forma única.
En lugar de representar un territorio (una época determinada), un mapa puede combinar
partes de diferentes épocas.
161 Consultar: www.artcom.de

160
Otro proyecto pionera de medios híbridos, creado por Sauter y Art+Com, es Interactive
Generative Stage (2002), un set virtual en donde sus parámetros son controlados
interactivamente por los actores durante la ópera162. Mientras se realiza el performance, la
computadora lee los movimientos del cuerpo y los gestos de los actores y los usa como
información para controlar la generación del set virtual proyectado en la pantalla detrás del
escenario. Las posiciones de los cuerpos humanos son mapeadas en varios parámetros de
la arquitectura virtual: disposición, textura, color y luz.
Sauter consideró que era importante preservar las características del formato de ópera
tradicional (actores resaltados mediante la iluminación y un escenario trasero), pero
añadiéndole cautelosamente nuevas dimensiones163. Por ende, al seguir las convenciones
de la ópera tradicional, el set virtual aparece como un telón atrás de los actores, cuando
en realidad no es una imagen fija sino una arquitectura dinámica que cambia durante el
espectáculo. Como resultado, la identidad del espacio teatral cambia, yendo del telón al
actor principal (y se vuelve un elemento importante porque adopta diferentes
personalidades y sorprende cada vez a la audiencia con nuevos comportamientos). Este
tipo de redefinición de diseño de un medio híbrido es infrecuente, pero cuando un
diseñador logra su propósito el resultado es muy fuerte.
No todos los híbridos son elegantes, convincentes o atractivos a la vista. Algunas de las
interfaces de software para la creación y acceso a medios parecen el trabajo de un
estudiante de DJ que mezcla operaciones de interfaces tradicionales con nuevos principios
de la GUI de forma discutible e impredecible. Para mí, un ejemplo claro de esta
problemática de hibridación es la interfaz de Adobe Acrobat versión 8.0, lanzada en
noviembre 2006164. (Hay que notar que las versiones del software comercial cambian de
una versión a otra. Éste ejemplo se refiere a la versión mencionada). Esta versión de la
interfaz de usuario de Acrobat combina metáforas de diferentes tradiciones y tecnologías
de medios en una manera que no siempre parece lógica. En una sola interfaz, tenemos: 1)
162 El nombre completo del proyecto es Interactive generative stage and dynamic costume for André
Werners `Marlowe, the Jew of Malta.’ Para más información e imágenes, ver: www.artcom.de
163 Joachim Sauter, comunicación personal, Berlín, julio, 2002.
164 http://en.wikipedia.org/wiki/Adobe_Acrobat#Version_8.0, febrero 8, 2012.

161
los elementos de la interfaz de grabadoras de medios análogos del siglo XX, es decir, el
estilo de botones de las videocaseteras; 2) elementos de la interfaz del software de edición
de imágenes, o sea, la herramienta zoom; 3) los elementos de la interfaz asociados con la
tradición de medios impresos que, aunque nunca existieron en los libros, se ven como
iconos de páginas; 4) los elementos que han existido en libros (los separadores); 5) los
elementos estándares de la GUI como buscar, filtrar, múltiples ventanas. Parece que los
diseñadores de Acrobat querían dar a los usuarios una gran variedad de formas de
navegar un documento. Sin embargo, considero que algunas de estas metáforas son
confusas. Por ejemplo, debido a que Acrobat fue hecho con la intención de simular la
experiencia de documentos impresos, no me queda claro porqué hay que moverse en las
páginas mediante botones con flechas que apuntan hacia adelante y hacia atrás, lo que es
una convención de interfaz normalmente usada en medios de imágenes animadas.
Los híbridos no implican, necesariamente, una reconfiguración “profunda” de los lenguajes
de los medios que lo componen, ni de la estructura de sus objetos mediáticos (tal como lo
hace The Invisible Shape con la estructura de un objeto fílmico). Tomemos los mashups
Web, que “combinan datos de múltiples fuentes, ocultos detrás de una interfaz gráfica
simple y unificada” 165. Además del ejemplo de mashup antes mencionado (Mappr!), he
aquí otros: Flickrvision 3D (David Troy, 2007) usa datos de Flickr y de Poly 9 FreeEarth
para crear un mashup que muestra las fotos que se publican en Flickr y la ubicación donde
se tomaron en un globo virtual 3D. LivePlasma (2005) usa servicios de datos de Amazon
en un “motor de descubrimiento”. Cuando un usuario busca algún artista o director de cine
o grupo musical o DVD, la aplicación genera un mapa interactivo de sus relaciones entre
similares. La disposición espacial sigue diferentes parámetros basados en estilo,
influencias, popularidad, etc166. A pesar de que el propósito de los mapas de LivePlasma
es que descubramos elementos que nos puedan gustar (para que los compremos en
amazon.com), estos mapas generados son interesantes por ellos mismos. Ahí se ve cómo
los datos de preferencias culturales y comportamientos de las personas, recolectados por
sitios Web 2.0 como Amazon, se pueden usar para algo que no era posible antes de los
2000’s. En lugar de graficar relaciones culturales según la idea una sola persona o grupo
165 http://en.wikipedia.org/wiki/Mashup_(web_application_hybrid), julio 19, 2008.
166 http://www.liveplasma.com/, agosto16, 2008.

162
de expertos, estos mapas muestran las relaciones desde el punto de vista de los
consumidores culturales reales.
El desarrollo de mashups tiene el apoyo de un número creciente de API’s Web puestas a la
disposición por gran variedad de compañías. Una API (acrónimo de Application User
Interface o Interfaz de Programación de Aplicaciones) da a los programadores un modo
fácil de crear nuevos programas, que usan servicios o datos de otras compañías Web. Por
ejemplo, podemos usar el API de Google Maps para generar mapas interactivos en nuestro
propio sitio. Cuando un usuario visita nuestra página y lanza una búsqueda o manipula la
interfaz, el servicio de datos se conecta a los servidores de Google para localizar datos y
traerlos de nuevo. Cuando revisé el sitio de rastreo de mashups programmableweb.com el
8 de febrero de 2012, enlistaba 2,337 mashups que usaban el API de Google Maps167.
Muchos mashups combinan hasta una docena de API’s de diferentes servicios. A inicios de
2012, había más de 5,000 API’s168. La distribución de los más populares era: el 2,341
mashups usaban el API de Google Maps; 680 el de Twitter; y, 609 el de YouTube. Estos
números representan un pequeño porcentaje de todos los mashups que existen
actualmente. Sin embargo, son indicadores útiles de la magnitud de los mashups que usan
API’s de diversas compañías.
Visualmente, quizá muchos mashups parecen una típica página Web multimedia, pero en
realidad son más que eso. El artículo de Wikipedia en inglés sobre “mashup (web
application hybrid)” explica: “un sitio que permite a un usuario incrustar un video YouTube,
por ejemplo, no es un mashup… el sitio debe acceder a datos de terceros usando un API, y
procesar esos datos de alguna manera que permita dar valor agregado a los usuarios”
(énfasis de Lev Manovich). No importa que la formulación de la frase “dar valor agregado”
parezca centrada en los negocios, se sigue percibiendo la diferencia entre multimedios y
medios híbridos de forma teórica muy clara. Parafraseando, podemos decir que, en el caso
de medios híbridos artísticos exitosos, como The Invisible Shape o Alsace, la separación de
formatos de representación (video, fotografía, mapas 2D, globos virtuales 3D) y de
167 http://www.programmableweb.com/apis/directory/1?sort=mashups, febrero 8, 2012.
168 http://blog.programmableweb.com/2012/02/06/5000-apis-facebook-google-and-twitter-arechanging-
the-web/, febrero 6, 2012.

163
técnicas de navegación de medios (reproducir un video, hacer zoom en un documento 2D,
moverse en el espacio con una cámara virtual) se unen de forma tal que aumentan el valor
representacional y expresivo de cada medio empleado. Sin embargo, al contrario de los
mashups Web que aparecieron en 2006, cuando Amazon,Flickr, Google y demás
compañías abrieron el acceso a sus bases de datos, estos proyectos también usan sus
propios datos, creados y seleccionados cuidadosamente por los artistas mismos. Como
resultado, los artistas tienen más control sobre la experiencia estética y la “personalidad”
proyectada por sus obras, en comparación con el creador de un mashup que confía en los
datos e interfaces de otras compañías.
(No intento criticar aquí la tecnología de los mashups, sólo estoy diciendo que, si el
objetivo de un proyecto es enfatizar un modelo de representación diferente o una
experiencia estética única, la elección de las mismas fuentes y datos disponibles para
cualquiera en el Web quizá no sea la mejor. Y el argumento de que un creador de mashups
toca como DJ que mezcal lo que ya existe tampoco tiene justificación aquí. Un DJ tiene
mayor control sobre los parámetros de un mix y grabaciones de dónde escoger).
Mi revisión de estos ejemplos de híbridos los ha presentado, implícitamente, como
combinaciones y reconfiguraciones de medios precedentes, que incluyen tanto
simulaciones de medios físicos como de nuevos medios. En otras palabras, me he basado
en la idea (consistente con la de Kay) que el metamedio computacional puede ser visto
como una colección de diferentes medios. Por ejemplo, hablamos de cómo Alsace
combina fotografía y video documental (medios pre-digitales simulados en una
computadora) con datos de localización y espacio virtual 3D (nuevos medios
computacionales).
Sin embargo, podemos pensar en la hibridación de medios mediante una
conceptualización distinta del metamedio. Esto implica, en lugar de poner acento en el
medio “completo” podemos enfocarnos en sus componentes básicos: los diferentes tipos
de datos de los medios (o el “contenido de los medios”) y dos tipos de técnicas que
pueden operar en ellos (las que son “específicas a los medios” y las que son
“independientes de ellos”).

164
Desde esta perspectiva, las nuevas especies de medios (un proyecto, un servicio Web, un
programa de software) representan la unión de varias técnicas que antes pertenecieron a
diferentes medios. En los siguientes capítulos desarrollaré esta idea con más
detenimiento, con ejemplos de diseño, gráficos animados y efectos visuales. También
veremos cómo la hibridación, tal como es posible con el software, se volvió la estética
dominantes de los medios contemporáneos. Pero, con el fin de seguir avanzando,
tomemos un ejemplo de hibridación y analicémoslo en términos de sus tipos de datos y
técnicas de manipulación de los mismos. Para esto, consideráremos una aplicación
familiar a muchas personas actualmente: Google Earth.
Google Earth está basada en una aplicación previa llamada Earth Viewer, desarrollada por
Keyhole, Inc. y adquirida por Google en 2004. En su momento, Earth Viewer se inspiró de
la navegación interactiva al estilo del cinematográfico del proyecto Terravision (1996), que
exploraba un tipo de representación detallada, híbrida y espacial. A su vez, este proyecto
fue hecho por el mismo grupo de creativos atrás de proyectos que ya citamos: Joachim
Sauter y Art+Com169. Google Earth, en su versión 5 (2009), permitía navegar alrededor de
la superficie de la Tierra, hacer acercamientos o distanciamientos, activar o desactivar un
capas sobrepuestas de información, buscar lugares y rutas, marcar lugares y compartir
estas informaciones con demás usuarios, importar nuestros propios datos (imágenes y
datos GPS), crear videos de nuestros desplazamientos, etc.
Cuando Google Earth se lanzó en junio 2005, Google lo llamó una “interfaz 3D del planeta”
170. Esta descripción indica rápidamente que no estamos tratando con un mapa del siglo
XX u otro tipo de representación familiar a los usuarios. Entonces, ¿cuáles son los
elementos principales de la experiencia que ofrece esta “interfaz 3D del planeta” que la
hacen sobresalir de otras aplicaciones culturales que permiten al usuario navegar y
realizar acciones con datos (Google Maps en 2D, navegadores Web, iTunes, aplicaciones
multimedia educativas, etc.)? Estos elementos son su campo de hibridación y sus
mecanismos de navegación híbrida.
169 http://www.artcom.de.
170 http://windowssecrets.com/langalist-plus/a-3d-interface-to-the-planet/, febrero 10, 2012.

165
La representación de la superficie de la Tierra que aparece en la ventana principal del
mapa, llamada “visualizador 3D”, combina fotografía satelital, datos de elevación 3D,
modelos 3D de edificios y elementos gráficos que nos son conocidos por los mapas en
papel (indicaciones de texto, elementos gráficos para representar fronteras, caminos,
carreteras, etc.). Lo más interesante es que estos cuatro tipos de datos están “pegados
juntos” (interpretados al mismo tiempo, unos sobre otros), parecen una sola fuente visual.
Este es un ejemplo perfecto de hibrido. Los diferentes medios se unen para crear una
nueva representación.
La interfaz que ofrece Google Earth es ella misma un híbrido. Funciona sobre la base de
los nuevos medios computacionales que han evolucionado desde finales de los 60’s, más
específicamente, sobre los espacios interactivos 3D. También simula,
computacionalmente, la cinematografía de Hollywood desarrollado en el campos de la
animación 3D desde los 70’s: el usuario navega mediante una serie de controles que
extienden el lenguaje del zoom y de los movimientos físicos de la cámara. (Google Earth 6
define las siguientes “técnicas de navegación 3D”: movimiento a la izquierda, derecha,
arriba y abajo; rotación en el sentido y contrasentido de las manecillas del reloj; inclinación
hacia arriba y hacia abajo; acercamiento y alejamiento; zoom con inclinación automática; y,
reajustar a la posición original171).
Además de este sistema básico de navegación, la aplicación también permite un método
más automático, y explícitamente cinematográfico, llamado Touring (viaje), en donde la
cámara vuela una trayectoria entre dos puntos.
(Estoy haciendo la distinción entre “espacio interactivo 3D” y “simulación de la cámara”
por la siguiente razón. Mientras el software usado por animadores digitales, diseñadores
de juegos y diseñadores de medios ofrece una interfaz de cámara virtual del espacio 3D
con todos sus controles cinematográficos tradicionales, otras aplicaciones que también
171
http://support.google.com/earth/bin/answer.py?hl=en&answer=148115&topic=2376154&ctx=topi
c, febrero 10, 2012.

166
usan espacios virtuales (como la Realidad Virtual o juegos digitales) no lo hacen. Entonces,
una representación del espacio 3D y un modelo de cámara 3D no siempre van juntos.)
Aún cuando el modelo de datos central de Google Earth no ha cambiado (imágenes
satelitales, elevación de datos, elementos de mapas), con el tiempo y con nuevas
versiones del software se añaden otras fuentes y tipos de datos, haciendo cada vez más
rica la representación (e incrementado su hibridación). Estas tipos de datos adicionales
incluyen contenido Web, Street View (lanzado el 25 de mayo de 2007), fotos de alta
resolución normales y panorámicas, imágenes históricas, terrenos submarinos, la Luna,
Marte, tráfico en tiempo real, etc. Estos nuevos datos requieren nuevos mecanismos de
navegación. Así, vemos frente a frente, junto a la interfaz original de cine 3D, otras
interfaces.
De esta forma, las técnicas para trabajar con datos provenientes de Google Earth se han
vuelto también más híbridas. Mientras interactuamos con un edificio 3D, es posible
“precipitarse a su lado o a su cima”. En el caso de fotos de alta resolución, Google Earth
permite una manera especial de “volar dentro” de la fotos, de donde podemos, después,
alejarnos o movernos. Y, para Street View, se incluyen otras técnicas más de
navegación172.
Estrategias de la hibridación
Como hemos visto, los medios híbridos pueden estructurarse de diferentes maneras.
En interfaces de usuario, como la de Acrobat Reader, las operaciones que antes
pertenecían a medios físicos, mecánicos y electrónicos se combinan para ofrecer al
usuario más formas de navegar y trabajar con documentos computacionales (una
172 “Con el teclado o con el mouse se puede seleccionar la dirección horizontal y vertical de la vista y
del nivel de zoom. Una línea continua o discontinua en la foto muestra el camino aproximado que
siguió la cámara del coche, y las flechas vinculan a la siguiente foto en la misma dirección. En los
cruces e intersecciones de la ruta de la cámara del coche, se presentan más flechas”
http://en.wikipedia.org/wiki/Google_Street_View, febrero 11, 2012.

167
combinación de diferentes técnicas de interfaz). Google Earth combina diferentes tipos de
medios para dar información digerida sobre lugares (una combinación de tipos de medios).
Mappr! encarna otra estrategia: usar mapas 2D como interfaz de una colección de medios,
en su caso fotos publicadas en Flickr (usando un tipo de medios como interfaz de otro
tipo). Alsace y The Invisible Shape son ejemplos de otra combinación de medios: usar un
tipo como cápsula de otro tipo (un espacio 3D que contiene cine y video).
Otra manera complementaria para categorizar los medios híbridos es preguntándose si un
híbrido particular brinda nuevas formas de representar el mundo y/o nuevas formas de
navegar dichas representaciones. Los híbridos pueden combinar y/o reconfigurar formatos
e interfaces de medios que nos son familiares pero con la intención de crear nuevos tipos
de representaciones de híbridos. Por ejemplo, Google Earth y Microsoft Virtual Earth
combinan tipos de medios y técnicas de interfaz para dar información sobre lugares. Pero
las ambiciones de Alsace e Invisible Shape son diferentes: no se trata de dar más
información sino de reconfigurar estos formatos en nuevas representaciones de
experiencias humanas colectivas e individuales, fusionando dimensiones objetivas y
subjetivas. En ambos casos, podemos decir que el objetivo principal es representar el
mundo o nuestra experiencia en una nueva manera, combinando y posiblemente
reconfigurando representaciones familiares de medios (fotos, video, mapas, objetos 3D,
páginas Web, fotos panorámicas, etc.). Otro buen ejemplo de estos híbridos es Microsoft
Photosynth, que ofrece nuevos tipos de representaciones 3D (“síntesis”) gracias al empate
de fotografías de una misma escena, tal es el caso del un modelo de la catedral de Notre
Dame hecho exclusivamente con fotos de Flickr173.
También se puede que los híbridos estén centrado en nuevas manera de navegación e
interacción con formatos de medios existentes. En este caso, el tipo de medio no es
modificado o combinado con otro medio, más bien la hibridación sucede en la interfaz de
usuario y las herramientas que provee la aplicación o servicio para trabajar con este tipo
de medio. Por ejemplo, en Mappr!, tanto el mapa 2D como las fotos ya existían
previamente por separado. El mashup los une, convirtiendo al mapa en una interfaz para
173 http://www.ted.com/talks/blaise_aguera_y_arcas_demos_photosynth.html, February 19, 2012.

168
las fotos disponibles mediante Flickr174. (Flickr mismo ofreció más tarde una interfaz de
mapa similar175 así como la aplicación que permite a los usuarios ubicar sus fotos en un
mapa mundial176. A inicios de febrero de 2012, el mapa de Flickr ya tenía más de 175
millones de fotos geo-etiquetadas.)
En resumen, un híbrido puede definir nuevas técnicas de navegación e interacción que
operan sobre formatos de medios que no han sido modificados. Alternativamente, un
híbrido puede definir nuevos formatos de medios pero usar técnicas existentes de
interacción y de interfaz. Un híbrido también puede combinar ambas estrategias, es decir,
puede definir nuevas interfaces, herramientas y nuevos formatos de medios el mismo
tiempo. Esto último requiere, sin embargo, creatividad y comprensión de medios
computacionales y su estética. Por eso, estos híbridos no aparecen muy seguido. (Alsace,
Invisible Shape y Photosynth son capaces de combinar ambas estrategias y por eso, para
mí, resaltan de los demás proyectos y aplicaciones creadas en las últimas dos décadas.)
Se puede notar que la diferencia entre una “representación” (o un “formato de medio”) y
una “interfaz/herramienta” corresponde a los dos componentes fundamentales de todo
software moderno: estructuras de datos y algoritmos. Esto no es accidental. Cada
herramienta incluida en aplicaciones de producción, edición o visualización de medios
corresponde a un algoritmo que posee, ya sea datos en un formato dado o genera datos
en este formato. Por ejemplo, pensemos que nuestro formato de medio es una foto (o,
más general, una imagen bitmap). Para generar una galería de varias fotos, un algoritmo
debe procesar cada foto para que quepa en un espacio determinado (esto se logra
calculando promedios de grupos de pixeles y mostrando instancias más pequeñas de
dichos valores). Para dibujar una línea en una foto se necesita de otro algoritmo, que
174 Este mashup también ejemplifica un desarrollo importante en la evolución de los metamedios: la
convergencia de medios y datos espaciales. Las tres principales formas de esta convergencia son: 1)
un mapa 2D usado como interfaz de otro tipo de medios (como Mappr!); 2) un mapa virtual 3D usado
como interfaz para otro tipo de medios (como en Alsace, Invisible Shapes y Google Earth); 3)
información de ubicación añadida a grabaciones de medios.
175 http://www.flickr.com/map/.
176 http://en.wikipedia.org/wiki/Flickr#Organizr, March 2, 2012.

169
calcula nuevos colores para los pixeles que están “abajo” de la zona donde se quiere
dibujar. Entonces, “trabajar con medios” con aplicaciones de software significa
esencialmente aplicar diferentes algoritmos en los datos.
Pero, mientras que esta diferencia puede parecer clara y útil para una persona que sabe
de programación, cuando consideramos la experiencia del usuario en aplicaciones para
producir, ver, catalogar, compartir medios, servicios Web y proyectos de medios
interactivos, se vuelve difícil sostenerla. En el universo del software de aplicaciones, los
datos de los medios y las interfaces/herramientas nunca existieron de forma
independiente. A menos que sepamos programar, nunca vemos el contenido mismo de los
medios (fotos digitales, videos digitales, mapas, etc.). Más bien, vemos el contenido
mediante aplicaciones de software particulares, o de las interfaces hechas por los mismos
diseñadores para un proyecto específico. En otras palabras, siempre trabajamos con datos
en el contexto de una aplicación que viene con su interfaz y herramientas. Lo que significa
que, tal como es experimentada por los usuario de las aplicaciones de software, la
“representación” consiste en dos partes: datos estructurados de formas particulares y las
interfaces/herramientas disponibles para navegarlos y usarlos. (Lo mismo aplica para el
concepto de “información”). Por ejemplo, un “espacio virtual 3D”, como se ve en
aplicaciones de animación por computadora, de diseño asistido, juegos, globos virtuales y
demás, no es sólo un conjunto de coordenadas que hacen al objeto 3D y una
transformación de perspectiva, sino también un conjunto de métodos de navegación (es
decir, una cámara virtual). Una “fotografía”, vista por las aplicaciones de edición, incluye
varias operaciones que se le pueden aplicar: escala, cortar, pegar, máscaras, capas, etc.
Los mapa interactivos culturales de LivePlasma no son solamente relaciones entre
elementos en un mapa sino también las herramientas para construirlos y navegarlos. Y la
“tierra” particular de Google no sólo está hecha de su modelo de datos híbridos (foto
satelital, elevaciones, modelos 3D, panoramas) sino también de sus técnicas enriquecidas
para navegar y explorar sus datos.
Capítulo 4. Evolución del software

170
Algoritmos y estructuras de datos
¿Qué hace posible la hibridación de técnicas de creación, edición y navegación de medios?
Para responder esta pregunta debemos retomar lo que significa simular medios físicos con
software. Por ejemplo, ¿qué significa simular la “fotografía” y los “impresos”?
Una respuesta ingenua sería que las computadoras simulan los objetos mediáticos reales.
Es decir, una fotografía digital simula una fotografía análoga impresa en papel; una
ilustración digital simulan una ilustración dibujada sobre papel; y, un video digital simula
un video grabado en un video-cassette. Pero así no es como funcionan las cosas.
Lo que el software simula son las técnicas físicas, mecánicas o electrónicas usadas para
crear, editar, navegar e interactuar con los datos de los medios. (Y, claro, también los
extiende y aumenta, como lo vimos en la parte 1). Por ejemplo, la simulación de los
impresos incluye técnicas para escribir y editar texto (copiar, cortar, pegar, insertar);
técnicas para modificar la apariencia del texto (cambiar fuentes o color de texto) y el
diseño del documento (definir márgenes, insertar números de páginas, etc.); y, técnicas
para visualizar el documento final (ir a la página siguiente, ver múltiples páginas, zoom,
crear un separador). Igualmente, la simulación del cine con software incluye las técnicas
de la cinematografía como el enfoque, la gramática de movimiento de cámara (paneo,
inclinación, zoom), los tipos lentes que definen lo que se ve en una escena, etc. La
simulación del video análogo incluye un set de navegación: reproducción, reproducción
inversa, adelantar, reproducción continua (loop), etc. O sea, simular un medio con software
significa simular sus herramientas e interfaces, en lugar de sus “materiales”.
Antes de su “softwareización”, la técnicas disponibles en un medio particular eran parte de
su “hardware”. Este hardware eran los instrumentos para inscribir información en algún
material, modificarla y (si la información no era directamente accesible a los sentidos
humanos, como era el caso de las grabaciones de audio) presentarla. Juntos, materiales e
instrumentos, determinaban lo que un medio dado podía hacer.

171
Por ejemplo, las técnicas disponibles para escribir eran determinadas por las propiedades
del papel y los instrumentos de escritura, como las plumas fuente o la máquina de escribir
(Por ejemplo, el papel permite hacer y sobreponer marcas, que se pueden borrar si están
hechas a lápiz, pero no con tinta, etc.). Las técnicas cinematográficas están determinadas
por las propiedades del celuloide y los instrumentos de filmación (es decir, las cámaras).
Debido a que cada medio usaba su propio material e instrumentos físicos, mecánicos o
electrónicos, cada uno también desarrollo su propio conjunto de técnicas, con ligera
superposición.
Entonces, como las técnicas de los medios eran parte de un hardware incompatible con
otro, esto impedía su hibridación. Por ejemplo, aún cuando era posible borrar una palabra
escrita a máquina de escribir o sobre-escribirla, esto no se podía con una película ya
revelada. O, se podían hacer alejamientos mientras se filmaba progresivamente una
escena, revelando así más información, pero no se podía hacer lo mismo mientras se leía
un libro (es decir, no se podía reformatear un libro para ver todo el capitulo de un solo
vistazo). La interfaz del libro impreso sólo permitía accede a la información con un nivel de
detalle constante: todo lo que cupiera en dos páginas juntas177.
La simulación con software libera la creación de medios y las técnicas de interacción de
sus respectivos hardware. Cuando las técnicas se traducen en software se vuelven
algoritmos. ¿Pero qué pasa con los materiales físicos de los diferentes medios? Parece
que, en el proceso de simulación, son eliminados. En su lugar, los algoritmos de los
medios, como todo el software, funcionan en un solo material: los datos digitales, los
números.
Sin embargo, la realidad es más compleja y más interesante. Las diferencias entre los
materiales de distintos medios no se evaporan en el aire. En lugar de una variedad de
materiales físicos, los medios computacionales usan diferentes maneras de codificar y
almacenar la información (diferentes estructuras de datos). Y aquí viene el punto crucial.
177 Ésta era una de las convenciones de los libros que, a inicios del siglo XX, tuvo poetas y
diseñadores detractores como los modernistas Marinetti, Rozanova, Kruchenykh, Lissitzky y demás.

172
En lugar de un gran número de materiales físicos, las simulaciones con software usan un
número más pequeño de estructuras de datos.
(Una nota sobre mi uso del término “estructura de datos”. En las ciencias
computacionales, una estructura de datos se define como “una forma particular de
guardar y organizar datos en una computadora, con el fin de que puedan ser usados de
forma más eficiente”. Algunos ejemplos de estructuras de datos son los arreglos, las listas,
y los árboles. Aquí quiero darle al término un sentido diferente. Hago referencia a
representaciones de “nivel más alto” que son centrales a los medios computacionales
contemporáneos: una imagen bitmap, una imagen vectorial, un modelo poligonal 3D,
modelos NURBS, un archivo de texto, un archivo HTML, y otros. Las TI, los medios y la
cultura siguen dando vueltas alrededor de estos formatos pero aún no tienen un nombre
unificado. Para mí, el término “representación” tiene mucha carga cultural, mientras que
“tipo de dato” suena muy técnico. Prefiero “estructura de datos” porque tiene, al mismo
tiempo, un sentido para las ciencias computacionales y para las humanidades (la
“estructura”). El término seguirá recordándonos que lo experimentamos como “medios”,
“contenido”, o “artefacto cultural”, es técnicamente un conjunto de datos organizados de
una forma particular.)
Pensemos en los tipos de materiales que se pueden usar para crear imágenes 2D (piedra,
pergamino, lienzo, tipos de papel). Añadamos las películas fotográficas, acetatos rayos X,
celuloides, etc. La imagen digital sustituye todos estos materiales con sólo dos estructuras
de datos. La primera es la imagen bitmap (un mapa de “elementos pictoriales” discretos,
como los pixeles, cada uno con su propio valor cromáticos o en escala de grises). La
segunda es la imagen vectorial, que consiste en formas y líneas definidas por ecuaciones
matemáticas.
¿Y entonces qué pasa con todos los efectos que era posible de lograr con estos materiales
físicos? Dibujar sobre una superficie rugosa produce un efecto diferente que dibujar sobre
un papel suave. Tallar una imagen en madera es diferente que grabarla sobre metal. Con
la softwareización, estos efectos pasan del “hardware” (materiales y herramientas físicas)
al “software”.

173
Los algoritmos para crear y editar imágenes bitmap operan sobre la misma estructura de
datos (una cuadrícula de pixeles). Aunque usan diferentes etapas computacionales, el
resultado final de estos cálculos siempre es el mismo (una modificación de los colores de
algunos pixeles). Dependiendo de cuáles pixeles se están modificando, y de qué manera,
los algoritmos pueden simular visualmente los efectos de las técnicas de dibujo, pintura y
grabado sobre diferentes superficies.
Antes, los efectos de un medio eran el resultado de la interacción entre las propiedades de
la herramienta con el material. Ahora es sólo cuestión de algoritmos, que modifican un
solo material, por así decirlo (una estructura de datos común). Hoy podemos aplicar
primero un algoritmo que actúa como pincel sobre lienzo, luego un algoritmo que dará un
efecto de acuarela sobre papel corrugado, luego una pluma fina sobre papel sueva, y así.
En pocas palabras, las técnicas de medios previamente separados ahora se pueden
combinar en una misma imagen. Y como las aplicaciones de medios, como Photoshop,
vienen con una docena de estos algoritmos ya integrados (presentado al usuario como
herramientas y filtros, con sus debidos controles y paneles), ésta posibilidad teórica se
vuelve una práctica estandarizada. El resultado es un nuevo medio híbrido que combina
las posibilidades de muchos medios sueltos, que ya existían previamente.
En lugar de numerosos materiales e instrumentos separados, ahora podemos usar una
sola aplicación de software, cuyas herramientas y filtros pueden simular diferentes
técnicas de creación y modificación de medios. Los efectos que antes no se podían
combinar, debido a que dependían de un material en particular, ahora están disponibles
en un menú desplegable. Y cuando alguien inventa un nuevo algoritmo, o una nueva
versión de un algoritmo existente, se pueden añadir fácilmente a este menú mediante la
arquitectura del plug-in, ya un estándar desde los 90’s (el término “plug-in” fue acuñado en
1987 por los desarrolladores de Digital Darkroom, una aplicación de edición de fotos178). Y
por supuesto también se añadir la variedad de técnicas de creación y manipulación de
imágenes que no existían previamente: imagen aritmética, generación algorítmica de
texturas (como el filtro Nubes de Photoshop), filtros de desenfoque, etc. (se pueden ver
más ejemplos en los menús de Photoshop).
178 http://en.wikipedia.org/wiki/Digital_Darkroom, febrero 19, 2012.

174
Para resumir este análisis, la simulación con software substituye una variedad de
materiales y herramientas usadas para inscribir información (es decir, hacer marcas) sobre
estos materiales mediante un nuevos medio híbrido definido por una estructura de datos
común. Gracias a esta estructura común, múltiples técnicas que antes eran únicas a
diferentes medios ahora pueden ser usadas de forma conjunta. Al mismo tiempo, nuevas
formas que no existían antes también pueden ser agregadas, siempre y cuando operen
con la misma estructura de datos.
(Nota: mucho formatos estándar de imagen contemporánea, como el formato .PSD de
Photoshop, son mucho más complejos que una simple cuadrícula de pixeles (también
incluyen canales alfa, capas, perfiles de color, etc.). También pueden combinar
representaciones bitmap y vectoriales. Sin embargo, para mi estudio sólo estoy hablando
de su común denominador, que al final es también sobre lo que trabajan los algoritmos de
una imagen cargada en la memoria (un arreglo de pixeles con valores de colores).)
Vemos ahora otro ejemplo de lo que pasa cuando materiales físicos de diferentes medios
se simulan en el software. Pensemos en el software de modelación 3D, como Blender,
Maya, 3ds Max, LightWave 3D o Google SketchUp. Estas aplicaciones proveen las técnicas
necesarias para crear formas 3D que antes eran “cableadas” a diferentes medios físicos.
Por ejemplo, en herramientas de escultura se pueden modelar formas redondeadas como
si fueran de barro o arcilla. Las aplicaciones 3D también incluyen nuevas técnicas para
definir y modificar formas que no había antes: biselado, extrusión, esferizar, aleatoriedad,
booleanos, suavizar, “loft”, “morth”, simplificar, subdividir y demás179. Tal como pasa con
software de edición de imágenes, las nuevas técnicas se pueden añadir siempre y cuando
respeten las estructuras de datos que el software 3D reconoce. (Los más comunes son los
modelos poligonales y los modelos NURBS180. El primero está hecho de polígonos y el
segundo de curvas suavizadas). Estas estructuras de datos son los nuevos “materiales”
179 Para una lista de operaciones que se pueden hacer en modelos 3D basados en polígonos ver:
http://en.wikipedia.org/wiki/Polygon_modeling#Operations.
180 http://en.wikipedia.org/wiki/3D_modeling.

175
que el software sustituye por una variedad de materiales físicos usados por los humanos
para crear formas 3D física, como la piedra, la madrea, la arcilla y el concreto.
Estos dos ejemplos de estructuras de datos bitmap y 3D nos dicen porqué es incorrecto
pensar que las computadoras trabajan con un solo material digital, por así decirlo, es decir,
los códigos binarios hechos de 0 y 1. Claro que esto es lo que ocurre en el nivel más
profundo de la máquina (pero esto es irrelevante para los usuarios del software y para la
gente que lo escribe). El software de medios contemporáneos tiene sus propios
“materiales” (estructuras de datos usadas para representar imágenes fijas y animadas,
formas, volúmenes y espacios 3D, textos, composiciones de audio, diseños editoriales,
páginas Web y otros “datos culturales”). Estas estructuras de datos no corresponden a
materiales físicos en escala 1:1. Más bien, varios materiales físicos están mapeados en
una simple estructura (por ejemplo, los diferentes materiales de imagen como papel,
lienzo, película fotográfica y cintas de video se vuelven una misma estructura de datos:
una imagen bitmap). Este mapeo de muchos hacia uno, de materiales físicos hacia
estructuras de datos, es una de las condiciones que permite la hibridación de las técnicas
de medios.
¿Qué es un “medio”?
Hemos dedicado un espacio considerable al análisis de aquello que hace único al software
de medios respecto de los medios pre-digitales. Este análisis también nos ha permitido
comprender mejor porqué la hibridación se volvió la siguiente etapa de la evolución del
metamedio computacional y empezar a buscar las huellas de los mecanismos de la
hibridación. Veamos ahora si podemos usar estas ideas para responder a una de las
preguntas clave de este libro: ¿qué son los “medios” después de su softwareización?
Para evitar confusiones: no estamos hablando del contenido real de los programas y
fuentes de los medios, ya sean programas de televisión, periódicos, blogs o el terreno en
Google Earth. Ya existen numerosas disciplinas académicas que estudian el contenido de
los medios y su recepción: los estudios de medios, la comunicación, el periodismo, el
análisis cinematográfico y televisivo, los estudios de los juegos, los estudios culturales, los

176
estudios de internet. Tampoco hablamos de las industrias de los medios (producción,
distribución, recepción, mercados, aspectos económicos, etc.) debido también hay
diversas disciplinas que se ya ocupan de ello. Sin embargo, lo que no se hace
comúnmente es observar detenidamente las herramientas, tecnologías y flujos de trabajo
usados para producir el contenido de los medios. Pero cuando lo hacen, éste análisis se
lleva a cabo en relación a las herramientas de un medio en particular. Esto sucede porque
el estudio académico moderno de la cultura sigue las industrias culturales comerciales con
su estricta división por tipo de contenido. Así, los estudios de juegos estudian juegos, el
análisis cinematográfico y televisivo se enfoca en películas y programas de TV, los estudio
de internet observan el web, etc. Debido a las divisiones, estas disciplinas dejan de lado
características comunes a todos los medios y a la producción cultural que se hace
actualmente, que es el resultado de la dependencia en una misma tecnología, el software
de aplicaciones para la creación y edición de medios (ésta es una de las razones por las
que necesitamos la perspectiva de los estudios del software, para enfocarnos en patrones
culturales comunes relacionados con el uso de tecnología de software en todos sus
campos e diferentes industrias de medios).
También trataremos de definir lo que son los “medios” para sus creadores, en lugar de sus
consumidores. (Mientras que en los 2000’s había una considerable discusión sobre el
desvanecimiento de estas definiciones, sobretodo por la reducción de los precios de
herramientas y la emergencia del web social, en la práctica no se han borrado). Y
finalmente, la gente que sabe de programación computacional y puede crear medios
mediante el desarrollo de programas tiene un entendimiento diferente de los medios (pero
la mayoría de las personas que crean contenidos de medios usan solamente software de
aplicaciones).
El universo de los usuarios de software de aplicaciones incluye “creativos profesionales”:
artistas de gráficos animados, diseñadores gráficos, fotógrafos, editores de video,
diseñadores de productos, arquitectos, artistas visuales, etc. También se pueden incluir a
los “prosumidores” que hacen mezclas de animé, que editan videos para subirlos a
YouTube o Vimeo, que toman fotos que publicarán en Flickr o que subirán sus imágenes
artísticas a deviantArt.

177
Queremos entender lo que significa crear “medios” para todos ellos, definidos por las
posibilidades del software que usan (Photoshop, Gimp, Illustrator, InDesign, After Effects,
Final Cut, Premiere, CinePaint, Maya, Dreamweaver, WordPress, Blogger, Flash,
OpenOffice, Pages, Microsoft Word, Flame, Maya, etc.). (Y hablando de Word y otras
aplicaciones de procesamiento de texto, también debemos añadir los millones de
personas que lo usan a diario y quienes, por ende, pueden ser considerados expertos o,
por lo menos, prosumidores de un medio).
Recordemos una de las definiciones de diccionario de “medio” que abre el capítulo Para
entender los metamedios: “un tipo específico de técnica artística o medio de expresión
determinados por los materiales usados o los métodos creativos involucrados”. (Como
ejemplo, el diccionario se refiere al “medio de la litografía”). Así, diferentes medios tienen
diferentes técnicas, medios de expresión y métodos creativos. Estas diferencias no
desaparecen cuando cambiamos a aplicaciones de software. Por ejemplo, aparte de las
diferencias de representación y expresión evidentes entre un modelo 3D y una imagen en
movimiento, un diseñador que está modelando un personaje en Maya y una diseñador que
está haciendo una animación en After Effects tendrán acceso a diferentes herramientas.
¿Pero habrá similitudes conceptuales entre la forma en que estos dos diseñadores
trabajan por medio del software de medios?
O sea: qué son hoy los “medios”, definidos por las aplicaciones de software usadas para
crearlos y editarlos.
Como ya hemos dicho, los medios físicos, mecánicos y electrónicos existentes consistían
en dos componentes: materiales usados para almacenar información y algunas
herramientas y equipo usado para grabar, editar y ver esta información. Por ejemplo, “el
medio cinematográfico” usaba stocks de film para almacenar información, una cámara
cinematográfica para grabar, un proyector para mostrar las películas, y aparatos de edición
como Moviola y Steenbeck. El medio de grabado a mano usaba placas de acero (o cobre)
para guardar la información y herramientas especiales de metal para crearla (haciendo
surcos en la placa).

178
¿Es posible encontrar el equivalente de estos dos componentes en el software? He aquí
una respuesta: los materiales se vuelven estructuras de datos; las herramientas físicas,
mecánicas y electrónicas se transforman en herramientas de software que operan en
ellas. Desde esta perspectiva, sin importar el campo particular del medio, todos los
diseñadores y artistas que trabajan con software de medios hacen lo mismo: usan las
herramientas del software para crear y modificar datos, organizados en estructuras
particulares.
Esta respuesta puede ser atractiva pero no es precisa. La razón es que el paso de medios
físicos a aplicaciones de software implica una redistribución de los roles previamente
jugados por las herramientas físicas y los materiales. Cuando usamos una brocha de
acuarela y un papel rugoso, los trazos son el resultado simultáneo de la brocha, el agua y
el papel. Pero cuando usamos una brocha de “acuarela” en Photoshop, o aplicamos el
filtro “acuarela” a una imagen existente, el resultado está determinado únicamente por el
algoritmo, que modifica los colores de los pixeles de una forma particular. Los pixeles sólo
son ubicaciones en la memoria que tienen valores cromáticos; no tienen ninguna
propiedad per se, contrario a los materiales físicos.
Por lo tanto, no tenemos un mapeo uno-a-uno entre materiales físicos y estructuras de
datos. Una misma estructura de datos, como una imagen bitmap, puede ser usada para
simular varias técnicas de imaginería: de la acuarela y el grabado a la fotografía. En
concreto, observa las imágenes JPG que tienes en tu computadora. Algunas fueron
cargadas desde una cámara digital o de la cámara del teléfono celular; otras son
pequeños iconos usadas por las aplicaciones; y otras más son creaciones personales
hechas con diversas aplicaciones. La misma estructura de datos contiene múltiples
medios.
Por esta razón, en lugar de decir que los materiales se vuelven estructuras de datos o que
las herramientas se vuelven algoritmos, sería más correcto decir que un medio, tal como
es simulado en software, es la combinación de una estructura de datos y un conjunto de
algoritmos. La misma estructura de datos puede ser compartida a través de diferentes
simulaciones de medios pero al menos algunos algoritmos seguirán siendo únicos a cada
medio.

179
Hemos llegado a la definición de “medio” de software, que puede ser escrita así:
Medio = algoritmos + una estructura de datos
Los algoritmos y las estructuras de datos son dos elementos fundamentales de la
programación computacional. De hecho, uno de los libros más influyentes de las ciencias
computacionales es Algorithms Plus Data Structure Equals Programs de Niklas Wirth,
publicado en 1975. Wirth y otros científicos computacionales conceptualizaron el trabajo
intelectual de la programación como dos partes interconectadas: creación de estructuras
de datos que cumplen lógicamente con la tarea que debe ser realizada, de forma
computacionalmente eficiente, y la definición de algoritmos que puedan operar en estas
estructuras de datos.
Podemos usar este modelo conceptual de la programación computacional para afinar
nuestro entendimiento de lo que hacen las aplicaciones de los medios. Todas las
aplicaciones, incluyendo el software de medios, son programas computacionales, así que
internamente implican algoritmos que funcionan sobre estructuras de datos. Este punto,
de forma aislada, no es muy revelador. Lo que es importante es que estos dos elementos,
para mí, también definen el modelo mental que el usuario tiene de la aplicación. En otras
palabras, el modelo mental del usuario refleja la estructura abstracta de una programa
computacional (algoritmos que operan sobre estructuras de datos), que subyace el la
particular aplicación de software de medios.
Dentro del ambiente de una aplicación, un usuario trabaja con uno o más documentos con
contenido estructurado en alguna forma particular. El usuario tiene conciencia de la
importancia de esta estructura, aún cuando las aplicaciones de los medios no usan el
término “estructura de datos”. El usuario entiende que la estructura de datos determina el
tipo de contenido que puede ser creado y las operaciones que pueden ser usadas para
darle forma y modificarlo. Si elegimos gráficos vectoriales como formato, esto implica que
estaré creando líneas rectas y curveadas, y degradados; será posible modificar cualquier
forma ulteriormente, sin pérdida de calidad; también será posible obtener calidad de
impresión perfecta a cualquier resolución. Si nuestro formato son imágenes basadas en

180
bitmap, podemos trabajar con fotografías, desenfocar o acentuar detalles, pintar por
encima, aplicar filtros y demás. Pero el precio a pagar por esta flexibilidad es que la
imagen tendrá imperfecciones si la agrandamos consecuentemente181 (En la práctica, esta
selección se hace al momento de elegir la aplicación principal que contendrá el proyecto.
Si es Illustrator implica que trabajaremos con imágenes vectoriales; si es Photoshop
implica que trabajaremos con imágenes bitmap. Aunque ambos programas soportan los
dos formatos, la mayor parte de las funciones y su interfaz están organizadas alrededor de
este tipo “nativo”).
Otro recordatorio importante: el término “estructura de datos” tiene un significado
particular en la ciencias computacionales, refiriéndose a cómo deben ser procesados los
datos por un programa. Como ya lo dijimos, intentamos mantener la idea de la
organización de datos pero no estamos interesados en cómo sucede esto en su nivel más
bajo (es decir, que el programa organice los datos mediante arreglo, listas, apuntadores,
etc.). Más bien, usamos este término para referirnos al nivel de organización de datos, que
es visible y accesible a un usuario y que se vuelve parte del modelo mental para crear y
editar medios. (Por ejemplo, cuando estamos en Photoshop podemos hacer un zoom para
examinar pixeles individuales; podemos revisar la resolución de la imagen en pixeles;
podemos ajustar el diámetro del pincel en pixeles, y demás. Todo esto recuerda que
estamos en una matriz de pixeles).
Para sintetizar esta discusión: hemos sugerido que tanto teórica como experimentalmente
(por lo menos para los usuarios que tienen experiencias más próximas con el software de
medios) los “medios” se traducen en dos partes que trabajan juntas. Una parte es un
número reducido de estructuras de datos básicas (o “formatos”) que son la esencia del
software de medios moderno: imágenes bitmap, imágenes vectoriales, modelos 3D
poligonales, modelos 3D de NURBS, textos ASCII, texto de marcado (como RDF o HTML,
XML). La segunda parte son los algoritmos (que también se pueden denominar
“operaciones”, “herramientas” o “comandos”) que operan en dichos formatos.
181 Un ejemplo de formato de imagen vectorial es el popular AI de Illustrator; en cuanto a formatos
bitmap, están JPG y PSD. Algunos formatos como EPS, PDF y SWS pueden contener ambos.

181
La forma en que estas dos partes son actualizadas en las aplicaciones de medios merece
una discusión adicional. Primero, las diferentes aplicaciones ofrecen seguido otros detalles
además de los básicos para darles mayor funcionalidad. Por ejemplo, una imagen, tal
como es definida por las capacidades de Photoshop (una aplicación profesional y más
cara) es sustancialmente diferente a como es definida por Apple iPhoto o Picasa que son
más baratas e incluso en línea. Para Photoshop (versión CS5.5), una imagen es una
estructura jerárquica compleja. Al inicio es una simple imagen bitmap, una matriz de
pixeles. Un pixel es un elemento mínimo que le usuario puede seleccionar y modificar. Esta
es la estructura de datos que Photoshop comparte con otros editores de imágenes.
(Notemos que, aunque el usuario no puede seleccionar partes de un pixel, los algoritmos
trabajan comúnmente a niveles sub-pixel). Un documento de Photoshop puede tener
muchas de esas matrices de pixeles y se les llama “capas”. Las capas también se pueden
agrupar o pueden ser versiones alternativas de una composición. Cualquier capa de un
documento puede tener varios estados: un usuario la puede hacer visible o invisible,
cambiar su transparencia, hacer que configure o influya las capas de abajo, etc. Photoshop
también tiene capas especiales de ajustes que no contienen contenido en forma de
pixeles. Según la documentación del programa, “una capa de ajustes aplica ajustes de
color o tono a una imagen sin cambiar de manera permanente el valor de los pixeles” 182.
Adicionalmente, una imagen puede tener máscaras que definen áreas de la imagen que
pueden ser editadas. Otros elementos que Photoshop añade a un estructura de datos
básica son gráficos vectoriales, guías y tipografía.
La interfaz de usuario de Photoshop emplea ventanas y menús para presentar esta
compleja estructura de la imagen con todas sus posibilidades. La ventana de Documento
muestra la composición actual. El panel de Capas muestra las capas, efectos de capas y
grupos de capas en la composición. El panel de Canales muestra los componentes de color
de la imagen (como R, G y B). Cada una de estas ventanas tiene varios controles y menús
dedicados a crear, ver y modificar las partes de la imagen.
Si las estructuras de datos forman una parte del modelo mental del usuario para la
creación de medios, las operaciones que se pueden realizar en estas estructuras son una
182 http://help.adobe.com/en_US/Photoshop/11.0/.

182
segunda parte. Esto es, un usuario también entiende que el proceso de definir y editar
contenido implica la aplicación secuencial de diferentes operaciones dadas por la
aplicación. Cada operación corresponde a un algoritmo, que ejerce alguna acción sobre
datos existentes o que genera datos nuevos (el filtro Onda de Photoshop es un ejemplo del
primero, mientras que Nubes es un ejemplo del segundo).
En el software de medios contemporáneo, a las herramientas que son representadas
mediante elementos de un menú se les llama “comandos”. El término “herramienta” está
reservado a las operaciones usadas frecuentemente, a las que se les da su propio ícono y
pueden ser seleccionadas directamente, sin tener que navegar los menús. (Aquí usaremos
la palabra “herramienta” para referirnos a ambos tipos). Las aplicaciones agrupan
operaciones relacionadas. Por ejemplo, Photoshop CS5.5 ubica sus herramientas clave en
el panel Herramientas; otras herramientas adicionales se pueden localizar en los menús
desplegables Editar, Imagen, Capa, Selección, Filtro y Vista. Muchas aplicaciones de
medios también hacen disponibles otras herramientas en forma de scripts que se pueden
correr desde la aplicación. Por ejemplo, en Photoshop CS5.5 estos scripts pueden provenir
de terceros desarrolladores y aparecen en Archivo>Scripts. Estos scripts se pueden escribir
en JavaScript, VB Script, AppleScript y demás lenguajes. Finalmente, la gente que usa
interfaces de línea de comando como Unix (o Linux) también puede usar un tercer tipo de
operaciones: diferentes programas de software que se pueden correr director desde la
línea de comando. Por ejemplo, dos de los programas más usados para la conversión y
edición de imagen y video son ImageMagic y Ffmpeg183. Debido a que estos programas no
tienen una interfaz gráfica de usuario, no son apropiadas para la manipulación interactiva
de imágenes, sin embargo sobresalen en la automatización y se usan para operaciones de
lote (como la conversión de un formato a otro) en grandes cantidades de archivos.
Sin importar que las herramientas de los medios se presentan vía una GUI, scripts o
programas independientes accesibles desde la línea de comando, todos tienen algo en
común: sólo pueden trabajar en una estructura de datos particular. Por ejemplo, las
aplicaciones de edición de imágenes definen docenas de herramientas para editar
183http://www.imagemagick.org/ y http://ffmpeg.org/

183
imágenes bitmap184, pero éstas no funcionarían en gráficos vectoriales. Siguiendo en la
misma línea, las técnicas para modificar modelos 3D que definen el volumen de un objeto
son diferentes de las técnicas para representar los límites de los objetos (como los
modelos poligonales).
Haciendo una analogía con el lenguaje, podemos comparar las estructuras de datos con
los sustantivos y los algoritmos con los verbos. Una analogía con la lógica sería comprarlos
con los sujetos y predicados. Como toda metáfora, éstas destacan y distorsionan, revelan y
ocultan. Sin embargo, espero que puedan ayudarme a transmitir mejor mi argumento: la
naturaleza dual de un “medio” como es definida por las aplicaciones de software.
Hemos llegado al punto de responder a la pregunta que hicimos en la introducción del
libro: ¿qué son los medios hoy tal como son definidos por las aplicaciones de software para
su creación y edición? Tal como son definidos por los software de aplicación y
experimentados por los usuarios, un “medio” es una vinculación de estructuras de datos
particulares y algoritmos para la creación, edición y visualización del contenido guardado
en su estructura.
Ahora que hemos establecido que los medios computacionales implican una vinculación
entre algoritmos y estructuras de datos, podemos entender mejor la distinción entre las
técnicas específicas e independientes de los medios. Una técnica específica es un
algoritmo que puede solo puede operar en una estructura de datos particular. Por ejemplo,
los filtros de desenfoque y asentamiento solo pueden funcionar en imágenes bitmap; la
operación de “extrusión”, usado comúnmente en los programas 3D para modelar, sólo
puede ser aplicada a una curva vectorial. En contraste, una técnica independiente es un
conjunto de algoritmos que ejecutan una tarea conceptualmente similar pero son
implementados para trabajar en un número de estructuras de datos. Ya hemos dado
ejemplos de estas técnicas cuando introdujimos el concepto: organizar, buscar, zoom,
184 Este artículo de Wikipedia enlista operaciones en imágenes comunes en estos programas:
http://en.wikipedia.org/wiki/Image_editing.

184
cortar, copiar, pegar, randomizar y varias manipulaciones de archivos (copiar, enviar por
mail, subir, comprimir, descargar), etc.
Para explicar cómo las técnicas independientes pueden ser implementadas, vemos los
comandos Copiar, Cortar y Pegar. Estas operaciones ya existía en algunos editores de texto
computacionales de los 60’s. En 1974-1975, Larry Tesler implementó estos comandos en
su editor de texto como parte de su trabajo en Xerox PARC sobre computadoras
personales185. Al ver que estos comandos se podían usar en todo tipo de aplicaciones, los
diseñadores de la Xerox Star (lanzada en 1981) incluyeron teclas especiales para estos
comandos186. El teclado tenía teclas marcadas con Repetir, Encontrar, Igual, Abrir, Borrar,
Copiar, Unir y Mover. Un usuario podía seleccionar cualquier objeto en una aplicación o en
el escritorio y después lanzar cualquier de estos comandos. El equipo de Xerox PARC los
llamo “comandos universales”. Apple, de manera similar, hizo disponibles estos comandos
en todas las aplicaciones que corren en su GUI universal, pero se deshizo de las teclas187.
En su lugar, los comandos se encuentran bajo el menú desplegable Editar.
La idea de que el usuario pudiera seleccionar objetos en cualquier documento sin importar
el medio, o el archivo, y usar los mismos comandos en sus objetos es una de las
invenciones más importantes de Xerox PARC. Da al usuario un solo modelo mental para
trabajar con documentos en diferentes aplicaciones y simplifica el aprendizaje de nuevos
programas.
Así es como los diseñadores de la Xerox Star describieron uno de estos comandos
universales:
MOVERSE es uno de los comandos más poderosos del sistema. Se usa mientras
editamos un texto para recomponer las letras de una palabra, las palabras de una
185 http://en.wikipedia.org/wiki/Cut,_copy,_and_paste.
186 http://en.wikipedia.org/wiki/Xerox_Star, febrero 20, 2012.
187 Un acercamiento y demostración de estas teclas especiales puede verse en este video,
especialmente en la parte sobre comandos universales del teclado de la Xerox Star:
http://www.youtube.com/watch?v=Cn4vC80Pv6Q&feature=relmfu, agosto 4, 2012.

185
oración, las oraciones de un párrafo, y los párrafos de un documento. Se usa
durante la edición de gráficos para mover elementos visuales, como líneas y
rectángulos, alrededor de una ilustración. Se usa mientras se edita una fórmula
para mover estructuras matemáticas, como aditivos e integrales, alrededor de una
ecuación188.
Sin embargo, según el tipo de aplicación de medios y el tipo de objetos que el usuario
selecciona, “copiar”, “cortar” y “pegar” accionan diferentes algoritmos. Por ejemplo, copiar
una frase en un documento necesita diferentes secuencias de operaciones que copiar una
selección en una imagen bitmap. En el primer caso se trata de una secuencia de
caracteres unidimensional, y en el segundo es un conjunto de pixeles dentro de un área de
dos dimensiones. Incluso en una misma aplicación, se emplean varios algoritmos para
cubrir los objetos que el usuario tiene a la disposición.
Un segundo ejemplo de cómo la implementación de técnicas independientes de los
medios implica diferentes algoritmos que funcionan en un medio en particular es la
generación de objetos aleatorios. El algoritmo que genera una secuencia de números
aleatorios es muy sencillo, simplemente llama al generador de números (una función
disponible en todo lenguaje de programación) para producir suficientes partes, luego
escala esos números dentro los límites especificados por el usuario (por ejemplo entre 0.0
y 1.0). Esta parte es independiente de los medios. Diferentes aplicaciones pueden usar
esta función de generación de números aleatorios como parte de un algoritmo específico a
un medio (es decir, algoritmos que funcionan en estructuras de datos particulares) para
crear diferentes tipos de contenidos. Por ejemplo, el comando “Añadir ruido” en Photoshop
(Filtros>Ruido) genera un conjunto de pares aleatorios X-Y y los usa para seleccionar
pixeles específicos en la imagen y cambiar sus colores o valores de gris (según como
decida el usuario). Una aplicación de modelado 3D también puede usar la misma técnica
para generar un conjunto de objetos idénticos en ubicaciones espaciales aleatorias. El
software de edición de audio puede generar ruido aleatorio, etc.
188 David Canfield Smith et al., “Designing the Star User Interface,” Byte, issue 4 (1982): 242-282.

186
La implementación de técnicas independientes de medios es estructuralmente similar a
varios sistemas estéticos en el arte, que fueron aplicados en diferentes medios. Por
ejemplo, el barroco, el neo-clasicismo, el constructivismo, el post-modernismo, y el remix.
Cada sistema se manifestó en diferentes medios. Así, la estética barroca puede
encontrarse en la arquitectura, la escultura, la pintura y la música; el constructivismo fue
aplicado en el diseño de producto, diseño gráfico, ropa, teatro y quizá en la poesía y en el
cine189. Pero tal como en las técnicas específicas, realizar un sistema estético particular en
diferentes medios requiere artefactos específicos para explorar las posibilidades y trabajar
con las limitantes de cada medio.
Formatos de archivo
El software usa archivos para almacenar y transferir datos. Por ejemplo, en Photoshop
salvamos imágenes junto con sus canales, capas, grupos, guías y demás información que
queda guardada en un formato particular. Junto con las estructuras de datos, los
algoritmos y las interfaces de usuario, un formato de archivo es otro elemento
fundamental de los medios computacionales. Se trata de mecanismos estandarizados
para almacenar y acceder a los datos organizados en una estructura determinada. Algunos
formatos de archivo son de domino público, como .rdf; otros son propietarios, como .doc.
Como veremos en la sección “El flujo de trabajo del diseño” de la siguiente parte del libro,
la estandarización de formatos de archivo es una condición esencial para su
interoperabilidad entre aplicaciones que, a su vez, afecta la estética de los medios creados
con esas aplicaciones. Desde el punto de vista de la teoría de medios y de la estética, los
formato de archivo constituyen la “materialidad” de los medios computacionales (porque
los bits organizados en estos formatos es lo que se inscribe en un dispositivo de
almacenamiento cuando se guarda, y también porque los formatos de archivo son mucho
más estables que otros elementos de los medios computacionales).
189 Vlada Petric, Constructivism in Film - A Cinematic Analysis: The Man with the Movie Camera
(Cambridge University Press, 1993).

187
Como los materiales y las herramientas de medios físicos están hoy implementadas como
software, en teoría se podrían crear fácilmente nuevos formatos de archivo y nuevos
algoritmos, así como ampliar los ya existentes. (Recordemos mi discusión previa sobre la
“Extensibilidad permanente”). No obstante, al contrario de los años 60’s y 70’s cuando
unos cuantos investigadores inventaban gradualmente el medio computacional, hoy el
software es una industria global enorme. Esto significa que la innovación del software está
conducida por factores económico-sociales, más que por posibilidades teóricas. Mientras
los formatos de archivo sean constantes, es fácil añadir nuevas herramientas en versiones
subsecuentes, mientras que las herramientas previas pueden seguir existiendo sin
modificación alguna. Además, en contraste con el periodo de Kay y demás gente que
estaba definiendo el “primer metamedio”, hoy millones de usuarios profesionales (así
como firmas de diseño y arquitectura, estudio de cine, agencias de marketing, compañías
de diseño web, y demás asociaciones creativas alrededor del mundo) guardan sus trabajos
y sus recursos (modelos 3D, fotografías, diagramas de impresión, sitios web, etc.) como
archivos digitales en formatos particulares: .doc, .pdf, .tiff, .html, etc. Si los formatos de
archivo cambiaran todo el tiempo, el valor de los recursos que posee un individuo o
compañía estarían en riesgo.
Como resultado, en la práctica los formatos de archivo casi nunca cambian. Por ejemplo, el
formato JPEG se usa desde 1992 y TIFF remonta a 1986. Por el contrario, la modificación
de las herramientas de software que trabajan con esos archivos y la creación de nuevas
herramientas ha sucede a un ritmo veloz. Cuando una compañía lanza una nueva versión
de su software de aplicación, generalmente añade varias nuevas herramientas y reescribe
otras, pero el formato de archivo sigue igual. Esta estabilidad de formatos de archivo
también permite que otros desarrolladores creen nuevas herramientas basadas en estos
formatos. Dicho de otra forma, es una de las condiciones que permite la “extensibilidad
constante” del software de los medios. He aquí un ejemplo: cuando visité el área de plugin’s
de Adobe Photoshop el 5 de agosto de 2012, se enlistaban 414 de ellos190. Dado que
190
http://www.adobe.com/cfusion/marketplace/index.cfm?event=marketplace.categories&marketplac
eId=2&offeringtypeid=5, agosto 5, 2012.

188
un solo producto puede incluir desde docenas hasta miles de filtros y acciones, el número
total plug-in’s disponibles puede llegar a los cientos de miles.
Cada formato de archivo y su correspondiente estructura de datos tiene sus fortalezas y
debilidades. Una fotografía representada como imagen bitmap puede dar una apariencia
pintoresca; ser difuminada o agudizada; o compuesta con otra fotografía, etc. Estas
operaciones son más difíciles o casi imposibles con una imagen vectorial. A la inversa, es
mucho más fácil editar curvas complejas si son representadas internamente como
fórmulas matemáticas (el formato usado por programas de dibujo vectorial como
Illustrator, Freehand e Inkscape). Como muchos proyectos requieren la combinación de
efectos que solo son posibles con diferentes estructuras de datos (como “raster” y
vectoriales), con el tiempo las aplicaciones de software profesionales han sido ampliadas
para soportar otros formatos de archivo además sus tipos nativos. Por ejemplo, sabemos
que las mayoría de las herramientas de Photoshop CS4 están orientadas a la
manipulación de imágenes raster, pero también se incluyen algunas herramientas para
trabajar con dibujos vectoriales. Photoshop puede importar vectores e Illustrator puede
importar bitmaps.
Sin embargo, esta hibridación de aplicaciones de software no afecta el hecho que cada
herramienta de aplicación separada solo pueda trabajar con un tipo de estructura de
datos, pero no es así con otras. Esto es verdad para comandos universales como “cortar”,
“copiar”, “pegar” y “ver”, así como para una multitud de comandos específicos a los
medios como “contar palabras”, “difuminar”, “extrusión” y “echo”. En consecuencia, detrás
de las herramientas independientes y específicas hay algoritmos diseñados para trabajar
con estructuras de datos particulares. Pero un usuario entiende de manera diferente
ambos tipos, debido a que su implementación no es visible directamente. El primero reúne
conceptualmente todos los medios, e incluso crea un horizonte imaginario en donde las
diferencias entre ellos desaparecen; al mismo tiempo, el segundo enfatiza estas
diferencias debido a que sólo se vuelven disponibles cuando un usuario trabaja en un
medio en particular.
Como su nombre lo indica, los “archivos” computacionales hacen referencia a los archivos
de papel que fueron la principal tecnología de gestión de información hasta mediados del

189
siglo XX y hasta que las computadoras se desarrollaron. La palabra “archivo” fue usada en
1950 en una publicidad de RCA para su nuevo tubo de vacío de “memoria”. En 1952 la
palabra hacía referencia a la información almacenada en tarjetas perforadas191. Con el
desarrollo del web en los 90’s, los “documentos” web192, tales como las páginas, se
volvieron igualmente importantes. Una página web puede consistir de un archivo HTML con
texto estático y otros contenidos de medios almacenados en un servidor. Alternativamente,
una página web puede ser “dinámica”, que significa que se construye cuando el usuario
accede a su dirección o porque puede cambiar con la interacción del usuario193. Las
páginas web dinámicas se pueden construir en el lado del cliente con tecnologías script
como JavaScript o Flash; del lado del servidor, se pueden usar scripts PHP o Perl. Los dos
métodos se pueden combinar usando técnicas Ajax. Un ejemplo popular que usa Ajax es
Google Earth. En 2011, HTML5, la siguiente generación del estándar HTML, permitió la
inclusión de elementos multimedia, videos, audio y SVG directo en el código sin tener que
usar plug-in’s. Mientras éstas y otras tecnologías se desarrollan y adoptan gradualmente,
la identidad del web ha ido cambiando (de páginas estáticas a inicios de los 90’s a
“aplicaciones internet enriquecidas” que igual muchas funcionalidades de las aplicaciones
de escritorio tradicionales) 194.
Con la complejidad y variedad de documentos y tipos de aplicaciones web, la multitud de
tecnologías y técnicas para crearlos, y la continua evolución de tecnologías y convenciones
web, no sería apropiado adaptar mecánicamente nuestro concepto de estructuras de
datos al web. Sin embargo, si pensamos en el significado de la estructura de datos como
modelo mental de los medios compartidos entre diseñadores y usuarios, opuesto a sus
implementaciones técnicas, entonces sí aplica nuestro concepto a los documentos y
aplicaciones web. No obstante, en lugar de referirnos al tipo de medio y sus características
(texto, imágenes, sonidos, modelos 3D, etc.) que se vuelven elementos de un documento o
aplicación web, podemos usar el concepto para describir las posibilidades de interacción y
convenciones ofrecidas por el web. Por ejemplo, las páginas web siguen teniendo
191 http://en.wikipedia.org/wiki/Computer_file.
192 http://en.wikipedia.org/wiki/Web_document.
193 http://en.wikipedia.org/wiki/Dynamic_web_page.
194 http://en.wikipedia.org/wiki/Rich_Internet_application.

190
hipervínculos que permiten a los usuarios ir a páginas relacionadas. Hoy, una página web
puede tener “botones de medios sociales” que permiten al usuario compartir fácilmente
contenidos. Géneros particulares de documentos y aplicaciones web ofrecen sus propias
posibilidades de interacción, por ejemplo un blog contiene típicamente un lista de entradas
organizada por fecha, una aplicación de mail tienen botones para responder, reenviar y
archivar los mensajes, etc.
Aunque en principio podemos investigar los tipos de documentos y aplicaciones web más
usados (enlistando las convenciones tal como existen hoy mismo), esta lista sería muy
larga y no muy exacta para cuando este libro sea impreso (4 de julio de 2013). Esta es una
de las razones por las cuales nos enfocamos en un estudio de los medios como
representación, más que comunicación e interacción (porque las estructuras de
representación basadas en software son más estables, menos numerosas, menos
complejas y cambian menos frecuentemente que los software y redes basados en
tecnologías de comunicación e interacción). Esto no significa que nos rendimos ante el
proyecto de entender los medios web (incluyendo aplicaciones móviles que actualmente
se cuentan en cientos de miles), más bien espero hacerlo de forma más comprensible en
un futuro, limitándome por el momento a breves discusiones en este libro.
Parámetros
Como sugerimos anteriormente, el modelo mental de un usuario para crear y editar medios
(ya sea en el contexto de un tipo de aplicación en particular o con “medios digitales” en
general) tiene dos elementos fundamentales que, tanto conceptual como prácticamente,
corresponden a dos elementos de la programación computacional: algoritmos y
estructuras de datos. Cuando un usuario selecciona una herramienta del menú y la usa en
parte del documento en el que está trabajando, el algoritmo detrás de ella modifica la
estructura de datos, que tiene almacenado el contenido de dicha parte. Muchos usuarios,
claro, no conocen los detalles de cómo funciona un software al nivel de la programación y
quizá sólo estén vagamente conscientes de lo que es un algoritmo. (En 2011, estaba
manejando rumbo a San Francisco y vi un espectacular junto a la carretera que tenía
explícitamente la palabra algoritmo, pero no pude detenerme para tomar una foto que me

191
hubiera gustado incluir en este libro). Pero, aunque sean desconocidos para ellos, los
principios de la programación computacional están “proyectados” al nivel de la interfaz de
usuario (dando forma a cómo los usuarios trabajan con los medios a través de las
aplicaciones de software y cómo entienden éste proceso cognitivamente). El modelo de
datos del algoritmo/estructura es un ejemplo de esto. Veamos otro ejemplo de esta
”proyección”: las opciones y sus implementaciones.
Uno de los principios de la programación computacional moderna (sin importar el
paradigma o el lenguaje de programación) es el uso de los parámetros195. La popularidad
de los parámetros (también llamados “variables” o “argumentos”) se debe a unas pocas
razones. Una de ellas es una práctica de programación moderna que fragmenta el código
en funciones separadas. Si un programa debe ejecutar la misma secuencia de pasos
varias veces, el programador encapsula esta secuencia en una función que después se
invoca mediante su nombre cuantas veces sean necesarias. (Dependiendo del lenguaje de
programación, las funciones se pueden llamar procedimientos, métodos, subrutinas o
rutinas). El dividir una programa largo en funciones modulares separadas lo hace más fácil
de escribir, leer y mantener (este paradigma se llama programación procedural). Las
funciones que ejecutan tareas relacionadas conceptualmente se agrupan en librerías de
software. Estas librerías o bibliotecas existen en todos los lenguajes de programación
populares y su uso hace más rápido y eficiente el desarrollo de software. La definición de
una función incluye típicamente algunos parámetros que controlan los detalles de su
ejecución. Por ejemplo, una función que convierte una imagen de un formato a otro tendrá
un parámetro para especificar si el formato de salida es JPEG, PNG, TIFF u otro (y si
seleccionamos JPEG se puede que obtengamos otro parámetro para especificar el nivel de
compresión).
Le segunda razón de la popularidad de los parámetros es que muchas funciones (y
programas enteros) resuelven fórmulas matemáticas. Una fórmula define una relación
entre variable. Por ejemplo, una fórmula seno se parece a esto: y = A * sin (w*x + O), en
donde A es la amplitud, w es la frecuencia y O es la fase. Si implementamos esta fórmula
195 http://en.wikipedia.org/wiki/Argument_(computer_science)

192
en una función de software, la función tendrá parámetros para cada variable (es decir: w,
x, O).
Si no sabes programar, quizá el concepto de parámetro te sea familiar si usas fórmulas en
Excel o Google Spreadsheet. Por ejemplo, para generar una columna de número aleatorios,
puedes llenar las celdas con la función RAND(). La fórmula no tiene ningún parámetro,
simplemente genera número aleatorios entre 0.0 y 1.0. Si quieres generar número en un
rango particular, hay que usar la fórmula RANDBETWEEN(límite_alto, límite_bajo). Esta
fórmula tiene dos parámetros que especifican los valores mínimos y máximos del rango de
números. Por ejemplo, RANDBETWEEN(10, 20) genera un valores entre 10.0 y 20.0.
En las GUI modernas, los parámetros, que controlan la ejecución del programa, se llaman
opciones. Los usuarios especifican los valores para las opciones de los campos de texto,
deslizadores, botones, listas desplegables, botones de chequeo y botones de radio.
Algunas veces la interfaz ofrece algunos valores predefinidos para que el usuario
seleccione entre ellos. En otros casos, el usuario puede ingresar su propio valor. Y en otros
más la aplicación da ambas posibilidades. Por ejemplo, un típico selector de colores
permite determinar valores R, G y B para crear un color único; alternativamente también se
puede seleccionar de una muestra de color predefinida.
El uso de las opciones extiende ampliamente la funcionalidad de las aplicaciones de
software porque permite a una misma aplicación realizar una amplia gama de acciones.
Por ejemplo, imaginemos que necesitamos clasificar un conjunto de números. En lugar de
usar dos programas diferentes, uno para ejecutar la clasificación en orden ascendente y
otra para el orden descendente, podemos usar una sola aplicación en donde simplemente
especificamos el valor de parámetro para especificar el tipo de clasificación que
necesitamos. O imaginemos una herramienta de brocha redonda en el menú de
herramientas de una aplicación de edición de fotos. No nos gustaría ver una herramienta
separada para cada color y tamaño de brocha. En su lugar, una sola herramienta con
múltiples opciones puede controlar el color, el tamaño y demás opciones.
¿Qué significa esto para la teoría de medios? Con la softwareización, las posibilidades y las
diferentes maneras de usar las herramientas físicas dejan de ser implícitas: se vuelve

193
completamente explícitas. Se dan controles explícitos y detallados a las “técnicas
artísticas” y a los “medios de expresión” (recordemos la definición de medio que abre la
primera parte). Como todos los programas y funciones computacionales, ahora todo viene
con varios parámetros. En Photoshop CS5, la herramienta de brocha tiene estos
parámetros básicos: tamaño, dureza, modo, capacidad de aerosol, opacidad y flujo. La
dureza, la opacidad y el flujo tienen valores que van del 0 al 100; hay 25 modos de forma
a elegir; y el diámetro puede variar de 1 a 2500 pixeles.
¿Serán necesarias todas estas opciones y tal grado de precisión? Quizá no. Por ejemplo,
en lo que respecta a la opacidad, probablemente para la mayoría de los usuarios sea
suficiente controlarla a intervalos de 5%. Pero el algoritmo que implementa la opacidad es
exactamente el mismo, sin importar que los parámetros tengan 10 ajustes ó 100. Una de
las lógicas de la industria del software es ofrecer siempre “más” que las versiones previas
o que los competidores, por esto es comprensible que la interfaz de la brocha nos dé una
larga lista de opciones y opciones de valores, aunque esa precisión pueda no ser muy útil.
Lo mismo se puede decir de las otras herramientas disponibles en el software de medios.
De esta manera, la lógica de la programación se proyecta al nivel de la GUI y se convierte
en parte del modelo cognitivo del usuario cuando trabaja en el software de los medios.
A pesar de que el hecho de añadir más opciones requiere un esfuerzo adicional de
programación, no sucede igual cuando se trata de ofrecer más valores de las opciones. Si
necesitamos que la herramienta de brocha tenga una opción de transparencia, es
necesario escribir nuevo código para este comportamiento. Sin embargo, si queremos
cambiar los valores disponible al usuario, ya sea 20 ó 100, esto no cuesta nada. Como
cualquier otra herramienta de medios, la brocha es un algoritmo, que toma entradas y
genera salidas mediante la aplicación de fórmulas a dichas entradas. En el caso de la
brocha, las entradas son los valores de las opciones elegidos por el usuario, y el color de
los pixeles sobre los que se mueve la brocha en el área de trabajo; las salidas son los
nuevos valores de estos pixeles, es decir, sus transformaciones. A las fórmulas no les
interesa el valor de entrada, tampoco el número de pasos requerido para ejecutarlos.
Las herramientas de los medios físicos pre-industriales no tenían controles explícitos. No
había parámetros que seleccionar para una pluma, una brocha o un cincel. Si queríamos

194
cambiar el diámetro de una brocha, teníamos que buscar otra brocha. La era industrial
introdujo un nuevo tipo de herramientas de medios que eran máquinas mecánicas o
electrónicas: el telégrafo, la cámara fotográfica, el proyector de cine, el gramófono, el
teléfono, la televisión, la videocámara. Como las demás máquinas industriales, éstas
tenían pocos controles. Físicamente, parecían perillas, palancas y botones. La siguiente
generación de medios (el software de aplicaciones) dio controles explícitos a sus
herramientas. Las herramientas que antes no tenían controles explícitos, las adquirieron y
las que ya tenían fueron aumentadas. (Si las máquinas hechas de partes mecánicas solo
podían tener un número limitado de ajustes, los parámetros del software pueden tener
rangos de valores casi ilimitados).
En El lenguaje de los nuevos medios introduje la idea de “transcodificación” (aplicar
convenciones y convenciones de la ingeniería de software a conceptos y percepciones
culturales). La parametrización explícita de las técnicas de creación y edición de medios
implementadas en el software es un ejemplo perfecto de la lógica de la transcodificación.
Entonces, el que la brocha de Photoshop tenga varias opciones y controles, y que los
valores de la transparencia vayan de 0 a 100, tiene que ver sólo parcialmente con el
significado de este comando (la simulación de diferentes brochas físicas). La verdadera
razón de esta implementación del comando radica en su identidad como programa
computacional. (En la programación moderna, los programas y sus partes tienen
parámetros, que en muchos casos toman valores de entrada que son arbitrarios).
Además de ser un buen ejemplo de cómo los principios y convenciones del desarrollo de
software son llevados a las aplicaciones de medios, la parametrización también es útil
para explicar otra tendencia. Algunas actividades de medios que aparentemente son
diferentes (editar fotografías, crear personajes 3D, editar video, trabajar en un sitio web o
una aplicación móvil) se vuelven similares en su lógica y flujo de trabajo: seleccionar una
herramienta, elegir sus parámetros, aplicarlos… y repetir esta secuencia hasta que el
proyecto sea terminado.
Claro que no debemos olvidar que las prácticas de la programación computacionales están
incrustadas en las estructuras económicas y sociales de la industrias del software y de los
electrónicos consumibles. Estas estructuras imponen sus propias restricciones y

195
prerrogativas a la hora de implementar controles, opciones y preferencias de hardware y
software (estos términos son sólo diferentes manifestaciones de parámetros de software).
Si echamos un vistazo a la historia de las aplicaciones de medios y aparatos electrónicos,
se pueden identificar algunas tendencias. Primero, el número de opciones en las
herramientas software y en dispositivos comercializados por profesionales se incrementa.
Por ejemplo, un número significativo de herramientas y filtros Photoshop tiene ahora más
opciones y controles que sus versiones precedentes. Segundo, las nuevas propiedades
,que primero aparecen en tecnologías profesionales, más tarde se vuelven disponibles en
productos de los consumidores comunes. Sin embargo, para conservar su identidad y
justificar las diferencias de precio entre diferentes productos, el software y equipo
profesional tiene más opciones y parámetros que sus equivalentes para consumidores.
Así, Photoshop viene con muchas herramientas; Photoshop Elements vienen con menos, y
iPhoto y Picasa aún con menos.
Todo esto parece obvio pero aquí viene una tercera tendencia que es más interesante para
la teoría de medios. Siguiendo el paradigma establecido a finales del siglo XIX por la
compañía Kodak, cuando empezó a comercializar sus cámaras acompañadas del slogan
“usted oprime el botón, nosotros nos encargamos del resto” (1892), las aplicaciones de
software contemporáneas y dispositivos de medios para los consumidores automatizan
significativamente la captura y edición de medios en comparación con sus contrapartes
profesionales. Por ejemplo, durante los 2000’s muchas cámaras digitales para
“consumidores” sólo ofrecían exposición automática. Para tener controles manuales era
necesario pasar a la siguiente categoría de precios, a los cámaras “semi-profesionales”. En
otro ejemplo, a final de esa década, las cámaras para consumidores empezaron a
incorporar la detección automática de rostros y sonrisas, una propiedad que no estaba
disponible en cámaras profesionales costosas.
Como todo tipo de automatización, la exposición automática requiere más procesos
computacionales que los ajustes manuales. Lo mismo pasa cuando aplicamos “contraste
automático” y “tonos automáticos” en el software de medios. Por consecuencia, si
equiparamos el uso de las computadoras con la automatización, paradójicamente son los
consumidores quienes aprovechan más sus beneficios. En contraste, los profesionales
trabajan estos ajustes de forma manual, pero claro, es parte de su labor: lograr efectos y

196
resultados que la automatización predefinida no puede. Simultáneamente, al ofrecer una
automatización de más alto nivel en productos para consumidores, la industria
menosprecia las habilidades de los profesionales. Por ejemplo, hoy existen varias
aplicaciones web y plug-ins que pueden retocar fotos de retrato: corrigiendo contraste,
tono de piel y eliminando imperfecciones en un solo paso. Los resultados pueden ser
sorprendentemente buenos, quizá no tanto como para la portada de Vogue pero
suficientes para la foto de nuestro perfil Facebook.
Es interesante notar que a principios de los 2010’s ésta tendencia se ha parcialmente
invertido. Debido a que el tamaño del mercado de consumidores es más grande y a que
los ciclos de lanzamiento de productos son más rápidos, los creadores de hardware y
software empezaron a ofrecer algunas nuevas propiedad primero en productos de bajo
nivel y, después, en sus productos más caros. Por ejemplo, Apple Aperture 3 (2010) añadió
opciones que estaban disponibles en la versión 2009 de iPhoto: Faces (reconocimiento de
caras) y Places (un sistema para identificar ubicaciones geográficas de las fotos y ponerlas
sobre una interfaz cartográfica) 196.
¿El metamedio o el monomedio?
Debido a la similitud del funcionamiento de los “medios”, tal como son implementados en
software, surge une pregunta lógica: ¿es necesario hablar de diferentes medios después
de todo? En otras palabras, ¿el metamedio computacional es una colección de medios
simulados, nuevos y por inventarse o es un monomedio?, ¿estamos tratando con el
metamedio o el monomedio?
Entendemos ahora que en la cultura del software lo que identificamos como “propiedades”
de los diferentes medios, por una inercia conceptual, son en realidad las propiedades del
software de medios: sus interfaces, sus herramientas, y las técnicas para acceder,
navegar, crear, modificar, publicar y compartir los documentos de medios. Por ejemplo, la
habilidad de alternar entre diferentes vistas de un mismo documento en Acrobat Reader o
196 http://photo.net/equipment/software/aperture-3/review/, marzo 4, 2012.

197
en Microsoft Word no es una propiedad de los “documentos electrónicos” en general, sino
un resultado de las técnicas del software cuya herencia puede rastrearse al “control de
vistas” de Engelbart. Igualmente, la habilidad de cambiar el número de segmentos que
componen una curva no es una propiedad de las “imágenes vectoriales”, es una opción
disponible en algunos (no todos) los software de dibujo vectorial.
Como hemos visto, todo software de medios incluye por lo menos algunas herramientas
que no son específicas a los medios, es decir, no están limitadas a funcionar sobre
estructuras de datos particulares como las imágenes “raster” o los dibujos vectoriales.
Conceptualizados originalmente en Xerox PARC como una manera en que el usuario
transfiriera el hábito cognitivo aprendido en una aplicación a otra, hoy un pequeño número
de “comandos universales” de Xerox se ha vuelto un mayor número de herramientas
“independientes de los medios” y técnicas de interfaz que está modelando la forma en que
los usuarios entienden los tipos de contenidos de medios.
A pesar de estas maneras fundamentales en que distintos medios se alinearon conceptual
y prácticamente, no quisiera abandonar el concepto de diferentes medios en conjunto. Las
diferencias sustanciales entre las operaciones de creación y edición soportadas por
diferentes estructuras de datos son una razón para mantener este concepto. Y aquí hay
otras tres razones:
1. Los “medios”, tal como son implementados en el software, son parte historias
culturales distintas que se remontan a cientos o miles de años. El texto electrónico es
parte de la historia de la escritura; las animaciones digitales en movimiento son parte de la
historia de las imágenes en movimiento, que incluyen los juegos de sombras, la
fantasmagoría, las juguetes ópticos del siglo XIX, el cine, y la animación; una fotografía
digital es parte de casi doscientos años de historia de la fotografía. Estas historias
influencian cómo entendemos y usamos estos medios hoy en día.
Puesto de otra forma, podemos decir que cualquier película de hoy existe frente al
horizonte de todos las películas hechas y al subconjunto de películas que una persona en
particular ha visto en su vida; en la misma línea, cualquier imagen digital existe frente al
horizonte de todas las imágenes de el “museo son muros” (André Malraux) de la historia

198
visual humana. Un medio, entonces, no es sólo un conjunto de materiales y herramientas,
y de técnicas artísticas soportadas por estas herramientas. Un medio es también una base
de datos imaginaria de todas las posibilidades expresivas, composiciones, estados y
dinámicas emocionales, técnicas de representación y comunicación, y “contenido”
actualizado en todos los trabajos creados con un una combinación particular de
materiales y herramientas.
La digitalización sistemática del legado cultural está convirtiendo gradualmente esta
imaginaria base de datos en una base de datos real. Para julio de 2010, Google había
digitalizado 12 millones de libros, mientras que artstor.org ofrecía 1 millón de imágenes de
arte y arquitectura digitalizadas de 228 museos y colecciones privadas. No obstante, este
proceso no empezó con las computadoras digitales. Para inicios del siglo XX, el desarrollo
de museos de arte públicos, revistas y libros de arte ilustrados, clases con filminas de
linterna y el estudio académico de las artes en las universidades ya había hecho visibles y
accesibles al público una gran cantidad de obras (en oposición al pequeño número que
sólo era accesible a cierta clase). Por ejemplo, los museos en EUA empezaron a desarrollar
en diapositivas de linterna sus colecciones adquiridas después de 1865; en 1905, la
Universidad de California en Berkeley ofreció su primer curso en historia de la arquitectura
usando diapositivas de linterna197.
La digitalización de colecciones culturales desde 1990 empezó a reunir los materiales
dispersos de todas estas fuentes, volviéndolos accesibles y localizables mediante sitios
web. Por ejemplo, Europeana198 provee información y ligas de 20 millones de objetos
culturales digitalizados (al 1 de diciembre de 2012199), incluyendo pinturas, dibujos,
mapas, libros, periódicos, diarios, música y discursos de cilindros, cintas, discos y
transmisiones de radio, películas y emisiones de TV contribuidas por 1500 instituciones
europeas. BBC Your Painting ofrece 200 mil pinturas británicas de todas las colecciones
nacionales del Reino Unido. Al momento de escribir estas líneas, este sitio ya tenía 110
197 http://en.wikipedia.org/wiki/Slide_library.
198 http://www.europeana.eu/.
199 http://pro.europeana.eu/web/guest/news/press-releases, marzo 4, 2012.

199
imágenes200. En EUA, la Librería del Congreso brinda acceso a docenas de colecciones
digitales a partir de un solo portal. Estas colecciones incluyen 4.7 millones de páginas de
diarios en alta resolución (desde c. 1860) y más de un millón de imágenes digitales, entre
las que hay 171 mil negativos escaneados del programa Farm Security Administration /
Office of War Information (1935-1945) 201.
Estas colecciones institucionales digitalizadas son complementadas por los contenidos
que los usuarios suben y hacen de manera digital nativa. YouTube, y otros servicios para
compartir videos, contienen muestras sustanciales de todas la historia del cine en forma
de videoclips cortos. Flickr tiene un gran número de fotos de obras tomadas por los
visitantes de museos alrededor del mundo. Los sitios de portafolios de creadores de
medios, como coroflot.com y behance.com, tienen millones de portafolios en dirección de
arte, diseño de exhibiciones, ilustración, diseño de interacción, gráficos animados y demás
campos. Los sitios de manga tienen millones de páginas escaneadas y traducidas por los
usuarios (en marzo 2012, mangapark.com albergaba 5,730,252 paginas de 2,020
series202). Scribd.com tienen millones de documentos de texto (quizá te encuentres este
libro ahí). deviantArt, la comunidad en línea de arte hecho por usuarios, tiene más de 100
millones de piezas203.
En relación con las películas y programas de TV, las compañías comerciales de renta de
videos, como Netflix y LoveFilm, ofrecen cientos de miles de contenidos del siglo XX y XXI a
sus clientes web (Netflix 100 mil, a inicios de 2009204. LoveFilm 67 mil a inicios de
2011205). Y redes de intercambio de par a par, como BitTorrent, tienen prácticamente
todas las películas y series recientes y en cartelera, así como discos musicales y software.
200 http://www.bbc.co.uk/arts/yourpaintings/, marzo 5, 2012.
201 http://www.loc.gov/library/libarch-digital.html, marzo 4, 2012.
202 http://www.mangapark.com/, marzo 4, 2012.
203 http://en.wikipedia.org/wiki/DeviantArt.
204 http://netflix.mediaroom.com/index.php?s=43&item=307, julio 8, 2011.
205 http://en.wikipedia.org/wiki/LoveFilm.

200
En cuanto a la música, Rhapsody.com cuenta un catálogo de 14 millones de canciones y
grabaciones de audio, desde Girls’ Generation (el grupo femenino de pop coreano número
1) hasta Steve Reich (el importante compositor minimalista). Y la iTunes Store de Apple
tiene más de 20 millones (a marzo 2012) 206.
Aunque la distribución de lo que está disponible en estos archivos en línea es muy
desequilibradas respecto de los tipos de medios, periodos históricos, países y demás, sí es
posible decir que los clásicos culturales de hoy (es decir, trabajos de personas
reconocidas: cineastas, directores, diseñadores gráficos, diseñadores de medios,
compositores, pintores, escritores, etc.) están todos disponibles en línea, si no completos
por lo menos parte de ellos. Como resultado, la idea de “medio” en el sentido de todos los
trabajos creativos y posibilidades realizadas hasta ahora usando un conjunto de
tecnologías se ha vuelto real. En contraste a esta base de datos imaginaria, en donde
guardamos obras clave en nuestra mente (que no es el lugar más fiable dada la enorme
cantidad a almacenar), ahora podemos rápidamente consultar el web para buscarlas y
estudiarlas. Además, no estamos limitados a la colección de un museo o biblioteca a la
vez, podemos usar Europeana, Artstor, o algún otro agregador masivo de colecciones, para
navegar meta-colecciones combinadas. Por ejemplo, mientras las Galería Nacional de
Londres tiene 2,300 pinturas, el sitio Your Painting, de la BBC, muestra 200 mil imágenes
de todas las colecciones del Reino Unido.
2. También podemos usar el término “medio” para referirnos a una plataforma de
presentación/interacción. Si tomamos las plataformas iOS y Android (cada una con sus
aparatos móviles, sistemas operativos y apps) como ejemplo, tendremos razón. Pero me
gustaría usar la palabra “plataforma” en un sentido más general. El medio como
plataforma se refiere a una serie de recursos, que permite a los usuarios acceder y
manipular contenido de maneras particulares. Así entendida, un cubo blanco en un galería
de arte moderno es un medio; también un complejo cinematográfico, una revista impresa,
una cadena de televisión, un DVD. (Notemos que así como iOS y Android son ecosistemas
que combinan muchos elementos, otras plataformas de medios funcionan de manera
206 Apple, “iTunes from A to Z,” http://www.apple.com/itunes/features, marzo 5, 2012.

201
similar. Cines, productores, distribuidoras y agencia de publicidad forman la plataforma
cinematográfica, por ejemplo).
Este significado de “medio” está relacionado con el concepto de “medios” del siglo XX, en
los estudios de comunicación influenciados por Teoría de la Información de Shannon: los
canales y herramientas de almacenamiento y transmisión usados para guardar y enviar
información. Sin embargo, si el concepto incluye canales y herramientas de
almacenamiento y transmisión, nuestra “plataforma de presentación” se enfoca más en
las tecnologías de recepción. Estas tecnologías incluyen espacios, arquitectura (por
ejemplo, grandes superficies de medios que se están volviendo parte de los interiores y
exteriores de edificios), sensores, luces y, claro, aparatos y apps para ver, editar y
compartir medios. Las plataformas de presentación también “programan” cierto patrón de
comportamiento: caminamos dentro y tocamos la arquitectura; quedamos sobresaltados
en un película; interactuamos con miembros de la familia mientras vemos TV; movemos
nuestro cuerpo frente a un muro interactivo, etc.
Durante los siglos XIX y XX, las plataformas de presentación estaba íntimamente
relacionadas con tipos particulares de contenido de medios. Un museo de arte mostraba
pinturas y esculturas (y más tarde performances e instalaciones); los periódicos publicaban
textos e imágenes; la TV presentaba programas, noticieros y películas. La adición gradual
de capacidades de visualización y reproducción de las computadoras (y después de
laptops, teléfonos móviles, tabletas y demás reproductores basados en tecnologías
computacionales) rompió esta conexión. La distribución, almacenamiento y presentación
de diferentes tipos de medios no estaba determinada por una tecnología o plataforma en
particular. (Esta separación equivale al proceso de “softwareización” que ya discutimos: las
técnicas y herramientas para la creación y edición de medios se liberaron de su vínculo a
una tecnología física o electrónica).
Hoy podemos acceder a la mayoría de los tipos de medios desde cualquier plataforma
computacional. Podemos ver imágenes, video, documentos de texto y mapas dentro de un
email, en un navegador, en una laptop, PC, tableta, teléfono, televisor o sistema de
entretenimiento de aviones o automóviles. Lo que distingue estos aparatos no son los
tipos de contenido que pueden reproducir, ni las interfaces básicas que ofrecen para ver e

202
interactuar con el contenido, sino más bien la relativa facilidad con la que uno puede
navegar varios medios.
Por ejemplo, mi pantalla LCD Samsung del 2011 viene con un navegador web (aunque la
experiencia de usuario es mejor para ver TV por cable o Netflix que para navegar web,
sobretodo por el control). En otro ejemplo, el relativo tamaño reducido de las pantallas de
teléfonos móviles, sus procesadores menos poderosos y con menos RAM, los hacen
indeseables para el montaje de películas o el diseño CAD (Computer Aided Design). Al
mismo tiempo, su tamaño pequeño les da otra ventajas. En muchos países, es
socialmente aceptado que una persona use el chat o que envíe y reciba mensajes de texto
en su teléfono durante una comida (pero hacerlo en una laptop no sería adecuado). El
tamaño de los móviles los hace perfectos para redes sociales de geo-localización (por
ejemplo Foursquare) y otros servicios: recomendaciones de eventos sociales en una
ciudad, seguir amigos en un mapa, juegos basados en ubicaciones, etc207.
Igual que los teléfonos móviles, muchas plataformas toman ventaja de sus propiedades
sociales para añadir características únicas. Por ejemplo, algunos sistemas electrónicos de
entretenimiento en vuelos de avión, permite a los pasajeros chatear con otros u organizar
a juegos multi-usuarios entre ellos (por el momento solo Virgin America y el sistema RED
de V Australia208).
Así como las plataformas de presentación e interacción de medios, los aparatos
computacionales para consumidores tienen sus diferencias. Actualmente, algunas
plataformas móviles209 no permiten a los usuarios guardar documentos directamente en el
aparato; más bien, los archivos deben ser guardados en sus respectivas aplicaciones.
Algunos dispositivos de medios como los lectores de libros digitales, reproductores video,
audio, carteleras digitales y consolas de juego, comúnmente pueden reproducir pocos
contenidos de medios, a veces incluso uno solo. (Mientras que las compañías de
electrónicos para consumidores estén en plena guerra de “convergencia”, añadiendo
207 http://en.wikipedia.org/wiki/Location-based_service.
208 http://en.wikipedia.org/wiki/In-flight_entertainment.
209 http://en.wikipedia.org/wiki/Mobile_platform.

203
gradualmente la posibilidad de reproducir todo tipo de medio en cualquier dispositivo, esta
tendencia no aplica para todos).
Debido a las diferencias en apariencia física (tamaño, peso, forma), en interfaz física
(táctil, teclado, control remoto, control de voz, sensor de movimiento) y en las capacidad
de reproducción, edición e intercambio de medios, es tentador pensar cada tipo de
aparato como un “medio” diferente. Como plataformas de representación/interacción,
estos dispositivos proveen distintas experiencias de usuario y fomentan distintos tipos de
comportamientos de medios (compartir ubicaciones, trabajar, chatear, etc.). Sin embargo,
también debemos recordar que todos ellos usan las mismas tecnologías (computación,
software y redes) y comparten muchas características fundamentales.
Con esto, quizá sea mejor pensar en las plataformas de presentación/interacción con la
idea de “parecido familiar” articulada en el siglo XIX por varios pensadores y, más reciente,
por Wittgenstein (las cosas pueden “conectarse mediante una serie de parecidos
sobrepuestos en donde ningún rasgo es común a todos” 210). Pero esto no sería exacto. La
teoría de prototipos desarrollado en los 70’s por la psicóloga Eleanor Rosch y otros
investigadores puede ser mejor. Según experimentos psicológicos, Rosch demostró que
para la mente humana algunos miembros de muchas categorías semánticas son mejores
representantes de estas categorías que otros (por ejemplo, una silla es más prototípica de
la categoría “mueblas” que un espejo211).
Si consideramos como prototipo (es decir, el miembro más central de una categoría) de
“metamedio computacional” a la mejor implementación actual de la visión de Kay, junto
con todas sus capacidades de conexión e intercambio, entonces todos los demás aparatos
computacionales se pueden situar a ciertas distancias según en cómo instancian esta
visión (para mí, mi laptop Apple puede ser cumplir con tal prototipo, aunque cualquier
laptop con todas sus funciones también puede serlo). Por supuesto, es posible argumentar
que ninguna de las computadoras o aparatos computacionales actuales realiza
210 http://en.wikipedia.org/wiki/Family_resemblance.
211 Eleanor Rosch, “Cognitive Representation of Semantic Categories.” Journal of Experimental
Psychology: General 104, No.3, (septiembre 1975): pp. 192–233.

204
suficientemente la visión de Kay, debido a que los usuarios casuales no pueden programar
fácilmente o inventar sus propios medios (de forma apropiada, Kay tituló la plática con la
que ganó el premio Turing en 1997 “La revolución computacional aún no ha sucedido” 212).
En esta interpretación, el Dynabook de Kay es el prototipo ideal imaginario, y cada aparato
computacional realizado se sitúa a cierta distancia de él.
3. Otro significado importante del concepto de “medio” está relacionado con los sistemas
sensoriales humanos, que captan y procesan información de formas particulares. Cada
sistema sensorial tiene receptores, vías neuronales y partes específicas del cerebro
responsables del procesamiento. La cultura humana tradicional han reconocido cinco
sentidos: vista, oído, gusto, olfato y tacto. Adicionalmente, los humanos también pueden
sentir la temperatura, el dolor, la posición de las partes del cuerpo, equilibrio y aceleración.
Como la idea de los sentidos ha sido importante para la filosofía occidental, antigua y
moderna, para el pensamiento budista y para otras tradiciones intelectuales, las
discusiones sobre los sentidos en relación con el arte y la estética también han sido
amplias. Un estudio serio al respecto necesitaría su propio libro. Más bien, nos limitaremos
a una breve discusión sobre una investigación reciente en psicología cognitiva que puede
usarse para apoya la idea de los múltiples medios: cómo los diferentes tipos de
información son representados y manipulados en el cerebro.
Una cuestión importante para la psicología cognitiva de los últimos cuarenta años ha sido
si las cognición humana opera en más de un tipo de representación mental. Una hipótesis
es que el cerebro usa una sola representación proposicional para diferentes tipos de
información. Según esta teoría, lo que experimentamos como imágenes mentales es
traducido internamente por la mente en proposiciones parecidas al lenguaje213. La
hipótesis alternativa es que el cerebro procesa y representa imágenes usando un sistema
representacional separado. Tras décadas de estudios psicológicos y neurológicos, el
212 http://blog.moryton.net/2007/12/computer-revolution-hasnt-happened-yet.html, marzo 5,
2012.
213 Ver: http://plato.stanford.edu/entries/mental-representation/.

205
consenso actual apoya ésta segunda teoría214. Así, en el caso de las imágenes mentales,
el cerebro funciona y mantiene imágenes mentales como unos todos parecidos a
imágenes.
Si aceptamos esta idea, que el lenguaje, las imágenes y las formas espaciales requieren el
uso de diferentes procesos y representaciones mentales para su procesamiento:
proposicional (es decir, basada en conceptos, o lingüística) para la primera; visual/espacial
(o pictorial) para las segundas; ayuda a entender porque las especies humanas necesitan
medios de escritura y visuales y especiales. Diferentes medios nos permiten usar nuestros
diferentes procesos mentales. En otra palabras, si los medios son “herramientas para el
pensamiento” (para citar el título del libro de Howard Rheingold de 1984 dedicado a las
computadoras215) mediante los cuáles pensamos y comunicamos los resultados de
nuestro pensamiento con otros, es lógico que usemos las herramientas que nos permiten
pensar verbalmente, visualmente y especialmente.
La teoría de la psicología cognitiva que postula la existencia de representaciones
proposicionales y pictoriales en la mente es sólo un ejemplo de varias teorías que
comparten la creencia que el pensamiento y comprensión humana no están limitadas
exclusivamente al uso del lenguaje. Por ejemplo, en 1983 Howard Gardner propuso una
teoría de inteligencias múltiples, que incluía seis categorías: corporal-cinestésica, verballingüística,
visual-espacial, musical, interpersonal, lógica-matemática, naturalista e
intrapersonal. Recordemos que también Alan Kay basó el diseño de su GUI en el trabajo
del psicólogo Jerome Bruner, quien postulaba la existencia de tres modelos de
representación y cognición: enactivo (basado en la acción), icónico (basado en la imagen) y
simbólico (basado en el lenguaje). A pesar de que el primer modelo fue implementado en
Xerox PARC se limitaba a la selección de objetos en la pantalla mediante el mouse, se le
han unido recientemente interfaces táctiles e interfaces gestuales. Así, las interfaces de
214 Rohrer, T. (2006). The Body in Space: Dimensions of embodiment The Body in Space:
Embodiment, Experientialism and Linguistic Conceptualization]. In Body, Language and Mind, vol.
2. Zlatev, Jordan; Ziemke, Tom; Frank, Roz; Dirven, René (eds.). Berlin: Mouton de Gruyter, 2006.
215 Howard Rheingold, Tools for Thought: The History and Future of Mind-Expanding Technology,
edición revisadas del libro original publicado en 1985 (The MIT Press, 2000).

206
las computadoras modernas y de los aparatos basados en la computación son ellos mismo
ejemplos de múltiples medios trabajando: añadiéndose cada vez más medios como
mecanismos de interacción, en lugar de que hagan convergencia en un solo, como el
lenguaje escrito (el UNIX original y otros sistemas operativos de los 60-70’s) o la palabra
(el personaje Hal de la película 2001 Odisea en el espacio de Kubrick).
La evolución de las especies de medios
Como vemos, mientras hay razones sustanciales para creer que la softwareización
redefine lo que son los medios y cómo interactúan, no elimina la idea de múltiples y
distintos medios. De hecho, en contraste con la idea de “convergencia”, que se hizo
popular en los 2000’s para entender la reunión de computadoras, televisión y teléfono, mi
propuesta es que entendamos los medios computacionales con el concepto de la
evolución biológica (que implica, con el tiempo, el incremento de la diversidad). Y es aquí
donde encontramos la dificultad máxima de seguir usando el término “medio” como un
descriptor útil del conjunto de actividades culturales y artísticas. El problema no es que
múltiples medios converjan en un “monomedio” (no lo hacen). El problema es
exactamente el opuesto: se multiplican a tal punto que el término pierde su utilidad. La
mayoría de los grandes museos y escuelas de arte tienen entre cuatro y seis
departamentos que supuestamente corresponden a diferentes medios (por ejemplo,
SFMOMA divide su colección en pintura y escultura, fotografía, arquitectura y diseño, y
artes de los medios216)… y está bien. Siempre podemos seguir usando nombres únicos
para diferentes medios aún si aumentamos el número a un par de decenas. ¿Pero qué
pasa cuando el número llega a miles o a decenas de miles?
Y esta es exactamente la situación en la que estamos hoy debido a la softwareización.
Haciendo una extrapolación de la definición de diccionario de medio que abre la primera
parte, podemos decir que diferentes medios tienen capacidades de representación,
expresión, interacción y/o comunicación suficientemente disimiles. (¿Cuánta diferencia se
necesita para decir que tenemos dos medios en lugar de uno? Ésta es otra pregunta
216 http://www.sfmoma.org/explore/collection, marzo 6, 2012.

207
fundamental que hace difícil el uso de “medio” en la cultura del software). Consideremos
los factores involucrados en la expansión gradual y sistemática de estas capacidades: la
“extensibilidad permanente” del software, el desarrollo de nuevos tipos de aparatos
basados en computación y redes, los procesos de hibridación de medios manifiestos en
aplicaciones de software, prototipos tecnológicos, comerciales y proyectos artísticos…
¿Tenemos un nuevo medio cada vez que se añade una nueva funcionalidad de
representación, expresión, interacción o comunicación? ¿o cada vez que se combinan? Por
ejemplo, ¿la adición de interfaces de control de voz a un móvil crea un nuevo medio? ¿qué
podemos decir de la combinación creativa de diferentes técnicas visuales en un video
musical? ¿se define un nuevo medio? ¿o qué pasa con las versiones de Google Earth que,
en una computadora soportan diferentes tipos de capas pero en iPhone OS versión 2008
sólo soportan Wikipedia y Panoramio? ¿son dos medios diferentes?
Si seguimos aferrados a nuestra definición clásica de medio (sin hacer la extrapolación
que aquí proponemos), deberíamos responder afirmativamente a todas éstas preguntas.
Ciertamente no queremos esto. Lo que significa que el fundamento conceptual del
discurso de medios y de los estudios de medios (la idea que podemos nombrar un número
relativamente pequeño de medios distintos) ya no se sostiene. Necesitamos algo más en
su lugar.
Para explicar esto de forma alternativa: cuando consideramos un solo aspecto del
ecosistema de medios (el diseño de medios), pudimos adoptar el entendimiento
tradicional de “medio” (materiales + herramientas) para describir las operaciones e
interfaces del software de aplicaciones. Esto fue posible mediante la propuesta de una
nueva definición: un medio = algoritmos + una estructura de datos. Siguiendo esta
perspectiva, podemos referirnos al texto simple, al texto formateado, a los gráficos
vectoriales, a las imágenes bitmap, a los modelos poligonales 3D, a los modelos de curvas,
a los modelos de voxels, a los archivos de audio WAV, o a los MIDI, como medios
separados. Pero incluso con este enfoque ya era difícil abarcar el diseño de aplicaciones
interactivas y sitios web. Cuando empezamos a considerar el ecosistema más amplio de
los dispositivos móviles, servicios de red, tecnologías de interfaces, proyectos de medios y
más de un millón de apps para plataformas móviles disponibles a los consumidores, el
concepto ya no puede seguir siendo estirado para describirlos de forma significativa.

208
Como lo sugiero en este capítulo, el modelo evolutivo de múltiples especies relacionadas
que tomamos prestado a la biología ofrece una alternativa plausible. Este modelo es útil
para pensar tanto las aplicaciones de creación/edición de medios como
proyectos/productos de medios particulares (que, al final, son también aplicaciones de
software pero más especializadas: si las aplicaciones de medios son agnósticas del
contenido, los proyectos ofrecen generalmente contenidos particulares). Las ventajas clave
del “modelo de especies” sobre el “modelo de medios” es su gran número (la Tierra tiene
varios millones de especies, por lo menos hasta ahora), sus vínculos genéticos (que
implican superposición significativa de propiedades entre especies) y el concepto de
evolución (que implica desarrollo constante en el tiempo y el incremento gradual de la
diversidad).
Cada una de estas ventajas es igualmente importante. En lugar de intentar dividir la
compleja diversidad de productos de medios de la cultura del software en un reducido
número de categorías (o sea, “medios”), podemos pensar en cada combinación distinta de
un subconjunto de todas las técnicas disponibles como una única “especie de medios”.
Claro está, un software de aplicación o un proyecto/producto no está limitado a
recombinar técnicas existentes; también puede innovar nuevas, que a su vez pueden
reaparecer en otros productos o aplicaciones. Estas nuevas innovaciones pueden ser
vistas como nuevos genes (teniendo en cuenta todas nuestras limitantes del uso de ésta
metáfora). Si las ciencias contemporáneas como la biología, la genética y la neurociencias
pueden describir (o por lo menos encaminarse a describir) millones de especies distintas,
3 billones de pares de DNA en el genoma humano, y 100 bullones de neuronas en la
corteza cerebral, ¿por qué no pueden, la teoría de medios y los estudios del software, lidiar
con la diversidad y variabilidad de la cultura del software mediante la propuesta de
mejores clasificaciones que las que tenemos actualmente? (Un par de ejemplos
inspiradores desde las ciencias de la vida son: el proyecto Human Connectome, que tiene
la intención de crear un mapa del cerebro adulto217 o, más ambicioso aún, el proyecto Blue
217 http://www.humanconnectomeproject.org/about/ marzo 7, 2012.

209
Brain, que intenta crear un software realista de simulación del cerebro a su nivel
molecular218).
Otra noción igual de importante es la que dice que las especies relacionada comparten
varias propiedades. La tradición estética de los siglos XVIII-XX, de Gotthold Lessing a
Clement Greenberg, fue insistente en oponer un pequeño número de medios entre ellos,
viéndolos como distintos y como categorías que no se sobreponen. Esta tendencia se
culminó en el modernismo europeo de los años 10-20’s, cuando los artistas trataron de
reducir cada medio a su cualidades únicas. Para ello, renunciaron a la representación y se
concentraron en los elementos materiales que pensaron eran única a cada medio. Había
poetas, como los futuristas rusos, que experimentaban con sonidos; los cineastas
proponían que la esencia del cine era el movimiento y el ritmo temporal (la teoría del cine
francés de los años 20219) o el montaje (como el afirmaba el grupo de Kuleshov en Rusia);
y los pintores exploraban colores puros y formas geométricas. La siguiente declaración,
hecha en 1924 por Jean Epstein, un cineasta vanguardista y teórico francés, es típica de la
retórica modernista de la pureza del medio; muchas más sentencias como ésta
aparecieron en páginas de publicaciones vanguardistas de la época:
Cada arte construye su ciudad prohibida, su propio dominio exclusivo, autónomo,
específico y hostil a cualquier cosa que pertenezca. Por asombroso que sea, la
literatura debe ser primero literaria; el teatro, teatral; la pintura, pictórica; y el cine,
cinemático. La pintura se está liberando hoy de muchas de sus preocupaciones
representacionales y narrativas… Y cualquier literatura digna de su nombre da la
espalda a los giros y vueltas de la trama que llevan al detective a descubrir el
tesoro secreto… El cine debe buscar convertirse, gradualmente y al final
únicamente, en cinemático; usar, en otras palabras, sólo elementos
fotogénicos220.
218 http://bluebrain.epfl.ch/, marzo 7, 2012.
219 See Jean Epstein, "On Certain Characteristics of Photogénie," in French Film Theory and Criticism,
ed. Richard Abel (Princeton: University of Princeton Press, 1988), 1: 314-318; Germaine Dulac,
"Aesthetics, Obstacles, Integral Cinégraphie," in French Film Theory and Criticism, 1: 389-397.
220 Ibid., 314-15.

210
A pesar de que los artistas se revolvieron contra esta tendencia en la segunda parte del
siglo XX, más bien se voltearon hacia los “medios mixtos”, el “ensamble”, el “arte
multimedia“ y las “instalaciones”. Esto no condujo a un mejor sistema para entender el
arte, sólo creó nuevas categorías (las que acabamos de mencionar) y los objetos del nuevo
arte, extremadamente variados, fueron ubicados en estas categorías.
La biología evolutiva nos da un espacio de objetos mucho más amplio, cuyas propiedades
se pueden sobreponer. Este modelo le queda mucho mejor a mi teoría de la cultura de
software: vista como una larga y creciente agrupación de técnicas que se pueden combinar
y que dan forma a aplicaciones o proyectos/productos creados con ellas, o mediante
programación a la medida. Obviamente, la mayoría de estas “especies de medios”
comparte por lo menos algunas técnicas. Por ejemplo, todas las aplicaciones con interfaz
GUI, diseñadas para correr en pantalla completa de la computadora, usan técnicas de
interacción similares. Para retomar otro conjunto de especies de medios, ya hemos
analizado algunos ejemplos de hibridación de medios, Alsace de Fujihata, Invisible Shape
de Sauter, y Google Earth. Todos ellos comparten la técnicas de incrustación de videos en
su espacio virtual navegable. Tanto Fujihata como Sauter construyen interfaces únicas de
objetos de video en sus mundos 3D. En contraste, Google Earth “habita” la interfaz de
YouTube cuando incluimos un video en la descripción de algún lugar (aunque existe cierto
control sobre las características del lector221). Esta habilidad de introducir un lector de
video YouTube es otro ejemplo de técnica que hoy es bastante usada en sitios web y blogs,
así como lo son las API’s disponibles por los servicios de medios sociales.
Finalmente, otro aspecto atractivo del “modelo de especies” es la idea de evolución (no
hablamos de los mecanismos de la evolución biológica en la Tierra, teorizados y discutidos
por los científicos, ya que estos mecanismos no soportan la evolución tecnológica y
cultural, más bien hablamos de la imagen de desarrollo temporal, gradual y continuo, y de
variabilidad y emergencia de nuevas especies), sin implicar progreso. Sin alinearnos con la
teoría de los memes, podemos nombrar varias formas en que nuevas técnicas se
221 YouTube, “Embedded player style guide,”
http://support.google.com/youtube/bin/answer.py?hl=en&answer=178264, marzo 7, 2012.

211
transmiten en la cultura del software: nuevos proyectos y productos que son vistos por
demás diseñadores y programadores; artículos científicos en ciencias computacionales,
ciencias de la información, HCI, computación de medios, visualización y demás áreas; y, el
código de programación en sí. Este último mecanismo nos es particularmente importante
debido a que es único a la cultura del software, en contraste con los medios precedentes
(al mismo tiempo, tiene su equivalente directo en el código genético). A medida que
nuevas técnicas para la creación, edición, análisis, interacción, transmisión, búsqueda,
visualización, etc. de medios se vuelven populares, se van programando en varios
lenguajes y ambientes de script (Java, C++, JavaScript, Python, MatLab, Processing, etc.) y
se quedan disponibles, comercial o libremente. Si un programador que trabaja en una
nueva aplicación, sitio web o cualquier proyecto de medio quiere usar alguna de estas
técnicas, puede hacer uso de una librería e implementar las funciones en su propio código.
Tradicionalmente, las nuevas técnicas culturales se reproducían por imitación o
adiestramiento. Los artistas profesionales o los artesanos veían algo nuevo y lo imitaban;
los pupilos, asistentes y estudiantes aprendían las técnicas de sus maestros; los
aprendices de arte pasaban años copiando obras famosas. En cualquier caso, las técnicas
pasaban de mente a mente, de mano a mano. Las tecnologías de medios modernas
añadieron un nuevo mecanismo para la transmisión cultural: las interfaces y los manuales
de los dispositivos de los medios traen consigo las mejores prácticas sugeridas para
usarlos. A estos mecanismos se les ha unido uno más: la transmisión de técnicas
culturales mediante algoritmos y librerías de software222. Esto no quiere decir
necesariamente que la transmisión no introduzca cambios: los programadores siempre
pueden modificar el código existente según las necesidades de su proyecto. Más
importante, creo, es la modularidad fundamental de los objetos culturales creados con
este mecanismo. Al nivel de las técnicas, un objeto cultural se vuelve una aglomeración de
funciones de diferentes librerías de software (el DNA que comparte con muchos otros
objetos que usan las mismas técnicas). (A pesar de que los productos de medios
222 Jeremy Douglass sugería que podemos estudiar la propagación de técnicas a través la cultura
del software mediante el rastreo del uso de particulares librerías de software y sus funciones en
diferentes programas Jeremy Douglass, presentación en el taller Softwhere 2008, University of
California, San Diego, mayo 2008.

212
comerciales usan ampliamente elementos comprados en catálogos de stock para su
contenido, ésta modularidad no está reconocida abiertamente). Debido a que las técnicas
están codificados como funciones de software, esta modularidad cultural está
íntimamente ligada al principio de modularidad de la programación computacional
moderna (un programa debe consistir de un número de partes independientes). Si un
proyecto de medios o una app introduce una nueva técnica (o técnicas) y resulta que son
valiosas, comúnmente el autor del proyecto las hace disponibles como funciones
separadas, que pueden embonar en las demás técnicas del metamedio computacional
(facilitando su adopción por terceras personas para crear nuevos proyectos).
Me gustaría terminar este capítulo con una cita del historiados Louis Menand, quien
explica que, antes de Darwin, los científicos veían las especies como tipos ideales. Darwin
cambió el enfoque hacia la variación, que para mí es la razón primordial para pensar los
medios en la era del software a través de los términos de la biología evolutiva:
“Una vez que nuestra atención apunta hacia el individuo, necesitamos otra forma de hacer
generalizaciones. Ya no estamos interesados en la conformidad de un individuo a un tipo
ideal; ahora nos interesa la relación de un individuo con otros individuos con los que
interactúa… Las relaciones se vuelven más importantes que las categorías; las funciones,
que son variables, son más importantes que los propósitos; las transiciones son más
importantes que las fronteras; las secuencias son más importantes que las jerarquías” 223.
223 Louis Menand. The Metaphysical Club (New York: Farrar, Straus and Giroux, (2001), p. 123.

213
TERCERA PARTE: El software en acción
Capítulo 5. Diseño de medios
“Primero moldeamos nuestras herramientas, después ellas nos moldean” Marshall
McLuhan, Understanding Media (1964).
After Effects y la revolución invisible
Los híbridos de medios no se limitan a tipos particulares de aplicaciones software,
interfaces de usuario, proyectos artísticos o el web. Si estamos en lo correcto al sostener
que la hibridación representa la siguiente etapa lógica en el desarrollo de los medios
computacionales (después de la primera etapa que fue la simulación de medios físicos en
la computadora), entonces debemos encontrarla en varias áreas de las producción de
medios de nuestros días. Y así es el caso. En esta capítulo describiremos cómo la
producción y estética de la las imágenes en movimiento cambió dramáticamente en los
90’s.
A mediados de ésta década, los medios físicos simulados para la producción de imágenes
fijas y animadas (cinematografía, animación, diseño gráfico, tipografía), el nuevo medio
computaciones (animación 3D) y las nuevas técnicas computacionales (composición,
múltiples niveles de transparencia) se encontraron en un mismo ambiente de software (es
decir, un número de programas de software, compatibles y que corrían en la misma
estación de trabajo computacional). Cineastas, animadores y diseñadores empezaron a
trabajar sistemáticamente en este ambiente, usando software para generar elementos
individuales y para ensamblarlos. El resultado fue el surgimiento de una nueva estética
híbrida que rápidamente se volvió la norma.

214
Hoy, este lenguaje domina la cultura visual de decenas de países. Lo vemos a diario en
comerciales, video musicales, gráficos animados (motion graphics), gráficos de TV, diseño
de créditos, interfaces interactivas de teléfono móviles y otros aparatos, menús dinámicos,
páginas web animadas, gráficos para móviles y en todo tipo de narrativas, interfaces y
contenidos que son producidos por profesionales (tanto compañías como diseñadores y
artistas independientes) y por estudiantes. Con todo, estimamos que por lo menos un 50
por ciento de las imágenes en movimiento de corta duración siguen este lenguaje. En este
capítulo analizaremos lo que percibo como sus características esenciales: nueva estética
de híbridos visuales; integración sistemática de técnicas de medios previamente
incompatibles; uso del espacio 3D como plataforma común de diseño de medios; cambio
constante en cada dimensión visual; y amplificación de técnicas cinematográficas.
La nueva estética de los híbridos existe en innumerables variaciones pero el principio
básico es el mismo: sobreponer diferentes lenguajes visuales de diferentes medios en una
misma imagen. Este es un ejemplo de cómo la lógica de la remezcla de medios
reestructura una gran parte de la cultura como todo. Los lenguajes del diseño, tipografía,
animación, pintura y cinematografía se reúnen en la computadora. Por lo tanto, además de
ser un metamedio, como lo formuló Kay, también podemos llamar a la computadora una
plataforma de metalenguaje: el lugar en donde muchos lenguajes culturales del periodo
moderno vienen y crean nuevos híbridos.
¿Cómo se dio este lenguaje? Considero que estudiar el software involucrado en la
producción de imágenes en movimiento es determinante para explicar porqué se ven
como se ven. Sin este análisis no podremos salir de las generalidades de la cultura
contemporánea (post-moderno, global, remix, etc.) para describir los lenguajes particulares
de las diferentes áreas del diseño; para entender las causas detrás de ellos y su evolución
en el tiempo. (En otras palabras, creo que la “teoría del software”, que este libro trata de
definir y poner en práctica, no es un lujo sino una necesidad).
Aunque las transformaciones que discutiremos incluyen muchos desarrollos tecnológicos y
sociales (hardware, software, prácticas de producción, flujos de trabajo, nuevos puestos y
campos profesionales), es apropiado resaltar un software en particular que ha estado al
centro de estos eventos. Esta aplicación es After Effects. En este capítulo haremos un

215
acercamiento a su interfaz, sus herramientas y su uso típico en el diseño de medios.
Introducido en 1993, After Effects fue el primer software diseñado para hacer animación,
composición y efectos especiales en la computadora personal224. Su efecto general en la
producción de imágenes en movimiento puede ser comparado con el efecto de Photoshop
e Illustrator en la fotografía, la ilustración y el diseño gráfico.
Claro que After Effects tiene su competencia. En los 90’s, la compañías también usaban
fuertemente otros software “de alta calidad” como Flame, Inferno o Paintbox que corrían
en estaciones gráficas de trabajo especializadas… y se siguen usando. En los 2000’s,
otros programas en la misma categoría de precios que After Effects, como Motion de
Apple, Combustion de Autodesk y Flash de Adobe, también pusieron en riesgo el dominio
de After Effects. Pero debido a su asequibilidad y su tiempo en el mercado, After Effects
sigue siendo el más popular, el más usado y el mejor conocido. Por esto, After Effects
tendrá un rol preferente en mi estudio, tanto como símbolo como material preponderante
que hizo posible la transformación de la cultura de la imagen en movimiento a gran escala.
(Cuando buscamos “mejores software de motion graphics” en el web y verificamos la
respuesta en diversos foros, el primer programa que se menciona siempre es After Effects.
Una de las personas de estos foros dijo: “Es el estándar por default. Apréndelo. Ámalo”225.
Otro ejemplo: cuando en 2012 Imaginary Forces, la compañía más relacionada con el
nacimiento de los gráficos animados en los 90’s, publicó descripciones de puestos en sus
oficinas de Los Ángeles y NYC para diseñadores y animadores, sólo enumeró un software
para la producción de imágenes animadas 2D: After Effects226).
224 El Video Toaster de NewTek, lanzado en 1990, fue el primer sistema de producción de video
para la computadora que incluía un switcher de video, generación de caracteres, manipulación de
imágenes y animación. Debido a su bajo precio, los sistemas Video Toaster fueron muy populares en
los 90’s. En el contexto de este libro, After Effects es más importante porque, como veremos,
introdujo un nuevo paradigma de diseño de imágenes en movimiento que era diferente al paradigma
familiar de edición de video soportado por sistemas como el Toaster o Avid.
225 John Waskey, http://www.quora.com/What-is-the-best-software-for-creatingmotion-graphics
(marzo 4, 2001).
226 http://www.imaginaryforces.com/jobs/los-angeles/designer/,
http://www.imaginaryforces.com/jobs/new-york/2d-animator/ (octubre 31, 2012).

216
Como veremos, la interfaz de usuario y las herramientas de After Effects reúnen técnicas
fundamentales, métodos de trabajo y supuesto de áreas previamente separadas del cine,
la animación y el diseño gráfico. Este ambiente de producción híbrido, encapsulado en un
mismo software, tiene una influencia directa en el lenguaje visual que habilita,
específicamente su enfoque en la exploración de posibilidades estéticas, narrativas y
afectivas de la hibridación.
El giro hacia las herramientas basadas en software en los 90’s no solo afectó a la cultura
de la imagen en movimiento sino todas las demás áreas del diseño. Todas adoptaron el
mismo flujo de trabajo de producción. (Cuando el proyecto es grande e involucra mucha
gente y muchos elementos de medios, el flujo de trabajo de la producción se llama
“tubería” o “pipeline”). En este proceso de producción los usuarios generalmente
combinan elementos creados en diferentes software, o mueven sus proyectos completos
de una aplicación a otra con la intención de tomar ventaja de sus funciones particulares.
Aunque cada área del diseño tiene sus aplicaciones especializadas (por ejemplo, los
diseñadores web usan Dreamweaver, mientras que los arquitectos usan Revit y los artistas
de efectos visual usan Nuke y Fusion), también usan varias aplicaciones comunes. Éstas
son Photoshop, Illustrator, Final Cut, After Effects, Maya, 3ds Max y otras más. (Si usas
software libre como Gimp o Cinepaint en lugar de las aplicaciones comerciales, tu lista de
aplicaciones clave puede variar pero los principios siguen siendo los mismos).
La adopción de este ambiente de producción, que consiste en varias aplicaciones
compatibles de diferentes áreas de las industrias creativas, tiene muchos efectos
fundamentales. Las fronteras profesionales entre las áreas del diseño se vuelven menos
importantes. Un diseñador o un pequeño estudio pueden trabajar hoy en un video musical,
en el diseño de un producto mañana y en un proyecto de arquitectura o sitio web pasado
mañana, etc. Otra distinción que antes era fundamental (la escala del proyecto) también
importa cada vez menos. Hoy podemos encontrar exactamente las mismas formas y
figuras en proyectos de pequeña escala (como las joyas), en medianos (diseño de

217
muebles, sillas, mesas) y de gran escala (edificios, diseños urbanos). Como ejemplo,
podemos referir a los proyectos de Zaha Hadid, que cubren precisamente este abanico227.
Un estudio detallado de estos y otros efectos de la adopción del software tomaría más de
un libro. Por eso, en este capítulo sólo me enfocaré en el impacto del flujo de trabajo,
basado en software, en el diseño de medio contemporáneos. Como veremos, este flujo
modela el diseño contemporáneo de varias maneras. Por un lado, nunca antes en la
historia de la comunicación visual humana habíamos visto formas tan variadas como hoy.
Por otro lado, la mismas técnicas, composiciones e iconografía pueden aparecer en
cualquier medio. Para evocar la metáfora de la evolución biológica otra vez, podemos decir
que a pesar de la infinita diversidad de “especies” de medios contemporáneos, visuales y
espaciales, todo comparten un DNA en común. Muchas de las especies también
comparten un principio de diseño básico: la integración de técnicas del diseño de medios
previamente incompatibles (un proceso que llamaré “remezcla profunda”). Por esto, al
darle una oportunidad al software de producción de medios, y sus usos en la producción,
nos permite empezar a construir un mapa de nuestro universo de medios y diseños
actuales, viendo cómo sus especies se relacionan entre ellas y revelan los mecanismos
subyacentes a su evolución.
La adopción de After Effects y software similar en la segunda parte de los 90’s
rápidamente condujo a la adopción de un término especial para designar los nuevos
visuales animados: “motion graphics” (gráficos animados). Definidos de manera concisa
por Matt Fritz en su tesis de maestría en 2003 como “visuales diseñados sin narrativa, no
figurativos, que cambian con el tiempo”228, los gráficos animados incluyen los créditos de
películas y TV, gráficos de TV, menús dinámicos, contenidos para medios móviles y demás
secuencias animadas. Típicamente, los gráficos animados aparecen como partes de
piezas más grandes: comerciales, video musicales, videos de capacitación, películas y
documentales, proyectos interactivos. O, por lo menos, así era en 1993. Desde ese tiempo,
la frontera entre los gráficos animados y todo lo demás se ha vuelto difícil de distinguir. Por
227 http://www.zaha-hadid.com/
228 Matt Frantz “Changing Over Time: The Future of Motion Graphics,” Tesis de maestría en artes,
2003, http://www.mattfrantz.com/thesisandresearch/motiongraphics.html

218
lo tanto, en 2008 en la versión del artículo sobre motion graphics en Wikipedia en inglés,
sus autores decían que “el término ‘motion graphics’ tiene el potencial de ser menos
ambiguo que el término cine para describir las imágenes en movimiento del siglo XXI”229.
Ciertamente, hoy numerosas imágenes en movimiento cortas combinan metraje,
animación 2D, animación 3D y demás técnicas similares (en oposición a las que privilegian
imágenes de acción en vivo, como lo hacen muchas películas), así que también se les
puede llamar “gráficos animados”.
¿Por qué elegí los gráficos animados como el caso de estudio central de este libro y no
cualquier otra área de la cultura contemporánea, igualmente afectada por un cambio o un
nacimiento en la producción basada en software? Algunos ejemplos de áreas que han
cambiado, también llamadas “orientadas hacia lo digital”, son la arquitectura, el diseño
gráfico, el diseño de producción, el diseño de información y la música. Ejemplo de las que
nacieron en la computadora son el diseño de juegos, diseño de interacción, diseño de
experiencia del usuario, diseño de interfaz de usuario, diseño web y visualización
interactiva de la información. Obviamente, la mayoría de las nuevas áreas que contienen
en su nombre las palabras “interacción” o “información” (y que surgieron a partir de la
década de los 90’s, también han sido ignoradas por la crítica cultural) solicitan nuestra
atención.
Mi razón tiene que ver con la diversidad de nuevas formas (visuales, espaciales y
temporales) que se desarrollaron durante el rápido crecimiento del área de los gráficos
animados después de la introducción de After Effects. Si nos acercamos a los gráficos
animados en los términos de estas formas y técnicas (en lugar de su contenido
únicamente), nos daremos cuenta que representan un giro significante en la historia de la
comunicación humana. Prácticamente todas las técnicas de comunicación desarrolladas
por los humanos hasta los 90’s se combinan ahora comúnmente en proyectos de gráficos
animados: mapas, pictogramas, jeroglíficos, ideogramas, escrituras, alfabeto, gráficos,
sistemas de proyección, gráficas de información, fotografías, lenguajes modernos de
formas abstractas (desarrollado en la pintura europea de 1910 y adoptados
subsecuentemente por el diseño gráfico y la arquitectura de los 20’s), técnicas
229 http://en.wikipedia.org/wiki/Motion_graphics

219
cinematográficas del siglo XX, gráficos 3D por computadora y, desde luego, toda la
variedad de efectos visuales “nacidos digitales”. Por lo tanto, casi todos los sistemas
semióticos previamente separados se vuelven opciones en la paleta del usuario (o “caja de
herramientas”, para usar la metáfora estándar utilizada por el software de medios). Las
inteligencias lingüísticas, cinésicas, espaciales, icónicas, diagramáticas y temporales
pueden funcionar juntas para expresar lo que ya sabíamos pero que no podíamos
comunicar (así como generar nuevos mensajes y experiencias cuyos significados aún están
por descubrirse).
Aunque todavía necesitamos descifrar cómo usar plenamente este nuevo metalenguaje
semiótico, la importancia de sus surgimiento es difícil de sobreestimar. O sea, la
emergencia de gráficos animados hechos con software es tan importante como la
invención de la imprenta, la fotografía o Internet.
Empezaremos por remontarnos a los 80’s. En el apogeo de los debates de la
postmodernidad, por lo menos un crítico estadounidense se dio cuenta de la conexión
entre el pastiche postmoderno y la informatización. En su libro After the great divide
(1986), Andreas Huyssen escribía: “todas las técnicas, formas e imágenes modernas y
vanguardistas están guardadas para acceso directo en los bancos de memoria
informatizados de nuestra cultura. Pero la misma memoria también almacena todo el arte
pre-moderno así como los géneros, códigos y los mundos de imágenes de las culturas
populares y de la cultura de masas moderna”230. Su análisis es preciso, excepto que estos
“bancos de memoria informatizados” no fueron disponibles hasta 15 años más tarde. Sólo
cuando el web absorbió suficientes archivos de medios fue cuando se volvió un banco de
memoria cultural universal accesible por todos los productores de cultura. Incluso para los
profesionales, la habilidad de integrar fácilmente múltiples medios en un mismo proyecto
(capas de video, imágenes fijas escaneadas, animación, gráficos y tipografía) se logró
hasta finales de los 90’s.
230 Andreas Huyssen, “Mapping the Postmodern,” in After the Great Divide (Bloomington and
Indianapolis: Indiana University Press, 1986), p. 196.

220
En 1985, cuando el libro de Huyssen estaba en preparación para su publicación, yo trabaja
en una de las pocas compañías de animación en el mundo. La compañía estaba ubicada
en la ciudad de Nueva York y se llama apropiadamente Digital Effects231. Cada animador
digital tenía su propia terminal de gráficos interactivos que mostraba los modelos 3D en
modo “wireframe” y monocromáticos. Para verlos a color, debíamos turnarnos porque la
compañía sólo tenía una pantalla de colores bitmap que todos compartíamos. Los datos
estaban guardados en cintas magnéticas que medían como 30 centímetros de diámetro.
Encontrar datos de un trabajo anterior era proceso incómodo que implicaba: ubicar la cinta
correcta en el armario correcto, ponerla en una unidad de lectura y buscar la parte
adecuada. No teníamos un escáner a color, así que lograr las “todas las técnicas, formas e
imágenes modernas y vanguardistas” en una computadora no era cosa sencilla. Pero aún
si hubiéramos tenido uno, no había forma de guardar, recordar y modificar esas imágenes.
La máquina que podía hacer eso era una Quantel Paintbox, que costaba más de 160 mil
dólares (fuera de nuestro presupuesto). Y cuando Quantel lanzó el modelo Harry en 1986,
el primer sistema de edición no lineal que permitía hacer composiciones de múltiples
capas de video y efectos especiales, su precio también estaba fuera del alcance, excepto
para televisoras y algunas cuantas casas productoras. Las capacidades de Harry eran algo
limitadas debido a que sólo podías grabar 80 segundos de video calidad broadcast. En el
campo de las imágenes fijas, las cosas eran mucho mejores. Por ejemplo, la unidad de
almacenamiento Picturebox, lanzada por Quantel en 1990, podía almacenar 500
imágenes de calidad broadcast pero su costo era también muy alto.
En breve, a mediados de los 80’s ni nosotros ni otras compañías de producción teníamos
nada aproximado a los “bancos de memoria informatizados” imaginados por Huyssen. Y
por supuesto que pasaba lo mismo para los artistas visuales asociados con el
postmodernismo y las ideas del pastiche, collage y apropiación. En 1986, la BBC produjo el
documental Painting with light para el cuál una media docena de pintores famosos,
incluyendo Richard Hamilton y David Hockney, fueron invitados para trabajar con una
Quantel Paintbox. Las imágenes resultantes no eran muy diferentes de las que estos
231 Wayne Carlson, A Critical History of Computer Graphics and Animations. Section 2: The
Emergence of Computer Graphics Technology,
http://accad.osu.edu/%7Ewaynec/history/lesson2.html

221
artistas hacían sin la computadora. Y a pesar de que algunos artistas hacían referencia a
“técnicas, formas e imágenes modernas y vanguardistas”, éstas referencias estaba
pintadas en lugar de ser cargadas directamente desde un “banco de memoria
informatizado”. Sólo diez años después, cuando estaciones de trabajo relativamente
baratas que corrían software de edición de imágenes, animación, composición e
ilustración se volvieron comunes y accesibles a diseñadores gráficos independientes,
ilustradores y pequeños estudios de animación y post-producción, la situación imaginada
por Huyssen empezó a volverse una realidad.
Los resultados fueron dramáticos. En el espacio de menos de cinco años, las cultura visual
moderna se transformó radicalmente. Visuales que antes era específicos a diferentes
medios (imágenes acción viva, gráficos, fotografía, animación tradicional, animación 3D
por computadora y tipografía) se empezaron a combinar de diferentes formas. Para finales
de los 90’s, la imagen en movimiento “pura” se volvió una excepción y los medios híbridos
la norma. Sin embargo, en contraste con otras revoluciones computacionales, como el
surgimiento del World Wode Web alrededor de los mismos años, éstas revolución no fue
reconocida por los medios populares ni por los críticos culturales. Lo que recibió atención
fueron los desarrollo que afectaban la narrativa cinematográfica: el uso de efectos
especiales generados por computadora en las películas de Hollywood o las herramientas
de bajo costo para la edición de video digital fuera de él. Pero otro proceso que sucedió a
gran escala, el de la transformación del lenguaje visual usado por todas las formas de
imágenes en movimiento fuera de la narrativa del film, no ha sido analizado críticamente.
De hecho, a pesar de que los resultados de esta transformación se han vuelto visibles
desde 1999, al momento de escribir este libro no conozco un solo artículo que los estudie
(por lo menos no en inglés).
Una de las razones es que, en esta revolución, ningún nuevo medio per se fue creado.
Justo como lo hicieron diez años antes, los diseñadores hacían imágenes fijas e imágenes
en movimiento. Pero la estética de estas imágenes era ahora muy diferente. De hecho, era
tan nueva que, en retrospectiva, la imaginería postmoderna de una década pasada, que
en aquellos tiempos se veía sorprendentemente diferente, ahora aparece como un punto
apenas perceptible en el radar de la historia cultural.

222
No es accidental mi elección de fechas de inicio y fin (1993-1999) para caracterizar el
desarrollo del nuevo lenguaje visual híbrido de las imágenes en movimiento. Claro que
pude haber tomado diferentes fechas, por ejemplo unos cuantos años antes. Pero como
After Effects jugará el rol principal en mi estudio, y fue lanzado en 1993, he decidido tomar
este año como inicio. Igual para mi fecha de fin, creo que para 1999 los mayores cambios
en la estética de las imágenes en movimiento se hicieron visibles. Si quieres hacer la
comprobación tú mismo, simplemente compara los demo reels que las compañías de
efectos visuales hicieron en los 90’s (muchos están disponibles en línea, por ejemplo
busca los de Pacific Data Ocean, o los del sistema Flame, disponibles para cada año a
desde 1995232). En trabajos de principios de la década, la imaginería computacional se ve
claramente, con comerciales y video-promocionales hechos completamente en animación
3D por computadora, y se resalta la novedad de este nuevo medio. Para finales de la
década, la animación computacional se vuelve un elemento integrado en la mezcal de
acción viva, tipografía y diseño gráfico.
Aunque estas transformaciones sólo sucedieron recientemente, la ubicuidad del nuevo
lenguaje visual híbrido de hoy es tal que cuesta trabajo recordar qué tan diferentes eran
las cosas antes. Igualmente, los cambios en los procesos de producción y equipamiento
que hicieron este lenguaje posible también se borran rápidamente de la memoria del
público y de los profesionales. A manera de evocar rápidamente estos cambios, vistos
desde la perspectiva de un profesional, citaré parte de la entrevista que hicieron a Mindi
Lipschultz en 2004, quien ha trabajado como editor, productor y director en Los Ángeles
desde 1979:
Si querías ser más creativo [en los 80’s], no bastaba con añadir más software a tu
sistema. Tenías que gastar cientos de miles de dólares y comprar una Paintbox. Si
querías hacer algo gráfico (una entrada para un show de TV con muchas capas),
tenías que ir a una casa editora y gastar unos miles de dólares por hora para
hacer exactamente lo mismo que se hace hoy en una computadora barata con
varios programas de software. Ahora, con Adobe After Effects y Photoshop, puedes
232 http://accad.osu.edu/~waynec/history/lesson6.html; http://area.autodesk.com/
flame20#20years.

223
hacer todo de una vez. Puedes editar, diseñar y animar. Puedes hacer 2D o 3D,
todo en tu computadora de escritorio en casa o en una pequeña oficina233.
En 1989, los antiguos países soviéticos de Europa Central y del Este se liberaron
pacíficamente de la Unión Soviética. En el caso de Checoslovaquia, este evento se
recuerda como la Revolución de Terciopelo (en contraste con las típicas revoluciones de la
historia moderna acompañadas de matanzas). Para enfatizar las graduales (y con un ritmo
casi invisible) transformaciones ocurridas en la estética de la imagen en movimiento entre
1993 y 1999, me apropiaré del término Revolución de Terciopelo para referirme a esas
transformaciones. (Aunque suene atrevido comparar transformaciones políticas con
estéticas éstas dos comparten la misma calidad pacífica, incluso es posible notar que
ambas están relacionadas).
Finalmente, antes de proceder, también debo explicar mi uso de ejemplos. El lenguaje
visual que estoy analizando nos rodea hoy en día (esto puede explicar porque los
académicos han estado ciegos ante él). Después de la globalización, éste lenguaje es
hablado por los profesionales de la comunicación en decenas de países de todo el mundo.
Puedes ver tú mismo los ejemplos de varias estéticas de las que hablaré con simplemente
ver la televisión y poner atención en los gráficos, ir a un club y ver el performance de un VJ,
visitar los sitios web de diseñadores de gráficos animados y compañías de efectos
visuales, o ver un libro sobre diseño contemporáneo. No obstante, he incluido el título de
proyectos específicos para que el lector pueda ver exactamente a lo que me estoy
refiriendo. (He elegido trabajos de estudios de diseño y de artistas de renombre para que
los puedas encontrar fácilmente en el web). Pero como mi objetivo es describir el nuevo
lenguaje cultural que para hoy se ha vuelto prácticamente universal, quiero enfatizar que
cada uno de estos ejemplos puede ser substituido por muchos otros.
La estética de la hibridación
233 Mindi Lipschultz, entrevistado por The Compulsive Creative, mayo 2004,
http://www.compulsivecreative.com/interview.php?intid=12

224
En la segunda mitad de los 90’s, una de las características que identifican a los motion
graphics que claramente lo distinguían de las demás formas de imágenes en movimiento
existentes en ese tiempo era el papel central de la tipografía dinámica. El término “motion
graphics” ha sido usado por lo menos desde 1960, cuando un pionero del cine por
computadora, John Whitney, llamó a su compañía Motion Graphics. Sin embargo, hasta
antes de las Revolución de Terciopelo, sólo un puñado de gente y compañías habías
explorado sistemáticamente el arte de la tipografía animada: Norman McLaren, Saul Bass,
Pablo Ferro, R. Greenberg y unos cuantos más234. Pero a mediados de los 90’s, las
secuencias de imágenes en movimiento, o películas cortas, dominaban mediante la
animación de texto y elementos gráficos abstractos, en lugar de acción viva, y empezaron a
producirse en mayor escala. ¿Cuál fue la causa material del despegue de los motion
graphics? Fue After Effects, y demás software relacionado, que corría en computadoras o
estaciones gráficas de trabajo relativamente baratas, que se hicieron disponibles a
pequeños estudios de diseño, efectos visuales y post-producción, y pronto a diseñadores
individuales. Casi de la noche a la mañana, el término “motion graphics” se volvió
conocido. (Como dice el artículo de Wikipedia sobre éste término: “el término ‘motion
graphics’ fue popularizado en el libro de Trish y Chris Meyer sobre Adobe After Effects
titulado ‘Creating Motion Graphics’235). El universo de Gutenberg, de 500 años de edad,
se volvió movimiento.
Junto con la tipografía, el lenguaje completo del diseño gráfico del siglo XX fue “importado”
en el diseño de imágenes en movimiento. Aunque este movimiento no recibió un nombre
en particular, es igualmente importante. (El término “cine de diseño” ha sido usado, pero
nunca ha ganado tanta popularidad como “motion graphics”). Entonces, mientras los
gráficos animados estuvieron durante años limitados a los créditos de películas, y por ende
usando tipografía, hoy el término “motion graphics” se usa para designar secuencias
animadas que combinan texto y elementos de diseño. Pero debemos recordar que, a pesar
de que en el siglo XX la tipografía se usaba en combinación con otros elementos, durante
234 Para una discusión poco frecuente de las prehistoria de los motion graphics, así como un raro
intento de análisis del campo usando conceptos en lugar de ejemplos del portafolio de artistas, ver:
Jeff Bellantoni & Matt Woolman, Type in Motion, 2da. edición (Thames & Hudson, 2004).
235 http://en.wikipedia.org/wiki/Motion_graphic

225
500 años ha dirigido su propia palabra. Así, creo que es importante considerar dos tipos
de “importaciones” que sucedieron en la Revolución de Terciopelo (tipografía y diseño
gráfico del siglo XX) como dos desarrollo históricos distintos.
Aunque los motion graphics ejemplifican los cambios que sucedieron en la Revolución de
Terciopelo, estos cambios son más grandes. Puesto de forma simple, el resultado de la
Revolución de Terciopelo es una nuevo lenguaje visual híbrido de las imágenes en
movimiento en general. Este lenguaje no está confinado a un forma de medios en
particular. Y aunque hoy se manifiesta más claramente en formas no-narrativas, seguido
está presente en secuencias y películas narrativas y figurativas.
He aquí algunos ejemplos. Un video musical puede usar acción viva pero también
tipografía y una variedad de transiciones hechas con gráficas computacionales (por
ejemplo “Go” de Common, dirigido por Convert/MK12/Kanye West, 2005). Otro video
musical puede incrustar al cantante en un espacio de pintura animada (Sheryl Crow “Good
is goog”, dirigido por Pysop, 2005). Un comercial puede sobreponer gráficas, datos y
visualizaciones sobre acción viva (spot para Thomson Reuters, de MK12, 2012). Un
secuencia de créditos puede contrastar figuras planas 2D con espacios de perspectiva 3D
(créditos de Mad Men, de Imaginary Forces, 2007). Un cortometraje puede mezclar
tipografía, gráficas 3D estilizadas, elementos de diseño 2D y acción viva (“Itsu for plaid”,
dirigido por el colectivo Pleix, 2002). (Algunas veces, como ya lo hemos dicho, el término
“cine de diseño” es usado para distinguir cortometrajes independientes que usan diseño,
tipografía y animación por computadora en lugar de acción viva de los “motion graphics”
producidos para clientes comerciales).
En algunos casos, la yuxtaposición de diferentes medios es evidente (el video “Don’t
panic” de Coldplay, 2001; los créditos del show TV “The Inside”, 2005; el comercial “Nike –
Dymamic feet”, 2005, hecho por Imaginary Forces). En otros casos, una secuencia puede
moverse entre diferentes medios tan rápido que apenas se notan (el comercial “Holes” de
GMC Denali, hecho por Imaginary Forces, 2005). Y en otros casos más, un comercial o
unos créditos de película pueden mostrar una imagen de acción viva continua pero
cambiándola periódicamente por otra con apariencia más estilizada.

226
Esta hibridación de medios no se manifiesta necesariamente en una estética de collage
que resalte la yuxtaposición de diferentes medios o diferentes técnicas de medios. Como
contraejemplo de lo que puede ser el resultado de la hibridación, pensemos en una
estética más sutil, bien captada con el nombre del software que en gran medida hizo el
lenguaje visual híbrido posible: After Effects. Este nombre anticipaba los cambios en la
apariencia de los efectos visuales que sucedieron años más tarde. En los 90’s, las
computadoras se usaban para crear efectos especial espectaculares o “efectos
invisibles”236, pero hacia finales de la década vimos que surgió algo más: una nueva
estética visual que va “más allá de los efectos”. En esta estética, el proyecto completo (ya
sea un video musical, un comercial de TV, un cortometraje o un largo segmento de una
película) tiene una apariencia especial en la cuál el realce del material de acción viva no es
completamente invisible pero al mismo tiempo tampoco sobresalta de la manera en que lo
hacían los efectos especiales de los 90’s (el comercial “Basketball black” de Reebok IPump
y los créditos de The Legend of Zorro, ambos por Imaginary Forces, 2005; el
comercial “Fage ‘plain” de Psyop, 2011).
Aunque la solución estética particular varia de un video a otro y de un diseñador a otro,
todos comparten la misma lógica: la aparición simultánea de múltiples medios en un
mismo cuadro. Que estos medios están sobrepuestos abiertamente o entremezclados
perfectamente es menos importante que el hecho de su co-presencia misma. (Una vez,
notemos que cada uno de los ejemplos anteriores puede ser reemplazado por muchos
otros).
El lenguaje visual híbrido también es común en una gran porción de cortos
“experimentales” e “independientes” (o sea, que no fueron encomendados por clientes
comerciales) producidos para festivales, web, aparatos móviles y demás plataformas de
236 Efectos invisibles es el término estándar de la industria. Por ejemplo, la película Contact, de
Robert Zemeckis, fue nominada en los premios VFX HQ en 1997 en las siguientes categorías:
Mejores Efectos Visuales, Mejor Secuencia, Mejor Toma, Mejor Efecto Invisible y Mejor Composición.
http://www.vfxhq.com/1997/contact.html

227
distribución de medios237. Muchos visuales creados por VJ’s y artistas de “cine en vivo”
también son híbridos, combinando video, capas de imágenes 2D, animación e imágenes
abstractas generadas en tiempo real238. Las animaciones de Jeremy Blake, Ann Lislegaard
y Takeshi Murata, que veremos más adelante, demuestran que por lo menos una parte de
los trabajos creados en el circuito del arte también usa el mismo lenguaje de la
hibridación.
Hoy, las características narrativas rara vez mezclan estilos gráficos diferentes en un mismo
fotograma. Sin embargo, un número creciente de películas sí incluye una estética
altamente estilizada que antes hubiera sido identificada con la ilustración, en lugar del
cine. Los ejemplos pueden ser: las series de The Matrix, de los hermanos Wachowski;
Immortal de Enki Bilal (2004); Sin City (2005) y The Spirit (2008) de Robert Rodríguez; 300
(2007) y Watchmen (2009) de Zack Snyder; Avatar de James Cameron (2009); Alice in
Wonderland de Tim Burton (2010); y, Hugo de Martin Scorsese (2011). Estas películas
ejemplifican la práctica de grabar grandes partes del film usando un “fondo digital” (es
decir, una pantalla verde)239. Por consiguiente, la mayoría de las tomas en esas películas
están creadas mediante la composición de secuencias de actores con escenarios
generados por computadora y demás visuales.
Estas películas no yuxtaponen sus diferentes medios de forma tan dramática como los
motion graphics. Tampoco buscan una integración perfecta de visuales CGI (Computer-
237 En diciembre 2005, mientras asistía al festival Impact Media, en Utrecht, Holanda, le pregunté al
director del festival qué porcentaje de las sumisiones recibidas tenía un lenguaje visual híbrido, en
comparación con un lenguaje “tradicional” de cine y video. Él estimó un 50%. En enero 2006, fui
parte del jurado evaluador de proyectos de alumnos de SCI-ARC, una reconocida escuela de
arquitectura en Los Ángeles. Según yo, la mitad de los proyectos incluía curvas geométricas
complejas hechas con Maya. Debido a que After Effects y el predecesor de Maya, Alias, fueron
lanzados en el mismo año, 1993, considero interesante esta similitud cuantitativa en el porcentaje
de proyectos que usan nuevos lenguajes de medios gracias a estos software.
238 Para ver ejemplos, consultar Paul Spinrad (ed), The VJ Book: Inspirations and Practical Advice for
Live Visuals Performance (Feral House, 2005).
239 http://en.wikipedia.org/wiki/Digital_backlot

228
Generated Imagery) y acción viva, que era característico de las primeras películas de
efectos especiales de los 90’s: Terminator 2 (1991) y Titanic (1997) de James Cameron.
Más bien exploran el espacio que se encuentra a la mitad, entre superposición e
integración total.
Las escenas de Matrix, Sin City, 300, Alice In Wonderland, Hugo, entre otras, grabadas
sobre un fondo digital combinan múltiples medios para crear una nueva estética estilizada
que no puede reducirse a la apariencia familiar de las acción en vivo cinematográfica o a
la animación 3D por computadora. Estas películas muestran la misma lógica de los cortos
con motion graphics, que a primera vista pueden parecer muy diferentes. Esta lógica es la
misma que la que se observa en la creación de nuevos híbridos biológicos. Esto es, el
resultado del proceso de hibridación no es simplemente la suma mecánica de las partes
antes existentes sino una nueva “especie”; una nuevo tipo de estética visual que no existía
antes.
En los comerciales de TV producidos en los 2000’s, ésta estética altamente estilizada se
volvió una de las apariencias clave de la época. Muchas capas de acción viva, elementos
animados 2D y 3D, efectos de partículas y demás que combinaban en un todo integrado.
Este resultado tiene los códigos del realismo (acortamiento de perspectiva, perspectiva
atmosférica, correcta combinación de luces y sombras), pero al mismo tiempo amplifica la
realidad visible. (No podemos calificar esta estética de “hiperreal” porque las imágenes
creadas tienen un look distinto de las obras de artistas hiperreales como Denis Peterson,
cuyos trabajos se veían como fotografías comunes). Nuestra percepción se aumenta
mediante: alto contraste en escala de grises; alta saturación de colores; pequeñas ondas
de partículas que emulan el movimiento de objetos; acercamientos a la textura de
superficies (gotas de agua, comida, piel, aparatos electrónicos, etc.); contrastes entre la
texturas irregulares de la naturaleza y suave degradados 2D y render 3D; la rápida
transformación de la composición del cuadro y la posición y dirección de la cámara (para
ver más ejemplo de estas estrategias, se pueden ver los comerciales de Psyop240).
240 http://www.psyop.tv/projects/live-action/ (October 31, 2012)

229
Se puede decir que estos comerciales crean un “mapa” más grande que el territorio que
representan, debido a que muestran espacialmente más detalles y texturas y, al mismo
tiempo, comprimen el tiempo, moviéndose más rápido a través de la información. También
podemos hacer una comparación con los satélites que observan la Tierra, que capturan su
superficie a un detalle imposible para el humano (de la misma manera en que un ser
humano no puede ver al mismo tiempo el close-up de la superficie y los detalles del
movimiento de los objetos presentados en el espacio ficcional del comercial).
En resumen, el resultado del cambio a un ambiente de producción basado en software por
parte de la creación de la imagen en movimiento, es un nuevo lenguaje visual. Así como
las técnicas individuales de software que lo hacen posible, este lenguaje, como un todo,
hereda los rasgos de medios precedentes de imagen: cine, animación de acetato y
modelos, animación por computadora, fotografía, pintura, diseño gráfico y tipografía. Sin
embargo, no se reduce a ninguno de estos medios. Más bien, es una verdadero híbrido
(hijo de medios de imagen del siglo XX que tiene rasgos de ellos pero conservando su
identidad).
Veamos al detalle dos cortometrajes para ver cómo la estética de la hibridación funciona a
través de toda la película. Sodium Fox de Blake (2005) y Untitled (Pink Dot) de Murata
(2007) son excelentes ejemplos de nuevos lenguajes visual híbridos que actualmente
dominan la cultura de la imagen en movimiento. Entre muchos artistas reconocidos que
trabajan con imágenes digitales en movimiento, Blake fue de los primeros en desarrollar
exitosamente su estilo propio de medios híbridos. Su video Sodium Fox es una
combinación sofisticada de dibujos, pinturas, animación 2D, fotografía y efectos
disponibles en el software. Mediante una estrategia comúnmente usada por los artistas en
relación con los medios comerciales del siglo XX, Blake reduce el ritmo veloz de los motion
graphics tal como los vemos actualmente. Sin embargo, a pesar del ritmo lento, su
densidad de información es similar a los gráficos que vemos en clubes, videoclips,
televisoras, y demás. Sodium Fox crea esta densidad mediante una original exploración de
la característica básica de los ambientes de producción basados en software en general, y
de After Effects en particular: la construcción de la imagen con varias capas. Claro que la
animación tradicional con acetato de inicios del siglo XX también implicaba la construcción
de una imagen con diferentes acetatos sobrepuestos (cada uno de ellos con diferentes

230
elementos de la imagen total, por ejemplo, uno podía tener una cara, otro los labios, otro el
cabello, otro un paisaje, etc.).
Con el software computacional, no obstante, los diseñadores pueden controlar
precisamente la transparencia de cada capa; también pueden añadir diferentes efectos
visuales (como el desenfoque) entre las capas. Como resultado, en lugar de crear una
narrativa visual basada en el movimiento de unos cuantos elementos visuales en el
espacio (como era común en la animación del siglo XX, tanto comercial como
experimental), los diseñadores tienen ahora muchas nuevas maneras de crear animación.
Al explorar esta posibilidades, Blake confecciona su propio lenguaje visual en el cuál los
elementos visuales en diferentes capas están, continua y gradualmente, “sobrescritos”
entre ellos. Si conectamos este nuevo lenguaje con el cine del siglo XX, podemos decir que
en las obras de Blake se funden continuamente en diferentes elementos de la imagen (y
no de todo del fotograma completo). El resultado es una estética que equilibra la
continuidad visual mediante un ritmo constante de re-escritura y desvanecimiento visual y
de superposición gradual.
Como Sodium Fox, Untitled (Pink Dot) de Murata también desarrolla su propio lenguaje
dentro del paradigma general de la hibridación de medios. Murata crea una imagen que
pulsa y respira, dándole una sensación biológica. En la última década, muchos
diseñadores y artistas han usado algoritmos y técnicas inspiradas de la biología para crear
movimientos semejantes a los animales en sus animaciones e interacciones generativas.
Sin embargo, en el caso de Untitled (Pink Dot), la imagen como todo parece cobrar vida.
Para crear este ritmo pulsante, similar a la respiración, Murata transforma material de
acción viva (escenas de la película Rambo, 1982) en una flujo de parches de colores
abstractos (a veces parecen pixeles gigantes y, otras, artefactos de alta compresión de
imagen). Pero esta transformación nunca llega a un estado final. Más bien, Murata ajusta
constantemente el grado. (En términos de las interfaces del software de medios, esto
puede corresponder a la animación del parámetro de un filtro o efecto). En un momento
vemos imagen viva casi sin procesamiento; al siguiente se convierte en un patrón
completamente abstracto; a continuación, partes del acción viva se hace visibles y así.

231
En Untitled (Pink Dot), la condición general de la hibridación de medios se realiza como
una metamorfosis permanente. Es cierto, seguimos viendo ecos del movimiento en el
espacio, que era el método central de la animación pre-digital. (Aquí, éste es el movimiento
de las figuras en las tomas de Rambo). Pero ahora el verdadero cambio que importa es el
que sucede entre diferentes estéticas de medios: entre la textura del cine y los patrones
abstractos pulsantes del flujo de parches de colores, entre la “vivacidad” original de las
figuras humanas en acción (captadas por la película) y la exagerada vivacidad artificial
generada es procesadas por la máquina.
Visualmente, Untitled (Pink Dot) y Sodium Fox, no tienen mucho en común. Sin embargo,
ambas películas tienen la misma estrategia: crear una narrativa visual mediante
transformaciones continuas de las capas de la imagen, en comparación con los
movimientos de trazos gráficos o caracteres, común a la animación clásica comercial de
Disney como a los experimentos de Norman McLaren, Oskar Fischinger y otros. Aunque
que podemos asumir que ni Blake ni Murata tenían este objetivo en mente, cada artista de
diferente manera muestra las técnicas claves e cambios conceptuales que definen la
nueva era de la hibridación de medios. El software de medios permite al diseñador
combinar cualquier número de elementos visuales sin importar su medio original y
controlarlos en el proceso. Esta habilidad básica puede ser explorada en varias estéticas
visuales. Las películas de Blake y Murata, con sus ritmos temporales diferentes y
diferentes lógicas de combinación de medios, ejemplifican esta diversidad. Blake usa
capas de gráficos fijos, texto, animación y efectos, haciendo visibles e invisibles los
elementos. Murata procesa la acción viva para crear un flujo constante de imágenes en
donde parece que las dos capas (acción viva e imagen procesada) se empujan entre ellas
hacia afuera.
Remezcla profunda
Creo que la “hibridación de medios” es una nueva etapa esencial en la historia de los
medios. Ésta se manifiesta en diferentes áreas de la cultura del software y no sólo en las
imágenes en movimiento (aunque ésta última ofrece un ejemplo particularmente ejemplar
de la nueva lógica cultural en acción). Aquí, el ambiente del software creación se vuelve un

232
plato de Petri en donde las técnicas y las herramientas de la animación por computadora,
cinematografía en vivo, diseño gráfico, animación 2D, tipografía, pintura y dibujo
interactúan. Y como lo demuestran los ejemplos anteriores, los resultados de los procesos
de la hibridación son una nueva estética y nuevas “especies de medios” que no pueden
reducirse a la suma de los medios que intervinieron en su creación.
¿Podemos entender el nuevo leguaje híbrido de la imagen en movimiento como un tipo de
remix? Desde sus orígenes en la cultura musical de los 80’s, el remix ha surgido
gradualmente como una estética dominante de la era de la globalización, afectando y
reformando todo: de la música y el cine a la moda y la comida. (Si Frederic Jameson se
refirió alguna vez al post-modernismo como “la lógica cultura del capitalismo moderno”,
quizá podemos llamar al remix “la lógica cultural del capitalismo global interconectado”).
Varios autores han estudiado los efectos del remix en muchas áreas culturales, del uso de
medios por niños japoneses (Mimi Ito) a la cultura web (Eduardo Navas). Entre otro libros
representativos, podemos mencionar: Rhythm Science (D. J. Spooky, 2004), Remix:
Making Art and Commerce Thrive in the Hybrid Economy (Lawrence Lessig, 2008),
Mashed Up: Music, Technology, The Rise of Configurable Culture (Aram Sinnreich, 2010),
Mashup Cultures (editado por Stefan Sonvilla-Weiss, 2010) y Remix Theory: The Aesthetics
of Sampling (Eduardo Navas, 2012)241.
Creo que los mecanismos combinatorios responsables de la evolución del “metamedio por
computadora” en general y la nueva estética visual híbrida que surgió en los 90’s pueden
ser considerados un tipo de remix, siempre y cuando hagamos una distinción crucial. La
remezcla típica combina contenidos en un mismo medio. Por ejemplo, un remix musical
puede combinar elementos musicales de varios artistas; un videos musical de animé
puede combinar partes de un film de animé y música tomada de un video musical. Los
241 Mizuko Ito, “Mobilizing the Imagination in Everyday Play: The Case of Japanese Media Mixes,” in
International Handbook of Children, Media, and Culture, Sonia Livingstone y Kirsten Drotner, (eds)
(Sage Publications, 2008); Paul D. Miller, Rhythm Science (MIT Press, 2004); Lawrence Lessig.
Remix: Making Art and Commerce Thrive in the Hybrid Economy (Penguin Press HC, 2008); Aram
Sinnreich, Mashed Up: Music, Technology, and the Rise of Configurable Culture (University of
Massachusetts Press, 2010); Stefan Sonvilla-Weiss, Mashup Cultures (Springer, 2010); Eduardo
Navas, Remix Theory: The Aesthetics of Sampling (Springer Vienna Architecture, 2012).

233
motion graphics producidos profesionalmente también mezclan contenido en un mismo
medio y/o en diferentes medios. Por ejemplo, al inicio del video de “Go”
(Convert/MK12/Kanye West, 2005), el video cambia rápidamente entre la acción viva en
un cuarto y el modelo 3D del mismo cuarto. Más tarde, las tomas de acción viva también
incorporan una panta generada a computadora y fotografías fijas de un paisaje en las
montañas. Las tomas de una bailarina están combinadas con tipografía animada muy
elaborada. Los personajes humanos se transforman en patrones abstractos animados. Y
así.
Estas remezclas del contenido de diferentes medios son comunes en la cultura
contemporánea de la imagen-movimiento. De hecho, empecé discutiendo el nuevos
lenguaje visual señalando que, en el caso de las formas cortas, constituyen la regla y no la
excepción. Pero este tipo de remix es tan sólo un aspecto de la “revolución de los
híbridos”. Para mí, su esencia radica en algo más. Llamémoslo “remezcla profunda”. Hoy,
los diseñadores no sólo remezclan contenido de diferentes medios sino además sus
técnicas fundamentales, sus métodos de trabajo y sus formas de representación y
expresión. Unidos en un mismo ambiente de software, los lenguajes de la cinematografía,
la animación tradicional, la animación por computadora, los efectos especiales, el diseño
gráfico, la tipografía, el dibujo y la pintura conforman un metalenguaje. Un trabajo
producido en este nuevo metalenguaje puede usar todas técnicas, o un subconjunto de
éstas, que antes eran exclusivas a estos diferentes medios.
Podemos ver este nuevo metalenguaje de las imágenes en movimiento como una gran
librería de todas las técnicas precedentes para crear y modificar imágenes en movimiento.
Un diseñador de imágenes animadas selecciona técnicas de la librería y las combina en
una sola secuencia o en un solo fotograma. Pero este marco es engañoso. ¿Cómo combina
exactamente estas técnicas?, Cuando remezclamos contenido, es fácil imaginarse
diferentes textos, audios, elementos visuales o flujos de datos puestos los unos junto a los
otros. Imagina un típica collage del siglo XX excepto que ahora se mueve y cambia con el
tiempo. ¿Pero cómo remezclar las técnicas mismas?
En los casos de las interfaces de los medios híbridos que ya hemos analizado (como la
interfaz de Acrobat), el “remix” de las técnicas significa simple combinación. Las diferentes

234
técnicas aparecen, literalmente, una junto a otra en la UI de la aplicación. En Acrobat: un
botón para adelantar y retroceder, uno para zoom, otro para “encontrar”, y otros más,
están puestos juntos en la barra de herramientas superior al documento en lectura. Otras
técnicas aparecen como herramientas en las listas verticales de los menús: deletrear,
buscar, email, imprimir, etc. Vemos los mismos principios en las interfaces de todas las
aplicaciones de creación y edición de medios. Las técnicas tomadas de varios medios se
presentan con las convenciones de la UI.
Esta “adición de técnicas” que existe en un solo espacio, combinadas sin más necesidad
de interacción está también presente en el remix de contenidos familiares: moda, diseño,
arquitectura, collages, motion graphics. Pensemos en el ejemplo hipotético de un diseño
visual que combina elementos de dibujo, fotos y formas 3D a computadora. Cada uno de
estos elementos visuales es el resultado del uso de particulares técnicas de medios para
el dibujo, la fotografía y las gráficas computacionales. Entonces, a pesar de que nos
podemos referir a estos objetos culturales como remezclas de contenidos, también está
justificado decir que son remezclas de técnicas. Esto aplica también para el diseño predigital,
cuando un diseñador podía usar herramientas y máquinas físicas separadas; y para
el diseño contemporáneo con software, en el diseñador accede a todas estas
herramientas con unos cuantos software compatibles.
Mientras las piezas del contenido, o los botones de la interfaz, o las técnicas estén
simplemente añadidas en lugar de estar completamente integradas, no necesitamos un
término especial como el de la “remezcla profunda”. Lo anterior sigue siendo, para mí,
“remix” en el sentido más común del término. Pero en el caso de la estética de la imagen
en movimiento también vemos algo más. En lugar de una simple añadidura, vemos
interacciones entre las técnicas previamente separadas de la animación por acetato, cine,
animación 3D, diseño y demás. Estas interacciones antes eran impensables. (El mismo
argumento puede ser hecho en relación con otros tipos de objetos culturales y
experiencias creadas con el software de creación de medios, como los diseños visuales y
la música).
Creo que esto es algo que ni los pioneros de los medios por computadora de los 60’s-70’s,
ni los diseñadores de las primeras aplicaciones de producción de medios que aparecieron

235
en los 80’s, tenían pensado. Sin embargo, una vez que todas estas técnicas de medios se
unieron en un mismo ambiente de software (lo que se fue volviendo más sólido en los
90’s), empezaron a interactuar de formas que no hubieran podido imaginar o predecir
anteriormente.
Por ejemplo, aún cuando las técnicas particulares de los medios siguen usándose con
relación a su medio original, también se pueden aplicar a otros medios. Los filtros de
Photoshop, que analizamos previamente, ilustran bien este efecto de “esparcimiento”: las
técnicas que originalmente pertenecían a un medio en particular ahora se puede aplicar a
otros tipos de medios. Por ejemplo, los filtros resplandor de neón, vidriera o destello,
pueden ser aplicados a fotografías y bocetos. (Más precisamente, pueden ser aplicados a
cualquier archivo cargado en la memoria gráfica y que esté en la ventana del documento).
He aquí algunos ejemplos típicos de la estrategia distribuida usada en el diseño de
imágenes en movimiento. El texto ejecuta una coreografía en el espacio 3D; un
desenfoque animado se aplica en un gráfico computacional 3D; campos de partículas
generados algorítmicamente que se fusionan con acción viva; una cámara virtual que se
mueve alrededor de un espacio virtual lleno de dibujos 2D. Para cada caso, la técnicas que
fue asociada originalmente con un medio en particular se aplica ahora a otro tipo de
medio. Hoy, un cortometraje típico o una secuencia puede combinar muchos de estos
pares en un mismo fotograma. El resultado es un lenguaje híbrido, intricado, complejo y
rico en medios (o más bien numerosos lenguajes que comparten la lógica de la remezcla
profunda).
De hecho, estas interacciones entre técnicas virtualizadas de medios define la estética
cultura de la imagen en movimiento contemporánea. Por eso he decidido acuñar un
término especial (la remezcla profunda). Mi idea es distinguir dos partes. Por un lado, las
formas complejas de interacciones entre técnicas (como las distribuidas a través de varios
medios). Por otro lado, la simple remezcla de técnicas y contenidos de medios con las que
somos familiares, ya sea música, animés, arte postmoderno de los 80’s, arquitectura, etc.
Para ejemplos concretos del “efecto distribuido” que ejemplifica la remezcla profunda,
podemos retomar el mismo video “Go” y verlo de nuevo, pero ahora desde una perspectiva

236
diferente. Antes habíamos señalado las formas en que combina elementos visuales de
diferentes tipos de medios: acción viva, fotografías fijas, elementos generados
proceduralmente, tipografía, etc. Sin embargo, exactamente las mismas tomas también
contienen ejemplos de las interacciones entre técnicas, que sólo son posibles en un
ambiente de diseño con software.
Cuando el video empieza, una estructura de bloques y paneles, monocromáticos
perpendiculares, crece rápida y simultáneamente en el espacio y rota para ubicarse en
una posición que permite reconocer que se trata de un cuarto (00:07–00:11). Después, el
cuarto se transforma de una estructura geométrica abstracta en una foto-realista:
aparecen muebles, la textura de madera rueda por el piso y una vista panorámica de una
montaña llena la ventana. Aunque los diferentes estilos de render de gráficos
computacionales ha estado disponible en software de animación de los 80’s, la forma
particular en que el video abre con una estructura 3D monocromática es un ejemplo claro
de remezcla profunda. A mediados de los 90’s, los diseñadores gráficos empezaron a usar
software de animación por computadora; trajeron sus técnicas y sensibilidad a la
animación por computadora que, hasta esas épocas, eran usadas al servicio del fotorealismo.
Las fuertes composiciones diagonales, la intencional interpretación aplanada y la
elección de colores en la apertura del video “Go” subordina las técnicas de foto-realismo
de los gráficos computacionales a una específica disciplina visual y, de ahí, a un diseño
gráfico moderno. La estructura animada 3D hace referencia al suprematismo de Malevich
y Lissitzky, quienes jugaron un rol clave en la creación de la gramática del diseño moderno
y que, en nuestro ejemplo, se ha vuelto un filtro “conceptual” que ha transformado los
gráficos computacionales.
Después de una pausa momentánea que sirve para identificar bien el cuarto, cuya
construcción ha sido casi completada, una cámara rota repentinamente 90 grados (00:15
– 00:17). Esta cámara imposible físicamente es otro ejemplo de remix profundo. Mientras
el software de animación implementa la gramática estándar de la cinematografía del siglo
XX (paneo, zoom, dolly, travelling, etc.), el software no tiene el límite del mundo físico. Por
consecuencia, una cámara se puede mover en dirección arbitraria, seguir una curva
imaginaria y hacer esto a cualquier velocidad. Estos movimientos de cámara imposibles se
vuelven herramientas estándar del diseño contemporáneo de medios y de la

237
cinematografía del siglo XXI; a partir de los 2000’s, los vemos cada vez más seguido en las
películas. Así como los filtros Photoshop, que se pueden aplicar a cualquier composición
visual, los movimientos de cámara virtual también se pueden estar superpuestos en
cualquier escena sin importar que haya sido modelada en 3D, generada proceduralmente,
capturada en video, fotografiada, dibujada o, como en el video de nuestro ejemplo, hecha
de una combinación de diferentes medios.
Siguiendo adelante en el video (00:15–00:22) notamos otra interacción previamente
imposible entre las técnicas de los medios. Esta interacción es una reflexión del lente que
se mueve lentamente a través de toda la escena. La reflexión del lente era originalmente
un artefacto de la tecnología de la cámara y ahora se ha vuelto un filtro (es decir, una
técnica que puede ser “dibujada” sobre una cualquier imagen construida con las técnicas
disponibles al diseñador). Si queremos una prueba más de que estamos frente a una
técnica visual, notemos que la “reflexión del lente” se mueve mientras la cámara se queda
perfectamente estática (00:17–00:22), lo que es lógicamente imposible pero sacrificado a
favor de una experiencia visual más dramática.
Me he referido al nuevo lenguaje de la imagen en movimiento como un “meta-lenguaje”.
Como nuestra argumentación ha estado basada en el término “metamedio
computacional”, ahora debo explicar la conexión entre ambos términos.
La aceleración de los cambios sociales, tecnológicos y culturales en la segunda parte del
siglo XX ha llevado al uso frecuente de los prefijos “meta”, “hiper” y “super” en la teoría y
crítica cultural. Desde el Superstudio (un grupo de arquitectura conceptual muy activo en
los 60’s), los hipermedios de Ted Nelson y el metamedio de Alan Kay al más reciente
supermodernismo e hipermodernidad242, estos términos pueden ser vistos como intentos
por capturar el sentimiento de haber pasado a una viaje a la velocidad de la luz. Así como
los cosmonautas contemplaban la Tierra en los años 60’s desde las órbitas de sus naves
espaciales y por primera vez como un solo objeto, parece que también nosotros estamos
viendo la historia de la humanidad desde otra órbita. Esta connotación parece encajar con
la redefinición práctica y conceptual de la computadora digital como metamedio, que
242 http://en.wikipedia.org/wiki/Hypermodernity

238
incluye la mayoría de las tecnologías y técnicas de los medios existentes así como la
invención de muchas nuevas.
Sabemos que el término de “metalenguaje” tiene significados precisos en la lógica, la
lingüística y la computación, pero aquí estamos usándolo en un sentido similar al de Alan
Kay cuando usa “meta” en “metamedio”. Normalmente un metalenguaje se refiere a un
sistema formal separado para describir medios o lenguajes culturales (así como la
gramática describe cómo un lenguaje natural particular funciona). Pero no es así como Kay
usa el prefijo. Él lo ve como reunión-inclusión-colección, en breve, juntar cosas que antes
estaba separadas.
Imaginemos este metamedio computacional como una conjunto de recursos vasto y en
constante expansión. Este metamedio incluye todas la técnicas de creación y manipulación
de medios, formas de interacción y formatos de datos disponibles a programadores y
diseñadores en su actual momento histórico. Todo está aquí: desde algoritmos para
ordenar y buscar hasta los menús desplegables, pasando por las técnicas de render de
agua y cabello, video juegos, inteligencia artificial y métodos de interfaces multi-touch.
Si vemos cómo se usan estos recursos en diferentes áreas culturales para crear tipos
particulares de contenidos y experiencias, veremos que cada uno de ellos sólo emplea un
subconjunto de todos los recursos disponibles. Por ejemplo, las interfaces gráficas de los
sistemas operativos de hoy en día (Windows, Linux, Mac OS) usan íconos estáticos. En
contraste, en algunas interfaces de aparatos electrónicos (como cierto teléfonos celulares)
vemos que casi todos los íconos tienen algún tipo de animación.
Además, el uso de un subconjunto de todos los elementos existentes no es al azar, sigue
una serie de convenciones particulares. Algunos elementos siempre van juntos. En otros
casos, el uso de algún elemento pondría en contradicción el uso de otro. En otras palabras,
las diferentes formas de medios digitales usan diferentes subconjuntos de todas las
técnicas que permite un metamedio computacional y dicho uso sigue patrones
discernibles.

239
Si estás notando los equivalentes entre lo que los críticos culturales llaman un “lenguaje
artístico”, un “estilo” o un “género”, estás en lo correcto. Cualquier obra literaria u obra de
algún autor o movimiento literario emplea solamente una parte de las técnicas literarias
existentes, y este uso sigue ciertos patrones. Lo mismo puede decirse del cine, de la
música y de todas las demás formas culturales reconocidas. Esto nos permite hablar del
estilo de una novela o película en particular, o del estilo de un autor o de un movimiento en
general. (Los investigadores de cine David Bordwell y Kristin Thompson llaman a esto un
“sistema estilístico” que definen como un “uso de técnicas modelado y significante”. En el
caso del cine, dividen éstas técnicas en cuatro categorías: puesta en escena,
cinematografía, edición y sonido243). Cuando todo un campo cultural puede ser dividido en
unos cuantos grupos de trabajo distintos, en cada uno comparte un patrón o modelo,
generalmente hablamos de “géneros”. Por ejemplo, los teóricos del antiguo teatro griego
identifican comedias y tragedias, y prescriben las reglas que cada género sigue. Hoy, las
compañías usan software automatizado para clasificar blogs en diferentes géneros.
Si por “medio” entendemos un conjunto de recursos tecnológicos estándar, ya sea un
escenario físico o una cámara de cine, luces y celuloide, podemos ver que cada medio
soporta generalmente múltiples lenguajes-estilos-géneros artísticos. Por ejemplo, el medio
de la cinematografía del siglo XX soportaba el montaje ruso de los 20’s, el neorrealismo
italiano de los 40’s, la nuevo ola francés de los 60’s, las películas fantásticas de Kung Fu
de los 80’s en Hong Kong, el cine chino de “quinta generación” de los 80-90’s, etc.
De forma similar, un metamedio computacional puede soportar múltiples metalenguajes
culturales o artísticos. En otras palabras, en el esquema teórico que propongo, sólo hay un
solo metamedio, pero muchos metalenguajes.
Entonces, ¿qué es un metalenguaje? Se definimos un lenguaje artístico como un uso
modelado (o que sigue un patrón) de un selecto número de técnicas disponibles en un
medio dado244, un metalenguaje es un uso modelado de un subconjunto de todas las
243 David Bordwell & Kristin Thompson, Film Art: an Introduction, 5ª. edición (The McGraw-Hill
Companies, 1997), p. 355.
244 Esta definición es adoptada de Bordwell & Thompson, Film Art, p. 355.

240
técnicas disponibles en el metamedio computacional. Pero no cualquier subconjunto. De
hecho, sólo tiene sentido hablar de metalenguaje (en oposición a lenguaje) si las técnicas
que usa vienen de diferentes lenguajes culturales previos. Como ejemplo, pensemos en el
metalenguaje de los globos terráqueos virtuales populares y comerciales (Google Earth o
Microsoft Bing Maps). Estas aplicaciones combinan sistemáticamente diferentes tipos de
formatos de medios y técnicas de navegación de medios que previamente estaba
separadas. Estas combinaciones siguen patrones comunes. Otro ejemplo sería el
metalenguaje común a muchas interfaces gráficas de usuario (recordemos mi análisis a la
interfaz de Acrobat, que combina metáforas de diferentes tradiciones de medios).
Debido a que las imágenes en movimiento de hoy combinan técnicas de diferentes medios
visuales que casi nunca se encontraron hasta mediados de los 90’s, podemos usar
justificadamente el término “metalenguaje” en su caso. El diseño visual de hoy tiene su
propio metalenguaje, que a su vez es un subconjunto del metalenguaje de las imágenes en
movimiento. Esto se explica porque el diseñador de imágenes en movimiento tiene acceso
a todas las técnicas de un diseñador visual y otras más debido a que trabaja con la
dimensión adicional del tiempo. Estos dos metalenguajes se sobreponen en cuanto a
patrones que les son comunes, pero hay algunas diferencias importantes. Por ejemplo,
sucede muy seguido que la imagen en movimiento de hoy navegue por un espacio 3D
junto con otros elementos 2D. En contraste, los diseños visuales de los medios impresos y
del web son generalmente bidimensionales: reúnen sus elementos en una superficie plana
imaginaria (Tengo la impresión de que la principal insistencia en esta característica plana
es que estos diseños existen junto a largos bloques de texto existentes en 2D).
Capas, transparencia y composición
Hasta ahora nos hemos enfocado en la estética de las imágenes en movimiento que
surgieron a partir de la Revolución de Terciopelo. Para continuar esta investigación,
pondremos más atención al análisis del nuevo ambiente de producción basado en
software que hizo posible esta estética. Las siguientes secciones del presente capítulo
observarán las herramientas de After Effects y demás aplicaciones de creación de medios,
sus interfaces y las formas en que se usan en conjunto durante la producción (o sea, en el

241
flujo de diseño). En lugar de estudiar todas las propiedades de las herramientas y de las
interfaces, resaltaremos un par de supuestos fundamentales (formas de entender lo que
es un proyecto de imágenes en movimiento) que, como veremos, son muy diferentes de
cómo fueron vistos en el siglo XX.
Probablemente el cambio más drástico sucedió entre 1993 y 1999, con la habilidad de
combinar múltiples niveles de imágenes con diferentes grados de transparencia mediante
la composición digital. Si comparamos un típico video musical o un spot de TV de 1986
con su contraparte de 1996, las diferencias son sorprendentes. (Lo mismo pasa con otras
áreas del diseño visual). Como ya lo hemos dicho, en 1986 los “bancos de memoria
informatizados” eran limitados en su capacidad de almacenamiento y prohibitivamente
caros. Los diseñadores no podían cortar y pegar fácilmente múltiples fuentes de imágenes.
Y cuando se lograban ensamblar varias referencias visuales, los diseñadores sólo podían
ponerlas al lado o encimarlas. No se podía modular con precisión su composición
mediante el ajuste de niveles de transparencia de uno o todas las imágenes. Más bien, se
tenía que conformar con las técnicas de fotomontaje popularizadas en los años 20’s. En
otras palabras, la falta de transparencia restringió el número de diferentes fuentes de
imágenes que se podían integrar en una misma composición sin que parecieran fotocollages
de John Heartfield, Hannah Hoch o Robert Rauschenberg (un mosaico de
fragmentos sin algún dominante claro)245.
Además de permitir la superposición de muchas capas transparentes, la composición
digital también facilitó otra operación que antes era muy complicada. Hasta los 90’s,
diferentes tipos de medios, como la animación a mano, las grabaciones ópticas y la
tipografía, prácticamente nunca aparecían en un mismo cuadro. Más bien, aparecían en
diferentes tomas, como se veía en algunos comerciales, spots y películas experimentales.
Poco directores lograron crear un sistema estético a partir de dichas yuxtaposiciones, por
ejemplo Jean-Luc Godard. En sus realizaciones sesenteras, como Weekend (1967), Godard
insertaba tipografías gruesas entre la acción viva, creando lo que se puede llamar un
245 En el caso del video, uno de los factores que dificultaba la combinación de múltiples visuales era
la rápida degradación de su señal cuando era copiado más de un par de veces. Una copia no ya no
cumplía con los estándares de teledifusión.

242
“media-montaje” (en oposición al montaje de tomas de acción viva, tal como lo hicieron los
rusos de los años 20’s). También en los 60’s, el pionero de los motion graphics Pablo
Ferro, quien apropiadamente llamó a su compañía Frame Imagery, creó cortos
promocionales y gráficas de TV que jugaban con la yuxtaposición de diferentes medios,
reemplazándose mutua y rápidamente246. En varios trabajos de Ferro, se observan
imágenes estáticas de diferentes formas de letras, líneas de dibujos, pinturas hechas a
mano, fotografías, secuencias cortas de noticias, y demás visuales, que venían uno
después del otro a una velocidad rapidísima.
En el cine, la superposición de diferentes medios en el mismo cuadro estaba usualmente
limitada a dos medios encimados de manera estándar, es decir, las letras estáticas que
aparecen sobre imágenes fijas o en movimiento en las secuencias de los créditos de
entrada. En los 60’s, tanto Ferro como otro pionero de los motion graphics, Saul Bass,
crearon algunas secuencias de créditos remarcables, en donde los elementos visuales de
diferente origen se sobreponían más dinámicamente (como los créditos de Vertigo de
Alfred Hitckcock hechos por Bass en 1958). (La secuencia de Bass hecha en 1959 para
North by Northwest es considerada como el primer uso de tipografía en movimiento).
Creo que podemos decir estas complejas yuxtaposiciones de medios en un mismo cuadro
eran excepciones raras en el universo de “unimedios”, en donde las imágenes filmadas
aparecían en películas y las imágenes dibujadas en cine de animación. El único director de
cine del siglo XX que sé que haya construido su estética única mediante la combinación
sistemática de diferentes medios en el cuadro fue Karel Zeman. Así, una típica toma de
Zeman podía contener figuras humanas filmadas, un antiguo grabado como fondo y un
modelo miniatura247.
Los logros de estos directores y diseñadores son particularmente sorprendentes debido,
sobretodo, a la dificultad que existía para combinar diferentes medios en el mismo
246 Jeff Bellantoni & Matt Woolman, Type in Motion (Rizzoli, 1999), pp. 22–9.
247 Mientras que los efectos especiales del cine combinaban seguido diferentes medios, se usaban
en conjunto para crear un solo espacio ilusorio, en lugar de sobreponer para el efecto estético, como
en las películas y secuencias de Godard, Zeman, Ferro y Bass.

243
fotograma en aquella época. Para hacer esto era necesario utilizar los servicios de efectos
especiales de diferentes compañías que usaban impresoras ópticas. Algunas técnicas que
eran más baratas y accesibles, como la doble exposición, eran limitadas en cuanto a su
precisión. Mientras los diseñadores de imágenes estáticas podían por lo menos cortar y
pegar múltiples elementos en la misma composición para crear un fotomontaje, su
equivalente en cuanto a imágenes en movimiento no era para nada trivial.
Para poner todo esto en términos más generales podemos decir que antes de la
informatización de los 90’s, las habilidades de los diseñadores para acceder, manipular,
remezclar y aplicar filtros a la información visual, fija o en movimiento, estaban algo
limitadas. De hecho, eran las mismas que cien años antes. En retrospectiva, podemos ver
que iban en contra de la flexibilidad, velocidad y precisión de la manipulación de datos que
ya se practicaba en la mayoría de los campos computacionales profesionales (ciencias,
ingeniería, contabilidad, gestión, etc.). Entonces, sólo fue cuestión de tiempo antes de que
todos los medios de imágenes se convirtieran en datos digitales y que los ilustradores,
diseñadores gráficos, animadores, editores de películas y video, y diseñadores de gráficos
animados empezaran a manipularlos vía software en lugar de las herramientas
tradicionales. Pero esto es obvio en nuestros días, después de la Revolución de Terciopelo.
En 1985, Jeff Stein dirigió un video musical para el sencillo “You Might Think” del grupo
new wave The Cars. Este video fue uno de los primeros en usar sistemáticamente gráficas
computacionales; tuvo un gran impacto en el mundo del diseño y MTV le otorgó el primer
lugar en sus primeros premios a la música. Stein logró crear un mundo surreal en donde la
cabeza de uno de los integrantes de la banda estaba animada sobre diferentes fondos de
video. En otras palabras, Stein tomó la estética de las caricaturas (personajes 2D
animados, superpuestos sobre un fondo 2D) y la recreó usando imágenes de video.
Además, también añadió elementos animados por computadora sencillos para resaltar el
efecto surreal. Esto era perturbador porque nadie había visto dichas yuxtaposiciones
antes. De repente, el foto-montaje modernista cobró vida. Pero diez años después, esos
video-collages no sólo se han vuelto ubicuos sino también más complejos, con más capas
y más sutiles. En lugar de dos o tres capas, una composición ahora puede tener cientos e
incluso miles de capas. Cada una con su propio nivel de transparencia.

244
En resumen, la composición digital de hoy ha permitido a los diseñadores de imágenes en
movimiento combinar fácilmente cualquier número de elementos visuales sin importar su
medio de origen y controlar cada uno de ellos en el proceso. Podemos hacer una analogía
entre la grabación multicanal de audio y la composición digital. En la primera, cada pista
de sonido puede ser manipulada individualmente para producir un resultado determinado.
Igualmente, en la composición digital, cada elemento visual puede ser modulado
independientemente de varias formas: redimensionado, coloreado, animado, etc. Así como
el artista musical se puede concentrar en una pista en particular al silenciar todas las
demás, el diseñador desactiva todas las capas excepto la que está ajustando. De manera
similar, tanto un músico como un diseñador pueden en todo momento sustituir un
elemento de la composición por otro, borrarlos o añadir otros. Pero lo más importante, así
como la grabación multicanal redefinió el sonido de la música popular desde los 70’s, una
vez que la composición digital se volvió accesible en los 90’s, cambió fundamentalmente
la estética visual de la mayoría de las formas de imagen en movimiento.
Esta discusión sólo ha evocado la superficie del tema de esta sección: capas y
transparencia. Por ejemplo, no hemos analizado las técnicas reales de la composición
digital ni tampoco el concepto fundamental de un canal alfa, que merece un tratamiento
separado y detallado. Tampoco hemos visto las posibles historias de medios que han
llevado a la composición digital, ni hemos examinado su relación con las tecnologías de
impresión óptica, de manipulación y de efectos de video de los años 80’s. Estas historias y
relaciones fueron examinadas en el capítulo “Compositing” de The Language of New
Media, aunque desde una perspectiva diferente a la que hemos tomado aquí. En aquel
tiempo (1999), me interesaba en la composición a partir de la cuestión del realismo
cinemático, las prácticas de montaje y la construcción de efectos especiales en películas.
Hoy, sin embargo, me queda claro que además de alterar el régimen del realismo
cinemático en favor de otra estética visual, la composición digital ha tenido otro, y más
fundamental, efecto.
A finales de los 90’s, la composición digital se ha vuelto la operación básica en la creación
de todo tipo de imágenes en movimiento, y no sólo en películas de gran presupuesto. Así
que, mientras que originalmente fue desarrollada como una técnica de efectos especiales,

245
en los 70’s e inicios de los 80’s248, la composición digital ha tenido un efecto más amplio
en la cultura visual y de medios contemporánea, más allá de los efectos especiales. La
composición ha jugado el rol principal que ha convertido a la computadora digital en una
especie de laboratorio experimental (o una placa de Petri) en donde diferentes medios se
encuentran, con sus técnicas y estéticas, para ser combinados para crear nuevas
especies. En breve, la composición digital ha sido esencial en el desarrollo del nuevo
lenguaje visual híbrido de las imágenes en movimiento que hoy vemos en cualquier parte.
Al inicio definido como una técnica digital particular diseñada para integrar la acción viva
del cine con las gráficas computacionales de secuencias de efectos especiales, la
composición digital se ha vuelto un “integrador universal de medios”. Y aunque fue
originalmente creado para soportar la estética del realismo cinemático, con el tiempo ha
tenido en realidad el efecto contrario. En lugar de llevar a los medios hacia una fusión
perfecta, la composición más bien ha llevado al surgimiento de nuevos híbridos de medios
en donde la yuxtaposición entre la acción viva y los algoritmos generativos, la 2D y la 3D,
los medios bitmap y los vectoriales, se hace deliberadamente visible en lugar de ocultarse.
La interfaz de After Effects: del tiempo a la composición
Mi tesis sobre la hibridación de los medios aplica tanto a los objetos culturales como al
software usado para crearlos. Así como los medios de imagen en movimiento hechos por
los diseñadores de hoy combinan formatos, supuestos y técnicas de diferentes medios, las
herramientas e interfaces del software que usan también se remezclan. Tomemos una vez
más After Effects como caso de estudio para ver cómo su interfaz remezcla los diferentes
métodos de trabajo previos en distintas disciplinas.
Cuando los diseñadores de imágenes en movimiento empezaron a usar software de
composición y animación como After Effects, su interfaz les invitaba a pensar dichas
imágenes de una nueva forma. Los sistemas de edición de cine y video que fueron
248 Thomas Porter & Tom Duff, “Compositing Digital Images,” ACM Computer Graphics 18, no. 3
(julio 1984): pp. 253–9.

246
denominados como editores no lineales (ejemplificados por Avid, Premiere y Final Cut249)
conceptualizaron un proyecto de medios como una secuencia de tomas organizada en el
tiempo. Consecuentemente, mientras los editores no lineales (abreviados como NLE por
sus siglas en inglés) dieron al usuario muchas herramientas para ajustar los cortes, dieron
por supuesto la constante del lenguaje cinematográfico que vino con su organización
industrial (todos los cuadros tienen el mismo tamaño y relación de aspecto). Este es un
ejemplo de un tendencia más grande. Durante la primera etapa del desarrollo de software
cultural, sus pioneros exploraban las posibilidades del nuevo metamedio computacional en
cualquier vía que les interesara, debido a que su uso comercial no era aún una opción
viable (a excepción del software CAD). Sin embargo, al inicio de los 80’s, una nueva
generación de empresas (Aldus, Autodesk, Macromedia, Adobe, entre otras) empezaron a
producir software basado en GUI para industrias particulares: producción de TV, diseño
gráfico, animación, etc. Como resultado, muchos de los principios del flujo de trabajo,
convenciones de interfaz y limitaciones de las tecnologías estándares de medios
empezaron a recrearse metódicamente en el software(aún cuando el software mismo no
tenía dichas limitaciones). El software de los NLE es un caso ejemplar. En contraste, desde
su inicio, la interfaz de After Effects introdujo un nuevo concepto de la imagen en
movimiento como una composición organizada en el tiempo y en el espacio.
El corazón de la interfaz de After Effects es una Composición, conceptualizada como un
gran lienzo que juega el rol de fondo de los elementos visuales que pueden tener tamaños,
proporciones y contenidos arbitrarios (videos, fotos, gráficos abstractos, texto, etc.).
Cuando yo empecé a usar After Effects poco después de que salió, recuerdo haberme
sorprendido de que el software no ajustara los gráficos automáticamente al tamaño del
cuadro a medida que los añadía a la ventana de la Composición. El supuesto
cinematográfico fundamental que lo había acompañado durante toda su historia había
desaparecido (que el cine consiste en muchos cuadros del mismo tamaño y relación de
aspecto).
249 La funcionalidad de la composición se añadió gradualmente con el tiempo a la mayoría de los
editores no lineales. Hoy la distinción entre las interfaces de After Effects, Flame, Avid y Final Cut es
menos pronunciada.

247
Para los paradigmas de edición de cine y video del siglo XX, la unidad mínima en la cuál
trabaja un editor es el cuadro. El editor puede cambiar la duración de un corte, ajustar en
dónde empieza o acaba un segmento, pero no puede modificar directamente el contenido
del cuadro. El cuadro funciona como un tipo de “caja negra” que no puede ser “abierta”.
(Este era el trabajo de las compañías y departamentos de efectos especiales). Pero en la
interfaz de After Effects, la unidad básica no es el cuadro sino un elemento visual puesto
en la ventana de la Composición. Cada elemento está conceptualizado como un objeto
independiente. En consecuencia, una composición de medios se entiende como un
conjunto de objetos independientes que puede cambiar con el tiempo. La palabra misma
“composición” es importante en este contexto debido a que hace referencia a medios 2D
(dibujo, pintura, fotografía, diseño) en lugar de la cinematografía (es decir que el espacio
se opone al tiempo).
¿De dónde salió la interfaz de After Effects? Dado que este software es comúnmente
usado para crear gráficos animados y efectos visuales, no es sorprendente que los
elementos de su interfaz pueden remontarse a tres campos distintos: animación, diseño
gráfico y efectos especiales. Y debido a que estos elementos están íntimamente
integrados para ofrecer al usuario una nueva experiencia que no se puede reducir a la
simple suma de los métodos de trabajo existentes, es razonable pensar la UI de After
Effects como un ejemplo de “remix profundo”.
En la animación tradicional del siglo XX, un animador sobreponía acetatos transparentes
entre ellos. Cada acetato contenía un dibujo diferente, por ejemplo el cuerpo de un
personaje en uno, la cabeza en otro, los ojos en otro, etc. Como los acetatos eran
transparentes, los dibujos se “componían” automáticamente en un solo cuadro. A pesar de
que la metáfora de After Effects no usa un pila de acetatos directamente, sí está basado
en el mismo principio. Cada elemento en la ventana de la Composición tiene una
“profundidad virtual” relativa a todos los demás elementos. Todos juntos forman un
apilamiento virtual. En cualquier momento, el diseñador puede cambiar la posición relativa
de un elemento, borrarlo o añadir nuevos.
También se puede ver una conexión entre la interfaz de After Effects y otra técnica de
animación popular del siglo XX: el stop motion. Para crear tomas stop motion, los muñecos

248
o cualquier otro objeto 3D se ponían frente a la cámara de cine y se iban animando
manualmente, paso por paso. Por ejemplo, un animador podía ajustar la cabeza de un
personaje, moviéndola progresivamente de izquierda a derecha en pequeños pasos.
Después de cada paso, el animador hacía una toma en la película, después hacía otro
ajuste y exponía de nuevo, etc. (Entre los animadores y cineastas que usaron
creativamente esta técnica están: Wladyslaw Starewicz, Oskar Fischinger, Aleksandr
Ptushko, Jiří Trnka, Jan Svankmajer, y los hermanos Quay).
De la misma manera que las prácticas de animación con acetato y stop motion, After
Effects no hace ningún supuesto sobre el tamaño o posición de los elementos individuales.
En lugar de trabajar con unidades de tiempo estándar (o sea con fotogramas de contenido
visual fijo), un diseñador ahora lo hace con elementos visuales separados. Un elemento
puede ser un cuadro de video digital, una línea de texto, una forma geométrica, etc. El
trabajo final es resultado de una disposición particular de estos elementos en el tiempo y
espacio. Por consecuencia, un diseñador que usa After Effects puede compararse con un
coreógrafo que hace una danza “animando” los cuerpos de los bailarines, especificando
cuando entran y salen de escena, sus trayectorias en el espacio y los movimientos que
hacen. (Al respecto, es relevante señalar que aunque After Effects no hace referencia
directa, otro software que era comúnmente usado para el diseño de multimedios,
Macromedia Director, sí se refería explícitamente a la metáfora del teatro en su interfaz de
usuario).
Aunque se puede vincular la interfaz de After Effects con los métodos de animación
tradicional, tal como fueron usados por estudios comerciales, el método de trabajo que
recalca este software está más relacionado con el diseño gráfico. En los estudios
comerciales de animación del siglo XX, todos los elementos (dibujos, escenarios,
personajes, etc.) se preparaban con anticipación. La cinematografía misma era un proceso
mecánico. Por supuesto que se pueden encontrar excepciones a esta separación de tareas
en la animación experimental, en donde una película era hecha generalmente por una
persona. Esto permitía al cineasta inventar una película sobre la marcha, en lugar de tener
todo listo con antelación. Un ejemplo clásico de esto es la Motion Painting 1 de Oskar
Fischinger, creada en 1949. Fischinger hizo este film de 11 minutos mediante la
modificación constante de una pintura y su exposición en celuloide en cada paso. Este

249
proceso le llevó nueve meses. Como Fischinger estaba filmando en celuloide, tuvo que
esperar un largo tiempo antes de ver sus resultados. Así como lo ha dicho el historiado de
animación abstracta William Moritz, “Fischinger pintaba diario durante cinco meses sin
saber cómo iba quedando su película, sobretodo porque quería mantener absolutamente
las mismas condiciones, incluyendo el celuloide, para evitar variaciones inesperadas en la
calidad de la imagen”250. En otras palabras, crear una animación y ver el resultado
estaban aún más separadas que un proceso de animación comercial.
Por el contrario, un diseñador gráfico trabaja en “tiempo real”. A medida que añade nuevos
elementos, ajusta sus posiciones, colores y demás propiedades, prueba diferentes
imágenes, cambia el tamaño de la tipografía, etc., puede ver inmediatamente el resultado
de su trabajo251. After Effects adopta este método de trabajo al hacer de su ventana de
Composición el centro de su interfaz. Como un diseñador tradicional, el usuario de After
Effects acomoda interactivamente los elementos en esta ventana y ve inmediatamente el
resultado. En breve, su interfaz convierte la cinematografía en un proceso de diseño, y una
película se re-conceptualiza como un diseño gráfico que puede cambiar con el tiempo.
Como vimos cuando revisamos la historia del software cultural, cuando los medios físicos y
electrónicos eran simulados en la computadora, el resultado no es simplemente un mismo
medio como los había antes. Al añadirle nuevas propiedades y métodos de trabajo, la
simulación por computadora cambia la identidad del medio. Por ejemplo, en el caso del
“papel electrónico” como un documento Word o un archivo PDF, podemos hacer muchas
250 Citado en Michael Barrier, Oskar Fischinger. Motion Painting No. 1,
http://www.michaelbarrier.com/Capsules/Fischinger/fischinger_capsule.htm
251 Dependiendo de la complejidad del proyecto y de la configuración del hardware, la computadora
puede, o no, seguir el ritmo de los cambios de un diseñador. Sin embargo, debido a que tiene control
sobre el render final, puede indicarle a After Effects que muestre sólo los contornos de los objetos, u
ocultar algunas capas, etc. Así, manda menos información a procesar a la computadora y libera
memoria para una comunicación en tiempo real. Un diseñador no tiene que esperar a que una cinta
sea revelada o a que la computadora acabe un ciclo de render, pero el diseño tiene sus propias
etapas de “render”, es decir, hacer pruebas. En la impresión, tanto digital como offset, el diseñador
envía pruebas a la impresora para revisar la producción. Si se encuentran problemas, como
desajustes de color, el diseñador hace los ajustes necesarios y envía nuevas pruebas.

250
cosas que antes no eran posibles en el papel ordinario: aumentar o disminuir el tamaño de
la letra, buscar palabras o frases, cambiar el formato, etc. De forma similar, los servicios
de mapas interactivos en línea, como Google o Microsoft, aumentan el papel tradicional de
múltiples y espectaculares formas.
Un gran parte de software contemporáneo para crear, editar e interactuar con medios ha
sido hecha de esta forma. Las tecnologías de medios precedentes se simulaban en la
computadora y se aumentaban con nuevas propiedades. Pero si vemos software de
producción como Maya (para la modelación y animación 3D por computadora) o After
Effects (que se usa en motion graphics, composición y efectos visuales), se observa una
lógica diferente. Estas aplicaciones de software no simulan ningún medio físico previo.
Más bien, hacen préstamos a diferentes medios para combinar y mezclar sus técnicas y
métodos específicos. (Y además añaden nuevas posibilidades que son características de la
computadora, por ejemplo el cálculo automático de valores entre dos fotogramas clave).
Por ejemplo, el software de modelado 3D mezcla técnicas de creación de formas que
antes pertenecían a ciertos medios físicos: modificar la curvatura de una forma redonda en
el modelado con arcilla, construir formas 3D complejas a partir de los materiales de una
maqueta arquitectónica, etc.
De forma similar, la interfaz, las herramientas y el flujo de trabajo de After Effects está
basado en las técnicas e animación y diseño gráfico. (También se pueden ver influencias
de la cinematografías y de los gráficos computacionales). Pero el resultado no es un simple
suma mecánica de los elementos que vinieron de otros medios. Más bien, como el
software remezcla las técnicas y los métodos de trabajo de varios medios que simula, el
resultado son nuevas interfaces, herramientas y flujos de trabajo con su propia lógica. En
este caso, el método de trabajo de After Effects no es ni la animación ni el diseño gráfico ni
la cinematografía. Es una nueva manera de hacer medios de imágenes en movimiento.
Igualmente, el leguaje visual de los medios producidos con éste y otros software similares
es diferente de los lenguajes de imágenes en movimiento que existían antes.
En consecuencia, la Revolución de Terciopelo que desató After Effects, y otros programas,
no solamente vulgarizó los gráficos animados que artistas y diseñadores como John y
James Whitney, Norman McLaren, Saul Bass, Robert Abel, Harry Marks, Richard y Robert

251
Greenberg , habían creado con stop motion, impresión óptica, efectos de video con
hardware y demás técnicas y tecnologías personalizadas. Sobretodo, llevaron al
surgimiento de numerosas estéticas visuales nuevas que no existían antes. Y si la
característica común a estas estéticas es el “remix profundo” no es difícil ver que se
observa en el “remix profundo” de la interfaz de usuario de After Effects.
El espacio 3D como plataforma de diseño de medios
Cuando investigaba lo que los usuarios y las industrias dicen sobre After Effects, me
encontré con una opinión que lo caracterizaba como un “Photoshop con fotogramas
clave”. Creo que esta denominación es de hecho útil252. Pensemos en todas la formas
posibles de manipular una imagen con Photoshop y el nivel de control que permiten su
variedad de controles. Pensemos también en el concepto de composición visual de
Photoshop como apilamiento de cientos de capas potenciales, cada una con su propia
transparencia y canales alfa. Si fuéramos capaces de animar una composición y seguir
usando las herramientas para ajustar los elementos visuales por separado, esto constituye
un nuevo paradigma de creación de imágenes en movimiento. Y esto es lo que After
Effects, y demás software de animación, efectos visuales y composición, hacen posible
hoy253. Y a pesar de que la idea de trabajar con numerosas capas no es nueva, el hecho
de trabajar con potencialmente miles de ellas, cada una con sus controles independientes,
sí cambia su forma y su mensaje. La composición 2D cambió: pasó de ser un efecto
especial reservado a tomas particulares a convertirse en una parte estándar de la interfaz
de creación de animaciones y edición de video.
252 Poco después de que After Effects fue lanzado en enero de 1993, CoSA (la compañía que
produjo este software), fue comprada por Aldus, que a su vez, fue comprada por Adobe (que ya tenía
a Photoshop entre sus productos.).
253 Photoshop y After Effects fueron diseñados, originalmente, por diferentes personas en diferentes
momentos. E incluso después de que Adobe adquirió ambos (Photoshop fue lanzado en 1989 y After
Effects en 1995), le tomó unos cuantos años hacer que se comunicarán e intercambiaran sus
recursos.

252
Aún con su carácter innovador, el paradigma de la composición 2D fue sustituido al inicio
de los 2000’s por uno nuevo: la composición 3D. Si la composición 2D puede ser vista
como una extensión de técnicas de medios ya conocidos, el nuevo paradigma, por su lado,
no surge de medio físicos o electrónicos precedentes. Más bien, toma los medios nacidos
en la era digital de los 60’s y consolidados en los 90’s (gráficas computacionales y
animación 3D interactiva), transformándolos en una plataforma general para el diseño de
medios en movimiento.
El lenguaje usado en el medio de la producción profesional de hoy refleja un
entendimiento implícito de las gráficas 3D como nuevo medio único de la computadora.
Cuando la gente usa términos como “visuales por computadora”, “imaginería
computacional” o “CGI” (abreviación de “Computer Generated Imagery”) todos entienden
que se refieren a gráficas 3D y no a otras fuentes de imágenes como la “fotografía digital”.
Considero los gráficos computacionales 3D como un nuevo medio (y no una extensión de
los bocetos arquitectónicos, la geometría proyectual o el diseño de espacios) debido a que
ofrecen un nuevo método de representar la realidad tridimensional hecha con objetos
existentes y objetos imaginados. Este método es fundamentalmente diferente a lo que
había sido posible con los medios representacionales de la era industrial: captura basada
en lentes (fotografía fija, film, video) y grabación de audio. Con los gráficas
computacionales 3D, podemos representar una estructura tridimensional del mundo en
lugar de capturar solamente una perspectiva del mundo, como en la captura a base de
lentes. También podemos manipular una representación usando varias herramientas con
facilidad y precisión, contrario a la limitada “maleabilidad” de un modelo hecho con
materiales físicos (aunque los avances en la nanotecnología prometen cambios en el
futuro). Y, como lo evidencia la estética de la arquitectura contemporánea, las gráficas
computacionales 3D no son solo una manera más rápida de trabajar con representaciones
geométricas, como los planos y los cortes transversales, usados durante siglos por los
dibujantes. Cuando las generaciones de nuevos arquitectitos y estudiantes empezaron a
trabajar sistemáticamente con modelos 3D y software de animación como Alias, a
mediados de los 90’s, ésta habilidad de manipular una forma 3D (en lugar de manipular
solamente su proyección) llevo rápidamente a la introducción de un nuevo lenguaje
basado en formas curveadas complejas.

253
Cuando la Revolución de Terciopelo de los 90’s hizo posible la combinación de múltiples
fuentes de medios en una simple secuencia de imagen en movimiento, usando una la
interfaz multicapas de After Effects, el CGI se añadió a la mezcla. Hoy, los modelos 3D se
usan de forma rutinaria en composiciones de medios creados en After Effects y software
similar, junto a demás medios. Pero, para ser parte del mix, estos modelos deben estar
ubicados en su propia capa 2D y, por consecuencia, ser tratado como imágenes 2D. Éste
fue el paradigma original de After Effects: todos los medios se pueden unir, siempre y
cuando sean reducidos a 2D254.
Por el contrario, en el paradigma de la composición 3D, todos los tipos de medios se sitúan
en un solo espacio 3D. Una ventaja de esta representación es que, como el espacio 3D es
“nativo” a las gráficas computacionales, los modelos 3D se quedan como son, es decir,
tridimensionales. Una ventaja adicional es que el diseñador puede usar todas las técnicas
de la cinematografía virtual desarrollada en la animación 3D por computadora. Es posible
definir diferentes tipos de luces, vuelos de la cámara alrededor y a través de planos de
imágenes usando cualquier trayectoria, con efectos tipo profundidad de campo y
desenfoque255.
Mientras que los modelos 3D generados por computadora “viven” en este espacio, ¿cómo
podemos traer elementos visual bidimensionales (por ejemplo, video, film digitalizado,
254 Digo “original” porque la última versión de After Effects añadió la posibilidad de trabajar con
capas 3D.
255 Si la composición 2D puede ser entendida como una extensión de la animación tradicional del
siglo XX, en donde una composición consistía en un apilamiento de dibujos en acetato, la fuente
conceptual del paradigma de la composición 3D es diferente. Éste surge de la integración de tomas
en acción con CGI hecho en los 80’s, en el marco de la producción cinematográfica. Tanto el director
de cine y el animador por computadora trabajaban en un espacio tridimensional: el espacio físico del
set, para el primero, y el espacio virtual definido por los modelos 3D, para el segundo. Entonces,
hace sentido usar el espacio tridimensional como plataforma para la integración de ambos mundos.
No es por accidente que NUKE, uno de los programas más importantes en la composición 3D, haya
sido desarrollado internamente en Digital Domain, co-fundada en 1993 por James Cameron, el
director de Hollywood que uso sistemáticamente y revolucionó a integración de CGI en sus películas
como The Abyss (1989), Terminator 2 (1991) y Titanic (1997).

254
tipografía, imágenes dibujadas)? Si el paradigma de la composición 2D trataba todo los
medios como imágenes 2D (incluyendo los modelos 3D), la composición 3D trata todos los
medios como 3D. Sabemos que los elementos 2D no tienen una tercera dimensión
predefinida; ésta tiene que ser añadida para que puedan entrar al espacio 3D. Para esto,
el diseñador pone cuadros planos en ciertas ubicaciones del espacio y sitúa las imágenes
bidimensionales en estos cuadros. Así, todo vive en un espacio 3D común. Esta condición
permite la “remezcla profunda” entre técnicas que ya hemos ilustrado con el ejemplo del
video “Go”. Las técnicas de dibujo, fotografía, cinematografía y tipografía que se usan para
crear y capturar elementos visuales 2D, pueden ahora “jugar” con todas las técnicas de la
animación 3D por computadora (movimientos de cámara virtuales, profundidad de campo
controlable, lentes variables, etc.).
En 1995 escribí el artículo “¿Qué es el cine digital?”. Aquí evocaba la cuestión de cómo los
cambios en la producción de imágenes en movimiento que estaba presenciando (mientras
estaba viviendo en Los Ángeles y siguiendo de cerca los cambios en Hollywood), afectaban
el significado de “cine”. En este texto, proponía que la lógica de la animación a mano, que
durante el siglo XX fue marginal en comparada con el cine, se volvía dominante en la era
del software. Debido a que el software permite al diseñador manipular manualmente
cualquier imagen, sin importar su fuente, las diferencias ontológicas entre las diferentes
imágenes se vuelven irrelevantes. Tanto conceptual como prácticamente, todas se
reducen a la animación a mano.
Por default, After Effects, y otros software de animación, edición de video y composición
2D, tratan un proyecto de imágenes en movimiento como un apilamiento de capas.
Entonces, puedo ampliar mi argumento original y proponer que la lógica de la animación
también se mueve de una posición marginal a una posición dominante. El paradigma de
una composición como apilamientos de elementos visuales independientes, tal como era
practicado en la animación por acetato, se vuelve en la forma estándar de trabajar con las
imágenes en un ambiente de software (sin importar su origen y formato final). En otras
palabras, una “imagen en movimiento” se entiende ahora como un compuesto de capas
de imaginería (en lugar de una imagen fija plana que solo cambia en el tiempo, tal como
fue el caso en la mayor parte del siglo XX). En el mundo de la animación, la edición y la
composición, dicha “imagen de una sola capa” se vuelve un excepción.

255
El surgimiento del paradigma de la composición 3D también se puede ver como un
seguimiento a la lógica de la inversión histórica. La nueva estructura representacional,
desarrollada en el campo de las gráficas computacionales (un espacio virtual 3D que
contiene modelos 3D), se ha movido gradualmente de un rol marginal a uno dominante. En
los 70’s y 80’s, las gráficas computacionales fueron usadas ocasionalmente en una
decena de películas, como Alien (1979), Tron (1981), The Last Starfighter (1984) y The
Abyss (1989), y en selectos comerciales y gráficas de TV. Pero al inicio de los 2000’s, la
estructura de representación de las gráficas computacionales, es decir los espacios
virtuales 3D, funcionaron como una sombrilla para todo tipo de imágenes sin importar su
origen. Un ejemplo de una aplicación que implementa este paradigma es Flame, descrito
efusivamente por un usuario como “un ambiente de composición 3D completo en donde
se pueden traer modelos 3D, crear verdaderos textos 3D y partículas 3D, y distorsionar
capas en el espacio 3D256.
Esto no quiere decir que la animación 3D se vuelve elle misma dominante en la cultura de
la imagen en movimiento, o que la estructura 3D del espacio en donde ahora se
construyen cotidianamente las composiciones de medios se vuelve visible (de hecho casi
nunca pasa). Más bien, la manera en que la animación 3D por computadora organiza los
datos visuales (como los objetos posicionados en el espacio cartesiano) se vuelve la forma
de trabajar con todas las imágenes en movimiento. Como ya lo hemos dicho antes, el
diseñador posiciona todos los elementos que van en la composición (secuencias animadas
2D, objetos 3D, sistemas de partículas, secuencias de video y film digitalizado, imágenes
fijas y fotografías) dentro de un espacio 3D virtual compartido. Ahí dentro, estos elementos
pueden seguir siendo animados, transformados, desenfocados, modificados con filtros,
etc. Así que, mientras todas las imágenes en movimiento se reducen a animación a mano
en términos de su maleabilidad, también podemos decir que todos los medios se vuelven
capas en el espacio 3D. En breve, el nuevo medio de la animación 3D por computadora se
ha “comido” los medios dominantes de la era industrial (fotografía, cine y video a base de
lentes).
256 Alan Okey, contribución en el foro: forums.creativecow.net, Diciembre 28, 2005,
http://forums.creativecow.net/cgi-bin/dev_read_post.cgi?forumid=154&postid=855029

256
Habiendo discutido cómo el software ha redefinido el concepto de “imagen en
movimiento” como un compuesto de múltiples capas, este es un buen momento para
hacer una pausa y considerar otras posibles formas en que el software ha cambiado este
concepto. Cuando el cine en su forma moderna nació a finales del siglo XIX, este nuevo
medio se entendió como una extensión de otro ya familiar (la imagen fotográfica sin
movimiento). Este entendimiento se puede ver en la prensa de aquellos días y también en
uno de los nombres oficiales dados al medio: “imágenes en movimiento”. Al nivel material,
un film consiste en cuadros fotográficos separados. Cuando se reemplazan rápidamente
entre ellos se crea el efecto de movimiento en el espectador. Así que el concepto usado
para entender el cine encajaba en la estructura del medio.
¿Pero sigue siendo apropiado este concepto hoy en día? Cuando grabamos y reproducimos
video, seguimos tratando con la misma estructura: una secuencia de fotogramas. Pero
para los diseñadores profesionales de medios, los términos han cambiado. La importancia
de estos cambios no solo es académica y puramente teórica (como los diseñadores
entienden su medio de forma diferentes, crean films y secuencias que también se ven
diferentes de lo que existía en el siglo XX).
Pensemos en las nuevas formas de crear imágenes en movimiento (a las que me he
referido como nuevos paradigmas) que hemos visto hasta ahora. (Aunque teóricamente no
son todas compatibles entre ellas, en la práctica de la producción estos paradigmas son
usados de forma complementaria). Una “imagen en movimiento” se convierte en un
híbrido que puede combinar todos las medios visuales inventados a la fecha, en lugar de
soportar sólo un tipo de dato, como la grabación con cámara, dibujo a mano, etc. En lugar
de ser visto como un cuadro plano singular (resultado de la luz que entra por el lente y
capturada en la superficie de grabación), es entendido como un apilamiento de
potencialmente infinitas capas. Y en lugar de estar “basado en el tiempo”, pasa a ser
“basado en la composición” u “orientado objetos”. Esto es, en lugar de ser tratado como
una secuencia de cuadros organizados en el tiempo, una “imagen en movimiento” se ve
como una composición bidimensional que consiste en un número de objetos que pueden
ser manipulados independientemente. Alternativamente, si un diseñador usa la
composición 3D, el giro conceptual es aún más dramático: en lugar de editar “imágenes”,

257
ahora trabaja en un espacio virtual tridimensional que alberga CGI e imágenes planas
grabadas con lente.
Por supuesto, la representación basada en fotogramas no desapareció, simplemente se
volvió un formato de grabación y de salida, en lugar del espacio en donde un film se
ensamblaba. Y mientras el término “imagen en movimiento” puede seguir siendo usado
como una descripción apropiada de cómo el resultado de un proceso de producción se
vive por los espectadores, no mantiene la esencia de cómo los diseñadores piensan sobre
lo que crean. Debido al ambiente de producción (flujo de trabajo, interfaces y
herramientas) ha cambiado tanto, ahora piensan muy diferentes de cómo se hacía hace
veinte años.
Si nos enfocamos en lo que estos paradigmas tienen en común, podemos decir que los
cineastas, editores, artistas de efectos especiales, animadores y diseñadores de gráficos
animados, trabajan en una composición 2D o en un espacio 3D que consiste de cierto
número de objetos separados. La dimensión espacial se ha vuelto tan importante como la
dimensión temporal. Del concepto de “imagen en movimiento”, visto como una secuencia
de fotografías fijas, nos hemos movido a un nuevo concepto: una composición modular de
medios. Mientras a una persona que dirige una película o un cortometraje centrado en
actores y acción viva se le puede seguir llamando “director de cine” o “cineasta”, en todos
los demás casos en donde la producción sucede en ambientes de software, es más
apropiado llamar a la persona “diseñador”. Este es otro cambio fundamental en el
concepto de “imágenes en movimiento”: hoy es más común que sean “diseñadas” en lugar
de ser “capturadas”, “dirigidas” o “animadas”.
Importar-exportar: flujo de trabajo del diseño
En nuestras discusiones sobre la interfaz y flujo de trabajo de After Effects, así como del
nuevo paradigma de la composición 3D, nos hemos encontrado con el aspecto crucial del
proceso de producción de medios basado en software. Hasta la llegada de las
herramientas software en los 90’s, la combinación de diferentes tipos de medios
temporales tomaba mucho tiempo, era cara y, en algunos casos, simplemente imposible.

258
Herramientas como After Effects cambiaron esta situación de forma fundamental. Ahora,
un diseñador puede importar diferentes medios en su composición con unos cuanto clics
del mouse.
Sin embargo, el diseño contemporáneo de imágenes en movimiento basado en software (o
cualquier otro proceso de diseño) no sólo implica combinar elementos de diferentes
fuentes en una sola aplicación. En esta sección veremos el flujo de trabajo general, típico
del diseño contemporáneo (ya sean imágenes en movimiento, ilustraciones fijas, objetos y
escenas 3D, arquitectura, música, páginas web o cualquier otro medio). (Muchos de los
detalles de la producción de imágenes en movimiento con software aplican también al
diseño gráfico. No obstante, en este sección queremos hacer esto explícito. Por
consecuencia, los ejemplos que siguen han sido tomados de diferentes campos).
A pesar de que los comandos “importar/exportar” se encuentran en la mayoría de las
aplicaciones para la producción y edición de medios basados en GUI, a primera vista no
parecen muy importantes para entender la cultura del software. Cuando “importamos”, no
creamos o modificamos objetos de medios, ni tampoco estamos accediendo a información
conectada, como en la navegación Web. Todos lo que estos comandos permiten hacer es
mover datos entre diferentes aplicaciones. En otras palabras, hacen que los datos creados
en una aplicación sean compatibles con otras. Y esto no parece muy importante.
Pero pensemos dos veces. ¿De dónde viene la mayor parte de la economía en el área de
Los Ángeles? No es del entretenimiento. Desde la producción de cine hasta los museos y
todos lo que está en medio, el entretenimiento sólo representa el 15 por ciento. Resulta
que la mayor parte de la economía viene del negocio de la importación/exportación (más
del 60 por ciento). En términos más generales, una de las características de la
globalización que se evoca a menudo es su conectividad (de lugares, sistemas, países,
organizaciones., etc.). Y la conectividad sólo sucede si se tiene cierto nivel de
compatibilidad entre los códigos y procedimientos de los negocios, entre las tecnologías de
distribución, entre los protocolos de red, entre los formatos de archivos computacionales, y
demás.

259
Hagamos un acercamiento a los comandos de importar/exportar. Como trataré mostrar
adelante, estos comandos juegan un rol crucial en la cultura del software, y en particular
del diseño de medios (sin importar el tipo de proyecto en que el diseñador está
trabajando).
Antes de adoptar herramientas de software en los 90’s, cineastas, diseñadores gráficos y
animadores usaban tecnologías completamente diferentes. En consecuencia, por mucho
que se hayan influenciado entre ellos y que compartan la misma sensibilidad estética,
inevitablemente crearon imágenes que se veían diferentes. Los cineastas usaban cámaras
y otras tecnologías diseñadas para capturar la realidad física tridimensional. Los
diseñadores gráficos trabajaban con impresión offset y litografía. Los animadores
trabajaban con sus propias tecnologías: acetatos transparentes y escritorios de animación
con una cámara capaz de hacer exposiciones cuadro por cuadro mientras el animador iba
cambiando sus acetatos o movía el fondo.
Como resultado, el cine, el diseño gráfico y la animación del siglo XX (estoy haciendo
referencia a las técnicas de animación estándar usadas por la mayoría de los estudios
comerciales) desarrollaron diferentes lenguajes y vocabularios artísticos, tanto en forma
como en contenido. Por ejemplo, los diseñadores trabajaban en un espacio 2D, los
directores de cine arreglaban sus composiciones en un espacio 3D y los animadores de
acetatos en “dos y media” dimensiones. Esto aplica para la mayoría de trabajos hechos en
cada uno de esos campos, aunque claro, ha habido excepciones. Por ejemplo, Oskar
Fischinger hizo un film abstracto que consistía únicamente en objetos geométricos
moviéndose en un espacio vacío, pero hasta donde sé, éste es el único film en la historia
de la animación pre-digital abstracta que tiene lugar en un espacio tridimensional.
Las diferencias tecnológicas influyeron en el tipo de contenido que aparecería en los
diferentes medios. El cine mostraba imágenes de naturaleza “foto-real”, escenarios
construidos y formas humanas articuladas en una iluminación especial. Los diseños
gráficos tenían tipografía, elementos gráficos abstractos, fondos monocromáticos y
recortes de fotografías. Y las caricaturas presentaban personajes planos dibujados a mano
y objetos animados (más detallados) por debajo, que funcionaban como fondos. Las
excepciones fueron raras. Por ejemplo, los espacios arquitectónicos aparecían

260
comúnmente en películas porque los directores podían explorar sus tridimensionalidad en
escenas, pero prácticamente no aparecieron en cine de animación a detalles hasta que los
estudios empezaron a usar animación 3D por computadora.
¿Por qué era tan difícil cruzar las fronteras? En teoría, uno podría imaginarse hacer una
película animada de la siguiente manera: imprimir una serie de diseños gráficos levemente
diferentes y después filmarlos como si fueran una secuencia de acetatos animados. O una
película donde un diseñador dibujara una serie a mano usando exactamente el mismo
vocabulario del diseño gráfico y después lo filmara uno por uno. Y por lo tanto, hasta
donde yo sé, nunca se hizo alguna película semejante. Lo que encontramos más bien son
muchas películas animadas abstractas que tienen cierta conexión con el estilo de las
pinturas abstractas. Por ejemplo, las animaciones y las pinturas de Oskar Fischinger
comparten ciertas formas. También se encuentran ejemplos de cine abstracto,
comerciales y secuencias de créditos que tienen conexión con la estética popular del
diseño gráfico de aquel tiempo. Entre ellos, podemos recordar algunas secuencias de
créditos con gráficos animados diseñados por Pablo Ferro en los 60’s que muestran la
estética psicodélica que se ve en carteles, portada de discos y demás producciones de la
época.
Con todo y estas conexiones, las diferentes producciones nunca usaron el mismo lenguaje
visual. Una razón es que el film proyectado no podía mostrar adecuadamente las
diferencias sutiles entre tipos de letra, entrelineados y escalas de tonos de grises,
cruciales para el diseño gráfico moderno. Por consecuencia, cuando los artistas trabajaban
en cine de arte abstracto o comerciales que adoptaban la estética del diseño gráfico
(muchos de los animadores abstractos del siglo XX trabajaban también en sus proyectos
personales), no podían simplemente expandir el lenguaje de la página impresa a la
dimensión del tiempo. Tenían, en esencia, que inventar un nuevo lenguaje visual paralelo
que usara contrastes acentuados, formas legibles más fácilmente y líneas más gruesas
(que, por su grosor, ya no eran líneas sino formas, de hecho).
A pesar de que las carencias en resolución y contraste de la imagen cinematográfica y
televisiva, en comparación con una página impresa, contribuyeron al distanciamiento entre
los lenguajes usados por los cineastas y los diseñadores gráficos durante gran parte del

261
siglo XX, no creo que éste haya sido el factor decisivo. Hoy en día, la resolución, contraste y
reproducción de color también son sustancialmente diferentes entre impresoras y
pantallas (computadora, TV y aparatos móviles), y aún así vemos comúnmente las mismas
estrategias visuales a través de los diferentes medios. Para acabar de convencernos,
basta echar una hojeada a cualquier libro o revista contemporánea de diseño 2D (es decir,
diseño gráfico para impresos, difusión y web). Cuando vemos páginas que muestran el
trabajo particular de un diseñador o de un estudio, en muchos casos es imposible
identificar el origen de las imágenes, a menos que leamos las leyendas. Sólo así podemos
saber cuál imagen es un póster, cuál es una captura de pantallas de un video musical y
cuál es el contenido editorial de una revista.
A continuación, usaré para mis ejemplos el libro Graphic Design for the 21st Century: 100
of the World’s Best Graphic Designers (Taschen, 2001) debido a que, para el año de su
publicación, los cambios de los que hablo ya habían sucedido. El diseño de Peter Anderson
que muestra una línea de texto en contra de una nube con cientos de pequeñas letras
orientadas de diferentes formas, resulta ser una imagen de la secuencia de créditos de un
documental de Channel 4. Su otro diseño, que de forma similar juega con el contraste
entre letras saltarinas de fuente grande en contra de planos irregulares hechos de densos
estrechamientos de letras de menor tamaño, resulta ser un desplegable de la revista IT
Magazine. Como el primer diseño fue hecho para la teledifusión y el segundo para los
impresos, se podría esperar que el primero usara formas más gruesas. Sin embargo,
ambos diseños usan la misma escala entre letras pequeñas y grandes, y muestran
texturas hechas de cientos de palabras tan pequeñas que su intención no es que sean
legibles. Una páginas más adelante vemos un diseño de Philippe Apeloig que usa
exactamente la misma técnica y estética que Anderson. En este caso, pequeñas líneas de
texto puestas en diferentes ángulos forman un modelo 3D que flota en el espacio. En la
siguiente página, otro diseño de Apeloig crea un campo en perspectiva, no hecho de letras
sino de cientos de formas abstractas idénticas.
Estos diseños se basan en la habilidad del software (o del diseñador siendo influenciado
por el uso del software y recreando con el software lo que antes hacía manualmente) de
tratar al texto como cualquier otro primitivo gráfico y de fácilmente crear composiciones
hechas de cientos de elementos similares o idénticos acomodados según un patrón. Y

262
como un algoritmo puede fácilmente modificar cada elemento del patrón, cambiando su
posición, tamaño, color, etc., en lugar de las cuadrículas modernitas regulares, vemos
estructuras más complejas que surgen de variaciones de un mismo elemento. Esta
estrategia es explorada de forma particularmente inventiva en los diseños de Zaha Hadid,
como el “Icone bag” para Louis Vuitton, en 2006, y sus planos urbanos para Singapur y
Turquía, que usan lo que ella llama una “cuadrícula variable”.
A cada diseñador en el libro se le pidió dar un breve explicación para acompañar su trabajo
de portafolio. Lust Studio eligió la frase “La forma viene después del proceso” como su
lema. Entonces, ¿cuál es la naturaleza del proceso de diseño en la era del software y cómo
influye en las formas que vemos actualmente entre nosotros?
Si actualmente estás involucrado de forma práctica en el diseño y arte, sabrás que los
diseñadores contemporáneos usan el mismo software para diseñar casi cualquier cosa. Ya
hemos mencionado estas herramientas: Photoshop, Illustrator, Flash, Maya, etc, Sin
embargo, el factor crucial no son las herramientas en sí, sino el proceso del flujo de
trabajo, permitido por las operaciones “importar” y “exportar” y métodos relacionados
(“colocar”, “insertar”, “suscribir”, “objeto dinámico”, etc.) que permiten la coordinación
entre estas herramientas.
Cuando se arma un proyecto de medios, el software usado en la fase final depende del
tipo de medio de salida y de la naturaleza del proyecto: After Effects para los proyectos de
gráficos animados y composición de video, Illustrator para ilustraciones en impresos,
InDesign para diseños multi-página, Flash para interfaces interactivas y animaciones web,
3ds Max o Maya para modelos y animaciones 3D, etc. Pero estos programas rara vez se
usan solos para crear un diseño de medios de principio a fin. Típicamente, un diseñador
puede crear elementos en un programa, importarlos en otro, después añadir elementos
creados en otro programa, etc. Esto sucede sin importar si el producto final será una
ilustración, un sitio web, un motion graphic, un matte painting, o si es una imagen fija,
animada o interactiva.
Los nombres mismos que las compañías de software asignan a sus productos hacen
referencia a sus características esenciales en el proceso de diseño basado en software.

263
Desde 2005, Adobe ha vendido sus diferentes aplicaciones de producción de medios bajo
el nombre general “Adobe Creative Suite” o conjunto creativo. Entre las frases o subtítulos
que han acompañado esta marca, hay uno en particular que es significativo para nuestra
discusión: “Diseña a través de los medios”. Esta frase describe de forma precisa tanto las
capacidades de las aplicaciones reunidas en la suite y su uso en el mundo real. Cada una
de las aplicaciones clave tienen propiedades orientadas a la producción del diseño de un
medio en particular. Illustrator está programado para trabajar con impresoras de alta
calidad; After Effects y Premiere envían video en varios formatos estándar, como HDTV;
Dreamweaver soporta lenguajes de programación para la creación de sitios web dinámicos
y sofisticados. Pero a pesar de que un proyecto sea terminado en una de estas
aplicaciones, muchas de las demás aplicaciones de la suite se usan en el proceso de
creación o edición de varios de sus elementos. Ésta es una de las formas en que Adobe
permite “diseñar a través de medios”. La compatibilidad entre aplicaciones también
significa que los elementos (llamados en el lenguaje profesional “recursos”) pueden ser
reusados más tarde en otros proyectos. Por ejemplo, una fotografía editada en Photoshop
poder ser usada primero en un anuncio de revista, después puesta en un video, un sitio
web, etc. O los modelos y personajes 3D creados para una película pueden ser reusado
para un videojuego basado en la misma. Esta habilidad de reusar un mismo diseño, o
parte de él, en diferentes proyectos es importante debido a la práctica generalizada en las
industrias creativas de crear productos, a través de diferentes medios, con las mismas
imágenes, diseños, personajes y narrativas. Una campaña de publicidad seguido funciona
“a través de los medios”, incluyendo banners web, spots TV, anuncios de revistas,
carteleras, etc. Y si convertir películas en juegos y juegos en películas ya ha sido popular
en Hollywood desde inicios de los 90’s, una nueva tendencia, desde mediados del 2000’s,
es crear juegos, películas, sitios web, y demás productos, al mismo tiempo (y que los
productos usen los mismos recursos, tanto por razones económicas como estéticas, para
asegurar el mismo estilo y continuidad). Por ende, un estudio puede crear fondos y
personajes 3D y usarlos en una película y en un juego, ambos lanzados simultáneamente.
Si las aplicaciones de los medios no fueran compatibles, dicha práctica simplemente no
sería posible.
Todos estos ejemplos ilustran el re-uso intencional de los elementos del diseño “a través
de medios”. Sin embargo, la compatibilidad entre aplicaciones de producción de medios

264
también tiene un efecto muchos más amplio y menos intencional en la estética
contemporánea. Con el flujo de trabajo de la producción que hemos descrito, podríamos
esperar que las mismas técnicas y estrategias visuales aparecieran también en los demás
tipos de proyectos de medios diseñados con software, sin que esto haya sido
deliberadamente planeado. También podríamos esperar que esto sucediera en un nivel
más básico. Y éste es el caso. Las mismas estrategias de diseño permitidas por el
software, las mismas técnicas basadas en software y la misma iconografía generada con
software se hallan ahora en todos los tipos de medios, todas las escalas y todos los tipos
de proyectos.
Ya hemos encontrado algunos ejemplos concretos. Por ejemplos, los diseños de Peter
Anderson y Philippe Apeloig, hechos para diferentes medios, usan la misma técnica básica
de gráficos computacionales: generación automática de un patrón repetitivo y variación de
parámetros que controlan la apariencia de cada elemento (posición, tamaño, orientación,
curvatura, etc.). (El principio general detrás de esta técnica también puede ser usado para
generar modelos 3D, animaciones, texturas, plantas y paisajes, etc.). Se le conoce
comúnmente como “diseño paramétrico” o “modelación paramétrica”). La misma técnica
es usada por el estudio de Hadid en el “Icone bag” de Louis Vuitton. En otro ejemplo, que
veremos abajo, Greg Lynn usó técnicas de sistemas de partículas (que en su momento se
usaban para simular fuego, nieve, cascadas y demás fenómenos naturales para cine) para
generar la formas de un edificio.
Para usar una metáfora biológica, podemos decir que la compatibilidad entre aplicaciones
crear condiciones favorables para la propagación del DNA de los medios entre las
especies, familias y clases. Y esta propagación sucede en todos los niveles: en todo el
diseño, en partes del diseño, en los elementos que lo conforman y en los “átomos” que
constituyen los elementos. Consideremos el siguientes caso hipotético de propagación a
un nivel bajo. Un diseñador puede usar Illustrator para crear una curva suavizada 2D
(llamada en gráficas computacionales como “spline”). Esta curva se puede convertir en un
componente básico para usarse en cualquier proyecto. Puede formar parte de una
ilustración o de un libro de diseño. Puede ser importada en un programa de animación en
donde se animará o en un programa 3D en donde será extruida y formará un objeto sólido.

265
Con el tiempo, la manufactura de software desarrollo nuevas formas de conectar sus
aplicaciones, con el fin de facilitar la movilidad de elementos entre ellas. Con los años, se
volvió posible mover proyectos completos entre aplicaciones sin ninguna pérdida (o casi).
Por ejemplo, al describir la integración entre Illustrator y Photoshop CS3, el sitio web de
Adobe señala que un diseñador puede “preservar capas, composiciones, transparencias y
archivos modificables”257. Otro desarrollo importante ha sido el concepto que Microsoft
llama “objetos vinculados”. Si vinculamos dos archivos, o parte de ellos (por ejemplo hoja
de cálculo Excel a una presentación PowerPoint), cada vez que la información cambia en el
primero, ésta se actualiza en el segundo automáticamente. Muchas aplicaciones de
medios implementan esta función. Para usar el mismo ejemplo de Illustrator CS3, un
diseñador puede “importar archivos de Illustrator en Premiere y, después, con el comando
Editar Original abrir la imagen en Illustrator, editarla y ver cómo los cambios se incorporan
en el proyecto de video”258.
Cada tipo de programa usado por los diseñadores de medios (gráficos 3D, dibujo vectorial,
edición de imágenes, animación y composición) sobresale en una función particular. Por
ejemplo, en formas particulares de creación o modificación de elementos gráficos. Estas
operaciones pueden compararse a los diferentes tipos de piezas de un juego LEGO.
Podemos crear un número infinito de proyectos usando un número limitado de piezas.
Según el proyecto, los bloques tendrán diferentes funciones y aparecerán en diferentes
combinaciones. Por ejemplo, un bloque rectangular puede ser la parte superior de una
mesa o el cuerpo de un robot, etc.
Un flujo de trabajo de diseño, basado en cierto número de programas compatibles, opera
de la misma forma… con una diferencia importante. Los componentes básicos usados en
el diseño contemporáneo no son únicamente diferentes tipos de elementos visuales
(patrones vectoriales, objetos 3D, sistemas de partículas, etc.) sino también diferentes
formas de modificarlos (extrusión, sesgar, vectorizar, modificar transparencia, desenfocar,
etc.). Esta diferencia es crucial. Si el software de creación y producción de medios ni
incluyera estas operaciones de modificación, hoy veríamos un lenguaje visual
257 http://www.adobe.com/products/illustrator/features/allfeatures/ (Agosto 30, 2008).
258 Ibid.

266
completamente diferente. Veríamos “multi-medios”, es decir, diseños que simplemente
combinan elementos de diferentes medios. Pero lo que tenemos es una “remezcla
profunda”, las interacciones “profundas” entre métodos y técnicas de diferentes medios
en un mismo proyecto.
En un uso “cruzado”, las técnicas que antes eran específicas a un medio, ahora se aplican
a otro tipo de medios. Esto se logra comúnmente en una sola aplicación, por ejemplo,
aplicar el filtro de desenfoque en After Effects a una composición que puede tener
elementos gráficos, video, objetos 3D, etc. El hecho de poder mover un proyecto completo,
o parte de sus elementos, entre diferentes aplicaciones abre más posibilidades debido a
que cada aplicación ofrece sus técnicas únicas. A medida que los datos de medios viajan
de una aplicación a otra, se transforman y enriquecen con las operaciones que brinda
cada aplicación.
El flujo de trabajo específico de la era del software que acabamos de describir tiene dos
consecuencias principales. Primero, la estética visual de la hibridación que domina el
universo del diseño contemporáneo. Segundo, el uso de las mismas técnicas y estrategias
a través de este universo, sin importar el formato final del medio o tipo de proyecto.
Como ya lo hemos dicho más de una vez, un diseño típico de hoy en día combina técnicas
de diferentes medios. Ahora estamos en una mejor posición para entender porqué pasa
esto. Cuando un diseñador trabaja en un proyecto, lo hace combinando el resultado de
funciones específicas de diferentes programas de software que originalmente fueron
creados para imitar el trabajo de diferentes medios físicos. (Illustrator fue hecho para crear
ilustraciones, Photoshop para editar fotografías digitalizadas, Premiere para editar video,
etc.). Mientras estas funciones siguen siendo usadas en relación con sus medios
originales, la mayoría de ellos también son usadas como parte del flujo de trabajo de
cualquier proyecto de diseño.
La condición esencial que hace posible la lógica del nuevo diseño y su estética resultante
es la compatibilidad entre archivos generados por diferentes programas. En otras
palabras, “importar”, “exportar” y demás funciones y comandos relacionados con software
de gráficos, animación, edición de video, composición y modelación, son históricamente

267
más importantes que las operaciones individuales que estos programas ofrecen. La
habilidad de combinar capas bitmap y vectoriales en una misma imagen, ubicar elementos
3D en una composición 2D y viceversa, y demás ejemplos, es lo que permite el flujo de
trabajo de la producción y el re-uso de las mismas técnicas, efectos e iconografía a través
de diferentes medios.
Las consecuencias de esta compatibilidad entre software y formatos de archivo, que se
logró gradualmente durante los 90’s, son difíciles de sobreestimar. Además de la
hibridación de la estética visual moderna y la reaparición de exactamente la misma
técnicas de diseño a través de los todos los productos de los medios, también hay otros
efectos. Por ejemplo, el campo entero de los motion graphics, tal como existe hoy, se volvió
popular gracias a la integración de software que integra gráficos vectoriales (como
Illustrator) con software de animación y composición (como After Effects). Un diseñador
define comúnmente varios elementos de la composición en Illustrator y después los
importa en After Effects para animarlos. Esta compatibilidad no existía en las primeras
versiones del software de producción y creación de medios en los 80’s. Se fue añadiendo
gradualmente con nuevos lanzamientos. Pero cuando se logró a mediados de los 90’s259,
en unos cuantos años el todo el lenguaje del diseño gráfico contemporáneo se importó al
área de la imagen en movimiento, tanto literal como metafóricamente.
En resumen, esta compatibilidad entre software de diseño gráfico, ilustración, animación,
edición de video, modelación y animación 3D, y efectos visuales, juega un rol clave en la
conformación de formas visuales y espaciales de la era del software. Por un lado, nunca
antes habíamos visto una variedad de formas tan amplia como hoy. Por otro lado,
exactamente las mismas técnicas, composiciones e iconografía aparecen en cualquier
medio.
Forma variable
259 En 1995, After Effects 3.0 permitió importar archivos de Illustrator y Photoshop como
composiciones: http://en.wikipedia.org/wiki/Adobe_After_Effects

268
Como lo muestran las películas de Blake y Murata analizadas precedentemente, en los
motion graphics contemporáneos las transformaciones afectan comúnmente el fotograma
completo. En contraste con la animación del siglo XX, todo lo que se halla dentro del
fotograma cambia continuamente: elementos visuales, su transparencia, la textura de una
imagen, etc. De hecho, si algo queda inmóvil por un rato, esto es una excepción en lugar
de la regla.
Este cambio constante, producido en muchas dimensiones visuales, es otra característica
clave de los motion graphics y del cine de diseño producido actualmente. Así como lo
hicimos en el caso de la hibridación de medios, es posible vincular esta preferencia por el
cambio constante con las particularidades del software usado en el diseño de medios.
Las computadoras digitales permiten representar cualquier fenómeno o estructura como
un conjunto de variables. En el caso del software de diseño y de animación, esto significa
que todas la formas posibles (visuales, temporales, espaciales, interactivas), son
igualmente representadas como un conjunto de variables que pueden cambiar
continuamente. Esta nueva lógica de la forma está profundamente codificada en las
interfaces de los programas y las herramientas que ofrecen.
Veamos nuevamente la interfaz de After Effects. Para crear una animación, un diseñador
añade algunos elementos a la composición y los anima. Cada elemento se muestra en la
interfaz como una lista de parámetros, cada uno con su canal de animación. Dependiendo
del tipo de elemento, los parámetros pueden ir de unos a una decena o más. Por ejemplo,
para las luces, sus parámetros son: intensidad, color, oscuridad de la sombra y difusión de
la sombra. Si el elemento es una cámara, los parámetros incluyen: punto de interés,
posición, orientación y rotación. Para las formas, la lista de parámetros es particularmente
larga: posición, escala, orientación, opacidad, propiedades de materiales incluyendo
proyección de sombras, transmisión de luz, ambiente, difusión y cualidades especulares,
brillo y demás.
Animar cualquier parámetro sólo toma unos cuantos clics. El proceso funciona igual sin
importar elemento o tipo de parámetro. Entonces, es igualmente fácil cambiar en el tiempo
la posición de una forma, su color o la intensidad de una luz.

269
En contraste con la animación del siglo XX, After Effects no favorece el movimiento de
objetos o personajes bidimensionales. Por consecuencia, la Ayuda de After Effects define
“animación” como “cambio en el tiempo”, sin especificar cuál cambio260. La respuesta
está dada por la interfaz misma. Su diseño sugiere que animemos cientos de
características visuales (y si consideramos que la mayoría de los filtros también pueden
ser animados, la lista llega a miles).
Debido a que la interfaz del software hace evidente cada parámetro de cada objeto en la
composición, asignando a cada uno su propio canal en la línea de tiempo, literalmente
invita al diseñador a animarlos. Somos invitados a empezar a mover y rotar objetos,
cambiar su opacidad, colores, etc. La misma lógica sucede en la cámara y las luces. Si
añadimos luces a la composición, éstas añaden automáticamente media docena de
nuevos canales de animación que se refieren a su color, posición, orientación, intensidad y
propiedades de las sombras. (Otros software de composición basados en capas y de
animación 2D y 3D comparten los mismos principios generales de la interfaz)261.
Durante los 80’s y 90’s, la lógica general de la representación por computadora (es decir,
representar todo como variables que pueden tener diferentes valores) estuvo
sistemáticamente incrustada en las interfaces del software de diseño de medios. Como
resultado, aunque algunas aplicaciones particulares no prescriban a sus usuarios lo que
pueden o no pueden hacer, la estructura de una interfaz influye profundamente la manera
de pensar del diseñador. En el caso del diseño de imágenes en movimiento, el resultado
de tener una interfaz de línea de tiempo con múltiples capas listas para ser animadas es
que el diseñador realmente las anima. Si las restricciones que antes había en la tecnología
de animación (desde los primeros juguetes ópticos hasta los sistemas de animación por
acetato) resultaron en una estética de cambios discretos y temporalmente limitados, las
260 http://help.adobe.com/en_US/AfterEffects/9.0/ (Noviembre 7, 2012).
261 El software de composición basado en nodos usa un principio de interfaz diferente: cada
elemento de la composición es representado como un nodo de un grafo. Esta interfaz es
particularmente útil para crear escenas que contienen un gran número de elementos interactuando
entre ellos. Algunas ejemplos comerciales son: Fusion y Nuke.

270
interfaces del software de animación por computadora rápidamente han llevado a una
nueva estética: las transformaciones continuas de la mayoría (o todos) los elementos
visuales que aparecen en un cuadro.
Este cambio en la estética de la animación, que deriva del diseño de la interfaz del
software de animación, fue paralelo al cambio en otro campo: la arquitectura. A mediados
de los 90’s, cuando los arquitectos empezaron a usar software desarrollado originalmente
para la animación por computadora y los efectos especiales (primero Alias y Wavefront,
después Maya y otros), la idea de formas animada entró en la inspiración arquitectónica.
Si el software de animación 2D, como After Effects, permite al animador modificar
cualquier parámetro de un objeto 2D (un video clip, una forma 2D, texto, etc.) en el tiempo,
la animación 3D por computadora permite lo mismo para cualquier forma 3D. Un animador
puede crear fotogramas clave manualmente y dejar que la computadora calcule cómo
cambia la forma en el tiempo. Alternativamente, puede dirigir algoritmos que no solo
modificarán la forma, sino que también generarán nuevas formas. (entre dichas
herramientas vemos sistemas de partículas, simulación física, animación comportamental,
evolución artificial, sistemas-L, etc.). Trabajar con software de animación 3D afectó la
imaginación arquitectónica literal y metafóricamente. Las formas que aparecían en los
proyectos de jóvenes arquitectos y estudiantes de arquitectura en la segunda parte de los
90’s parecían que estaban en proceso de ser animados, capturas de la transformación de
un estado a otro. Las presentaciones de proyectos e investigaciones empezaron a incluir
múltiples variaciones de una misma forma generada mediante parámetros de software.
Finalmente, en proyectos como New York Port Authority Gateway de Gregg Lynn (1995)262,
las trayectorias de los objetos de una animación fueron literalmente convertidos en un
diseño arquitectónico. Usando un sistema de partículas (parte del software de animación
Wavefront), que genera un nube de puntos y los mueve en el espacio según se desee, Lynn
capturó esos momentos y los convirtió en curvas que sirvieron de base para su
construcción.
Igualmente crucial fue la exposición de los arquitectos a la nueva generación de
herramientas de modelación en el software comercial de animación de los 90’s. Durante
262 Gregg Lynn, Animate Form (Princeton Architectural Press, 1999), pp. 102–19.

271
dos décadas, la principal técnica de modelado 3D era la de representar un objeto como
una colección de polígonos planos. Pero a mediados de los 90’s, el incremento de
velocidad y almacenamiento de las computadoras hizo posible otra técnica: modelado
basado en curvas suavizadas (splines). Esta nueva técnica para representar la forma
sacudió el pensamiento arquitectónico, llevándolo de la geometría modernista rectangular
hacia el favorecimiento de formas complejas y suaves hechas de curvas continuas. Como
resultado, desde la segunda parte de los 90’s, la estética de los “gotas” se ha vuelto
dominante en la inspiración de estudiantes, jóvenes arquitectos y “estrellas” establecidas
como Zaha Hadid, Eric Moss y UN Studio.
Pero ésta no fue la única consecuencia del cambio: de las herramientas estándar de
arquitectura y software de diseño asistido por computadora (como AutoCAD) hacia el
software de animación y efectos especiales. Tradicionalmente, los arquitectos creaban
nuevos proyectos con base en la topología existente. Una iglesia, una residencia privada,
una estación de tren… todas tenías tipos bien conocidos: plantillas espaciales que
determinaban cómo se debía organizar el espacio. De forma similar, cuando se diseñaban
los detalles de un proyecto, un arquitectos seleccionaba de varios elementos estándar con
funciones y formas establecidas: columnas, puertas, ventanas, etc263. En las viviendas del
siglo XX producidas masivamente, sólo más tarde se adoptó esta lógica que
eventualmente se codificó en las interfaces del software CAD.
Pero cuando a inicios de los 90’s, Greg Lynn, la firma Asymptote, Lars Spuybroek y demás
jóvenes arquitectos empezaron a usar software 3D que era usado por otras industrias, se
dieron cuenta que esos software no venían con ninguna plantilla arquitectónica estándar.
Además, si el software CAD para arquitectos ve las formas rectangulares como los
componentes básicos de una estructura, el software de animación 3D no tenía dichos
supuestos. Más bien, ofrecía curvas y superficies suavizadas, y formas creadas con dichas
curvas (que eran apropiadas para la creación y animación de personajes de juegos y
productos industriales). (De hecho, las splines fueron introducidas originalmente en las
263 Agradezco a Lars Spuybroek, director de Nox, por haberme explicado cómo el uso del software
para el diseño arquitectónico subvirtió el pensamiento tradicional basado en tipologías.

272
gráficas computacionales, en 1962, por Pierre Bézier, para su uso en el diseño de
automóviles asistido por computadora).
Como resultado, en lugar de ser entendido como una composición hecha de plantillas de
partes estándar, un edificio se pudo imaginar como una forma curveada continua que
puede variar infinitamente. También se pudo imaginar como varias formas continuas
interactuando entre ellas. En cualquier caso, la forma de cada una de esas formas no
estaba determinada a priori por ningún tipo tipología.
(En retrospectiva, podemos pensar este “mal-uso” altamente productivo del software de
animación y modelación 3D por los arquitectos como otro caso de hibridación de medios,
en particular a lo que me he referido como efecto “de cruzada”. En este caso, es un cruce
entre las herramientas y convenciones de un campo de diseño (animación de personajes y
efectos especiales) y las formas de pensar y conocimientos de otro campo (aquí, la
arquitectura).
Para relacionar esta discusión sobre la arquitectura con el principal tema de este capítulo
(la producción de imágenes en movimiento), podemos ver que ambos campos fueron
afectados por la informatización en una forma estructuralmente similar. En el caso de la
animación comercial del siglo XX, todos los cambios temporales dentro de un cuadro está
limitado, eran discretos y usualmente de orientación semántica (es decir, conectados a la
narrativa). Cuando un personaje se movía, caminaba dentro de un cuadro, volteaba su
cabeza o extendía su brazo, esto era usado para desarrollar la historia264. Después del
cambio a un proceso de producción basado en software, las imágenes en movimiento
mostraban transformaciones constantes en muchas dimensiones visuales que no ya
estaban limitadas por la semántica. Tal como es definida en numerosas secuencias de
gráficos animados y cortometrajes de los 2000’s, las formas visuales contemporáneas
cambian constantemente, pulsan y mutan más allá de la necesidad de comunicar
264 En el caso de la animación narrativa producida en Rusia, Europa del Este y Japón, los cambios
visuales en una secuencia no siempre estaban guiados por el desarrollo de una narrativa y podían
servir otros propósitos: crear una atmósfera, representar un estado emocional o simplemente usado
estéticamente por sí mismo.

273
significados y narrativas. (Las películas de Blake y Murata son ejemplos claros de esta
nueva estética de la forma variable; muchos otros ejemplos se pueden hallar fácilmente
navegando sitios web que exhiben trabajos de estudios y diseñadores independientes).
Un proceso paralelo tomó lugar en el diseño arquitectural. Las diferencias en una forma
arquitectónica tradicional estaba conectadas con la necesidad de comunicar significados
y/o de cumplir con un programa arquitectural. Una apertura en un muro era una puerta o
una ventana; un muro era una frontera entre diferentes espacios funcionales. Así, tal como
en la animación, los cambios en la forma estaban limitados y eran guiados por la
semántica. Pero hoy, la forma arquitectónica diseñada y con software de modelación
puede cambiar su geometría a través del diseño completo, y estos cambios ya no tienen
que estar justificados por una función.
La terminal aérea del Yokohama International Port (2002), diseñada por Foreign Office
Architects, ilustra claramente esta estética de la forma variable en la arquitectura. El
edificio es un volumen espacial complejo y continuo, sin un solo ángulo recto y sin
fronteras que dividan la forma en partes o que la separen del plano del suelo. Cuando
visité el edificio en diciembre de 2003, pasé cuatro horas explorando las continuidades
entre sus espacios exteriores e interiores, disfrutando el cambio constante de sus
superficies curveadas. El edificio puede ser comparado con un cinta de Möbius, excepto
que es mucho más compleja, menos simétrica y más impredecible. Sería más apropiado
pensar en esta estructura como un conjunto de cintas Möbius interconectadas
suavemente.
Para resumir esta discusión de cómo el cambio a una representación basada en software
afectó el lenguaje moderno de la forma: todas las contantes fueron reemplazadas por
variables, cuyos valores pueden variar continuamente. Como resultado, la cultura entró en
lo que llamamos el giro de la continuidad. Tanto la forma temporal-visual de los motion
graphics y el cine de diseño como la forma espacial de la arquitectura entraron al universo
del cambio constante y transformaciones. (Los campos del diseño de productos y del
diseño de espacios también fueron afectados). Antes, esta estética de “continuidad total”
sólo fue imaginada por algunos artistas. Por ejemplo, en los 50’s, el arquitecto Frederick
Kiesler diseñó un proyecto llamado Continuous House que, como su nombre lo indica, es

274
una sola y continua forma curva espacial, sin las limitantes de las divisiones habituales en
los cuartos. Pero cuando los arquitectos empezaron a trabajar con software de animación y
modelación 3D en los 90’s, dicha forma de pensar se volvió normal. De igual forma, dichas
formas continuas se volvieron eventualmente la norma en el cine.
Hoy existen muchos cortometrajes famosos y proyectos de edificios de pequeña escala
basados en la estética de la continuidad, es decir, una sola forma en constante cambio.
Pero el próximo reto para los motion graphics y la arquitectura es descubrir maneras de
usar esta estética a gran escala. ¿Cómo escalamos la idea de una simple forma en
constante cambio visual y espacial, sin cortes (para el cine) o divisiones de sus distintas
partes (para la arquitectura)?
En la arquitectura, algunos arquitectos ya han empezado a abordar exitosamente este
reto. Entre los ejemplos ya realizados contamos con la terminal Yokohama International
Port; el Kunsthaus en Graz, de Peter Cook (2004); y, el Museo Ordos, de MAD Architects en
Mongolia, China (2012). Otros en construcción son el Performing Arts Centre de Zaha
Hadid en la isla Saadiyat en Abu Dhabi, Emiratos Árabes Unidos. (Después de la crisis
económica del 2007, muchos proyectos ambiciosos en Dubái y Europa del Este fueron
retrasados o cancelados, pero China y otros países siguen tomando riesgos y adoptando el
nuevo diseño arquitectural de curvas complejas continuas).
¿Y qué hay de los motion graphics? Blake ha sido uno de los pocos aristas que han
explorado sistemáticamente cómo el lenguaje híbrido visual se puede usar en piezas más
largas. Sodium Fox dura 14 minutos; una pieza previa, Mod Lang (2001) dura 16 minutos.
Las tres películas que constituyen la trilogía Winchester (2001-2004) duran 21, 18 y 12
minutos. Ninguna de estas películas tiene un solo corte.
Sodium Fox y la Winchester Trilogy usan una variedad de fuentes visuales, incluyendo
fotografías, metraje viejo, dibujos, animación, texto e imaginería por computadora. Todos
estos medios están tejidos en un flujo continuo. Como ya lo hemos mencionado en
relación con Sodium Fox, en contraste con piezas de motion graphics más pequeñas que
muestran un frenesí de movimientos y animación, las películas de Blake tienen muy poca
animación, en su sentido tradicional. En su lugar, varias imágenes fijas y en movimiento se

275
funden gradualmente entre ellas. Así que mientras cada film se mueve a un terreno vasto
de diferentes visuales, es imposible dividir el film en unidades temporales. De hecho, aún
cuando tratáramos, no podríamos dar seguimiento a cómo llegamos a cierta imagen en
unos cuantos minutos. Y, por lo tanto, estos cambios fueron producidos por cierta lógica,
aún cuando nuestro cerebro no los asume mientras vemos el film.
La continuidad hipnótica de estas películas puede ser parcialmente explicada por el hecho
de que todos los recursos visuales en las películas fueron manipulados con software de
gráficos. Además, muchas imágenes se borran levemente. Como resultado, sin importar el
origen de las imágenes, todas adquieren cierta coherencia. Así, aunque los films juegan en
las diferencias semánticas y visuales entre acción viva, dibujo, fotografía con filtros
animados y demás medios, estas diferencias no generan yuxtaposición o montaje
estilístico265. Más bien, varios medios parecen coexistir pacíficamente, ocupando el
mismo espacio. En otras palabras, las películas de Blake parecen sugerir que el remix
profundo no es el único resultado posible de la softwareización.
Ya hemos discutido en detalle el concepto de metamedio de Alan Kay. Según la propuesta
de Kay, hecha en los 70’s, debemos ver la computadora digital como un metamedio que
contiene todos los “diferentes medios existentes y aún no inventados”266. ¿Qué implica
esto para la estética de los proyectos digitales? En mi opinión, no implica necesariamente
que los diferentes medios se fusionan, o crean un nuevo híbrido, o resultan en
“multimedios”, “intermedios”, “convergencia” o una Gesamtkunstwerk totalizante. Como lo
he argumentado, en lugar de colapsar en una sola entidad, los diferentes medios (es decir,
las diferentes técnicas, formatos de datos, fuentes de datos y métodos de trabajo),
empiezan a interactuar, produciendo gran número de híbridos o nuevas “especies de
medios”. En otras palabras, justo como la evolución biológica, la evolución de medios en la
era del software lleva a la diferenciación e incremento de diversidad (más especies en
lugar de menos).
265 En el capítulo de la “Composición” de The Language of New Media, he definido “montaje
estilístico” como “yuxtaposiciones de diversas imágenes con diversos estilos en diferentes medios”.
266 Kay & Goldberg, “Personal Dynamic Media”.

276
En el mundo dominado por híbridos, las películas de Blake son raras, al presentarnos
relativamente apariciones de medios “puros”. Podemos interpretar esto ya sea como la
lentitud del mundo del arte, que está detrás de la etapa evolutiva de los medios
profesionales (o como una estrategia inteligente de Blake para separarse del frenesí y
sobre-estímulo común de los gráficos animados). O podemos leer su estética como un
supuesto implícito contra la idea popular de “convergencia”. Como lo demuestran sus
películas, mientras medios diferentes se han vuelto compatibles, esto no significa que sus
distintas identidades han colapsado. En Sodium Fox y en la Winchester Trilogy, los
elementos visuales de diferentes medios mantienen sus características esenciales y
apariencia única.
Las realizaciones de Blake también expanden nuestro entendimiento de lo puede abarcar
la estética de la continuidad. Diferentes elementos de medios se añaden continuamente,
unos sobre otros, creando una experiencia de flujo continuo que, sin embargo, preserva
sus diferencias. La artista danesa Ann Lislegaard también pertenece a la “generación de la
continuidad”. Algunas de sus películas involucran navegación continua o una observación
de los espacios arquitectónicos imaginarios. Podemos relacionar estas películas con un
número de pintores y cineastas del siglo XX, que estaban preocupados por similares
experiencias espaciales: Giorgio de Chirico, Balthus, los surrealistas, Alain Resnais
(L'année dernière à Marienbad, 1961), Andrei Tarkovsky (Stalker, 1979). Sin embargo, la
sensibilidad de Lislegaard es, sin duda, de inicios del siglo XXI. Los espacios no chocan
entre ellos, como en L'année dernière à Marienbad, ni tampoco se vuelven misteriosos con
la introducción de figuras y objetos (una práctica de René Magritte y otros surrealistas).
Más bien, como sus contemporáneos Blake y Murata, Lislegaard nos presenta formas que
cambian continuamente frente a nuestros ojos. Nos ofrece otra versión de la estética de la
continuidad hecha posible por software como After Effects, que traduce la lógica general
de la representación por computadora (la sustitución de todas las constantes en variables)
en interfaces y herramientas concretas.
Los cambios visuales en Crystal World de Ann Lislegaard (basada en J.G. Ballard) (2006),
ocurren justo frente a nosotros, y aún así son casi imposibles de seguir. En el espacio de
un minuto, un espacio se transforma completamente en algo muy diferente.

277
Crystal World crea su propia estética de híbridos, combinando espacios foto-reales, formas
completamente abstractas y fotografías digitalizadas de plantas. (Aunque no conozco el
software que uso el asistente de Lislegaard, se trata evidentemente de un paquete de
animación 3D). Como todo está interpretado en escala de grises, las diferencias entre los
medios no son pronunciadas. Y por lo tanto ahí están. Ese este tipo de sutil, y al mismo
tiempo precisa, distinción entre diferentes medios lo que da al video su belleza única. En
contraste con el montaje del siglo XX, que creaba significado y efecto mediante
yuxtaposiciones dramáticas de semántica, composiciones, espacios y diferentes medios, la
estética de Lislegaard cabe junto con otros proyectos culturales minimalistas de principios
del siglo XXI. Hoy, los creadores de arquitectura mínima y diseño de espacio, gráficos web,
animaciones generativas e interactivas, música ambiente electrónica, asumen igualmente
que el usuario es suficientemente inteligente para entender y disfrutar distinciones sutiles
y modulaciones continuas.
Bellona, de Lislegaard, (basada en Samuel R. Delany) (2005) lleva la estética de la
continuidad en una dirección diferente. Nos movemos dentro y a través de lo que parece
un solo conjunto de espacios. (Históricamente, este movimiento continuo en un espacio
3D tiene sus raíces en los primeros usos de la animación 3D por computadora: primero
para simulaciones de vuelo y, después, en recorridos y disparadores en primera persona).
Mientras atravesamos los mismos espacios varias veces, cada vez son interpretados en
una paleta de colores diferente. La transparencia y los niveles de reflexión también
cambian. Lislegaard está jugando con el espectador: mientras la estructura general del
film se vuelve clara rápidamente, es imposible seguir el espacio en que estamos en algún
momento. Nunca estamos seguros si ya hemos estado ahí o si es un espacio nuevo.
Bellona puede ser visto como una alegoría de la “forma variable”. En este caso, la
variabilidad se lleva a cabo como paletas de colores y ajustes de transparencias infinitas.
No importa cuantas veces hayamos visto el mismo espacio, siempre puede aparecer de
una nueva forma.
Enseñarnos nuestro mundo y nosotros mismos de una manera nueva es, claro, el objetivo
de todo arte moderno, sin importar el medio. Al sustituir las constantes por variables, el
software de medios institucionaliza este deseo. Ahora todo puede cambiar y todo puede

278
ser interpretado (“rendereado”) de nueva forma. Por supuesto, simples cambios en color o
variaciones en una forma espacial no son suficientes para crear una nueva visión del
mundo. Hace falta talento para transformar las posibilidades que ofrece el software en
supuestos significativos y experiencias originales. Lislegaard, Blake y Murata (junto con
muchos otros diseñadores y artistas talentosos que siguen trabajando) nos ofrecen
visiones distintas y originales de nuestro mundo en un estado de transformación y
metamorfosis continuo: visiones que son totalmente apropiadas para nuestro tiempo de
rápidos cambios sociales, tecnológicos y culturales.
Amplificación
Aunque las discusiones en este capítulo no han cubierto todos los cambios que se
produjeron durante la Revolución de Terciopelo, sí debe dejar clara la magnitud de las
transformaciones en la estética de la imagen en movimiento y en las estrategias de
comunicación. A pesar de que podemos nombrar muchos factores sociales que pudieron
haber jugado un rol importante (el surgimiento de las marcas, la economía de la
experiencia, mercados jóvenes y el web como plataforma global de comunicación en los
90’s), creo que estos factores, solos, no pueden abarcar el diseño específico y la lógica
visual que vemos hoy en la cultura de medios. De forma similar, no pueden ser explicados
con sólo decir que la sociedad consumista contemporánea requiere constante innovación,
estética y efectos. Esto puede ser cierto (pero ¿por qué vemos estos lenguajes visuales, en
particular, y no otros? Y, ¿cuál es la lógica de conduce su evolución?) Considero que para
entender esto de forma adecuada, necesitamos observar el software de creación,
producción y diseño, y sus usos en el ambiente de producción, que pueden ir de una
persona con una laptop a una variedad de compañías de producción alrededor del mundo
con miles de personas colaborando en un mismo proyecto de gran escala, como un
largometraje. En otras palabras, necesitamos usar la perspectiva de los Estudios de
Software.
Los creadores del software usado en la producción de medios usualmente no se
establecen la creación de una revolución. Al contrario, el software es creado para encajar
en procedimientos existentes de producción, puestos de trabajo y tareas familiares. Pero

279
las aplicaciones de software son como especies en una misma ecología (en este caso, un
ambiente compartido de computadoras digitales). Una vez “lanzado”, empiezan a
interactuar, mutar y crear nuevos híbridos. La Revolución del Terciopelo puede, por
consecuencia, ser entendida como el periodo de hibridación sistemática entre diferentes
especies de software originalmente diseñadas para trabajar en diferentes medios. Para
1993, los diseñadores tenían acceso a varios programas que ya eran bastante poderosos
pero muy incompatibles: Illustrator para hacer dibujos vectoriales, Photoshop ara editar
imágenes de tonos continuos, Wavefront y Alias para modelar y animar 3D, After Effects
para animación 2D, etc. Para finales de los 90’s, fue posible usarlos en un mismo flujo de
trabajo. Un diseñador podía ahora combinar operaciones y formatos representacionales,
como una imagen de bitmaps fija, una secuencia de imágenes, un dibujo vectorial, un
modelo 3D y un video digital, específicos a estos programas en un mismo proyecto. Creo
que el lenguaje visual híbrido que vemos hoy a través de la cultura de la “imagen en
movimiento” y del diseño de medios en general es en gran medida el resultado de este
nuevo ambiente de producción. Mientras este lenguaje soporta, aparentemente,
numerosas variaciones, como se ve en diseños particulares, su propiedad estética clave se
puede resumir en una idea: remezcla profunda de lenguajes de medios previamente
separados.
Como ya lo he dicho más de una vez, el resultado de esta hibridación no es simplemente
una suma mecánica de las partes previamente existentes, sino nuevas “especies”. Esto
aplica tanto para el lenguaje visual de diseño particulares como para las operaciones
mismas. Cuando una operación de medios pre-digital se integra en el ambiente general de
un software de producción, comúnmente funciona de nueva manera. Quisiera concluir con
el análisis a detalle de cómo funciona este proceso en el caso de una operación en
particular (con el fin de enfatizar una vez más que la remixabilidad de los medios no se
trata simplemente de añadir contenido de diferentes medios, o de añadir sus técnicas y
lenguajes juntos). Y como se entiende comúnmente que el remix en la cultura
contemporánea tiene este tipo de adiciones, podríamos usar un nuevo término para
referirnos a las transformaciones que ilustra el ejemplo que sigue. He llamado
provisionalmente a esto “remezcla profunda”, pero lo que es importante es la idea y no un
término en particular (así que si tienes una sugerencia de un mejor término, envíame un
correo electrónico).

280
¿Qué significa cuando vemos efectos de profundidad de campo en gráficos animados,
películas y programas de televisión que usan acción viva, o gráficos 3D foto-realistas, pero
que tienen una apariencia más estilizada? La profundidad de campo, originalmente un
artefacto de la grabación basada en lentes, fue simulada en el software en los 80’s,
cuando el objetivo del campo de las gráficas computacionales 3D fue crear un “fotorealismo”
máximo, es decir, escenas sintéticas que no se distinguen de la cinematografía
de acción viva. Pero una vez que esta técnica se volvió disponible, los diseñadores de
medios gradualmente se dieron cuenta que podía ser usada sin importar qué tan realista o
abstracto es el estilo (siempre y cuando hubiera se evocara un espacio 3D). Tipografía que
se mueve en perspectiva a través de un espacio vacío; personajes 2D dibujados, puestos
en diferentes capas del espacio 3D; un campo de partículas animadas (cualquier
composición espacial puede ser puesta en la profundidad de campo simulada).
El hecho que este efecto sea simulado y retirado de su medio físico original significa que
un diseñador puede manipularlo en una variedad de formas. Los parámetros que definen
cuál parte del espacio está activo pueden ser animados independientemente, es decir,
pueden definirse para cambiar en el tiempo (porque son simplemente los números
controlando el algoritmo y no algo construido en la óptica del lente físico). Así que,
mientras la profundidad de campo simulada mantiene la memoria del medio físico
particular de donde vino, se vuelve esencialmente una nueva técnica que funciona como
un “personaje” en su propio derecho. Tiene la fluidez y versatilidad no disponible
previamente. Su conexión con el mundo real es, en el mejor de los casos, ambigua. Por un
lado, sólo tiene sentido usar la profundidad de campo si estamos construyendo un espacio
3D, incluso si está definido de forma mínima, usando sencillamente unas pocas o una sola
señal de profundidad, como las líneas convergentes hacia la línea de horizonte. Por otro
lado, el diseñador ahora puede “dibujar” este efecto en cualquier forma que desee. El ejes
que controla la profundidad de campo no tiene que ser perpendicular a la imagen plana, el
área activa puede estar en cualquier lugar del espacio, también se puede mover
rápidamente alrededor del espacio, etc.
En resumen, cuando quitamos la profundidad de campo de su hardware original (cámaras
de foto y cine) y los movemos al software, cambiamos cuándo y cómo puede ser usado. Lo

281
podemos seguir usando en su contexto original (el de los medios basados en lentes). Esto
es, lo podemos aplicar a elementos gráficos 3D por computadora con el fin de hacerlos
más compatibles con el video capturado mediante lentes. Pero también lo podemos usar
con muchos otros medios, para un efectos meramente artístico. Y lo podemos usar en
muchas formas que no habían sido consideradas cuando era parte de su hardware
original.
Siguiendo la Revolución de Terciopelo, la carga estética de muchos diseños de medios es
derivada de “simples” operaciones de remix: yuxtaposición de diferentes medios en lo que
llamamos “montaje de medios”. Sin embargo, para mí, la esencia de esta Revolución es
fundamentalmente una “remezcla profunda”, ilustrada, en este ejemplo, en la forma en
cómo la profundidad de campo se amplifica grandemente cuando se simula en software.
La informatización virtualizó prácticamente todos las técnicas de creación y producción de
medios, “extrayéndolos” de sus medios físicos particulares y volviéndolos algoritmos. Esto
significa que, en muchos casos, ya no veremos ninguna de las técnicas pre-digitales en su
estado original puro. Esto es algo que ya he discutido en general cuando vemos la primer
etapa de la historia del software cultural, es decir, los 60’s y los 70’s, examinando el
trabajo de Sutherland en el primer editor gráfico interactivo, Sketchpad, los conceptos de
Nelson sobre el hipertexto e hipermedios, y las discusiones de Kay sobre el libro
electrónico (“ [el libro electrónico] no tiene que ser tratado como un libro de papel
simulado debido a que es un nuevo medio con nuevas propiedades”). Ahora hemos visto
cómo esta idea general, ya articulada en los 60’s, se abrió paso hasta los detalles de las
interfaces y herramientas de aplicaciones para el diseño de medios que, eventualmente,
reemplazaron la mayoría de las herramientas tradicionales: After Effects, Illustrator,
Photoshop, Flash, Final Cut, etc. Así que lo que es cierto para la profundidad de campo
también lo es para muchas de las otras herramientas que ofrecen las aplicaciones de
diseño de medios.
Lo que fue un conjunto de conceptos teóricos implementados unos cuantos sistemas de
software sólo accesibles a sus creadores en los 60’s y 70’s (como Sketchpad o la estación
de trabajo de Xerox PARC), más tarde se convirtió en un ambiente de producción universal
usado actualmente en todas las áreas de las industrias culturales. Las interacciones en

282
curso entre las ideas de la industria del software y los deseos de los usuarios sobre sus
herramientas (diseñadores de medios, diseñadores gráficos, editores de film, etc.) llevó a
una evolución posterior del software, por ejemplo el surgimiento de una nueva categoría
de sistemas de “gestión de recursos digitales” en los primeros años de los 2000’s, o el
concepto de “tubería de producción” que se volvió importante a mitad de esa década. En
este capítulo hemos descrito solo una dirección, entre muchas otras, de la evolución del
software, sus herramientas y sus formatos de medios. Como hemos visto, el resultado de
esta tendencia fue el surgimiento de un nuevo tipo de estética que hoy domina la cultura
visual y de medios.

283
Conclusión
Software, hardware y medios sociales
En 1997, Alan Kay y Adele Goldberg imaginaron que una mundo de la informática se
convertiría en un “metamedio” que contendría “una gran variedad de medios existentes y
aún no inventados”. Justo como lo predijeron, las computadoras han sido usadas para
inventar varios nuevos tipos de medios que son simulaciones de medios físicos previos.
Los ejemplos incluyen espacios navegables tridimensionales construidos con gráficas
computacionales, bases de datos de medios, o videojuegos “sim” como SimCity, The Sims
y Civilization. Y, claro, el Internet en particular ha sido un anfitrión muy productivo para
inventar nuevos tipos de comunicación y colaboración: e-mail, foros, blogs, micro-blog
(como Twitter), wikis, RSS, redes sociales, etc.
Al mismo tiempo, el metamedio computacional también evolucionaba en otra dirección.
Mientras que efectivamente llegó a integrar “una gran variedad de medios existentes”, ya
hemos discutido que una vez que estos medios eran simulados en una computadora, su
identidad cambiaba. Sutherland, Engelbart, Nelson, Kay y demás inventores de
metamedios informáticos entendieron que las simulaciones de medios existentes, físicos y
electrónicos, podían añadir muchas nuevas propiedades a estos medios. Como Kay y
Goldberg lo escriben en su artículo, un medio simulado se puede volver un “nuevo medios
con nuevas propiedades”.
Una computadora “sopla nueva vida” al medios físico y electrónico que simulan. Los
medios se pueden volver “dinámicos” (para usar una palabra que Kay prefería, en lugar de
“interactivo”). También pueden ser “inteligentes”: pensemos en Sketchpad, que
automáticamente puede “limpiar” los bocetos hechos por los diseñadores al aplicar ciertos
parámetros, como el paralelismo. Además, se pueden volver colectivamente compartibles
y colectivamente modificables. Pensemos en proyectos de software social de gran escala
como Wikipedia y OpenStreetMap. Los objetos de los medios, como las imágenes, sonido,
video, text y código, pueden brincar de máquina en máquina de manera verdaderamente
mágica: de una teléfono móvil a un reproductor de medios, después a una tarjeta de

284
memoria, a una laptop, al web, etc. Imagina que vivimos en el siglo XVI y nos dicen que
podemos ordenarle a una imagen en una pintura que viaje y que aparezca en otra pintura
en otro país. O que el texto en un libro se pueda borrar y reemplazar por otro. Y esto es
exactamente lo que muchos de nosotros hacemos diario, sin siquiera pensar en lo mágico
que es.
Creo que la escala, diversidad y radicalidad de estas “adiciones” (aquellas ya inventadas y
muchas más que serán inventadas en el futuro) es de tal magnitud que explorar lo que
puede ser hecho con ellas nos llevará mucho tiempo. Y ésta es una de las razones por las
cuales la “revolución digital” es diferente de todas las otras revoluciones tecno-culturales
anteriores. La habilidad de simular no sólo uno o dos sino la mayoría de los medios en una
computadora (combinado con las habilidades de la computadora para controlar, procesar
en tiempo real, calcular, transformar entradas, probar escenarios si…entonces, y enviar
información por las redes) abre un espacio ilimitado de posibilidades creativas.
A medida que seguimos la evolución del medio computacional, desde la primera etapa de
su invención y experimentación (1960’s-1970’s) a la segunda etapa de comercialización y
adopción masiva (1980’s-1990’s), hemos descubierto la tercera etapa en esta evolución
del metamedio computacional: la hibridación. Al ser traducidos en software, los diferentes
tipos de medios empezaron a actuar como especies en una ecología común (en este caso,
un ambiente compartido de software). Una vez que se “soltaron” en este ambiente
empezaron a interactuar, mutar y hacer híbridos. Tanto los medios simulados como los
nuevos tipos (texto, hipertexto, fotografías fijas, gráficos vectoriales, video digital,
animación 2D, modelos 3D, espacios navegables, mapas, información geográfica,
mensajes, códigos de programación) se volvieron los componentes básicos para muchas
combinaciones de medios. Como lo muestran muchos ejemplos, dichos híbridos pueden
ser usados a diferentes escalas, desde sistemas software de gran escala, como Google
Earth, hasta las imágenes y cortos de gráficos animados creados por diseñadores
individuales.
La llegada de los medios y redes sociales en el web a mediados de los 2000’s, su
expansión a plataformas móviles en los siguientes años, y el desarrollo de mercados para
las apps de estas plataformas llevó a nuevos tipos de híbridos. Los elementos funcionales

285
de los medios sociales y móviles (búsqueda, clasificación, posteo en el muro, suscripción,
mensaje de texto, e-mail, llamadas de voz y video, etc.) forman su propio ecosistema de
medios. Así como el ecosistema de técnicas para la creación, edición y navegación de
medios realizado en el software profesional de medios durante los 90’s, éste ecosistema
permite más interacciones entre sus elementos. Estos elementos están combinados en
una variedad de formas en diferentes plataformas y en diferentes apps de medios
sociales. Las nuevas “características” del software social y móvil entran en este
ecosistema y expanden la reserva de elementos.
Las tecnologías detrás de estos elementos, como las aplicaciones web de terceros y script
del lado del servidor, facilitan la creación de nuevas combinaciones y elementos, en
comparación con las aplicaciones de escritorio267. Mientras que las nuevas versiones de
aplicaciones de escritorio, como Photoshop, InDesign o Maya, pueden contener nuevas
técnicas, éstas se lanzan de manera poco frecuente. Las aplicaciones en línea, como
Facebook, YouTube o Google Search pueden ser actualizadas por las compañías de forma
muy frecuente (Facebook y Google actualizan su código diariamente) y se les pueden
añadir nuevas propiedades a cada momento.
La competencia entre servicios líderes de redes sociales ha sido un motor para nuevos
elementos, así como de variaciones en elementos existentes. Para dar un par de ejemplos
del 2011, Google+ introdujo la función de los “Círculos”, para permitir a los usuarios
organizar contactos en grupos y controlar lo que publican y comparten en cada grupo268.
Facebook introdujo un botón de Suscripción para permitir a los usuarios estar en contacto
con las novedades que publican contactos particulares269. En el mismo año, el nuevo
servicio de foto social, Pinterest, se elevó meteóricamente, debido a sus nuevas funciones
267 Para una buena explicación de cómo las aplicaciones web funcionan, técnicamente, ver:
http://en.wikipedia.org/wiki/Web_application
268 Vic Gundotra, “Introducing the Google+ project: Real-life sharing, rethought for the web,” Junio
28, 2011, http://googleblog.blogspot.com/2011/06/introducing-google-project-real-life.html
269 Meghan Peters, “Facebook Subscribe Button: What It Means for Each Type of User,”
mashable.com, Septiembre 15, 2011, http://mashable.com/2011/09/15/facebook-subscribeusers/.

286
como la disposición flexible de fotos, en contraste con la rigidez cuadriculada de Facebook
y Google+.
Mientras que este libro se enfoca en la evolución de software para la creación y edición de
medios, ésta evolución está íntimamente ligada con los desarrollos prácticos de hardware,
incluyendo computadoras de escritorio, laptops, plataformas móviles, redes, servidores,
granjas de render, tarjetas de video, pantallas, etc. Pantallas de mayor resolución, discos
duros más rápidos y con mayor capacidad, redes más rápidas, mayor facilidad de conexión
entre aparatos para captura, distribución y reproducción de medios… todos estos
desarrollos amplifican las capacidades de los medios informáticos, cambiando lo que
podías ser imaginado y diseñado. Por ejemplo, una pantalla digital con una resolución de
35,840 x 8,000 pixeles (la computadora de súper-visualización HIPerSpace que mi
laboratorio ha tenido la fortuna de usar desde 2008) no sólo es diferente en cantidad sino
también en calidad, comparado con una pantalla de 1024 x 768 pixeles (el estándar de
las computadoras de escritorio de los 90’s)270. De forma similar, la experiencia de usar
Internet con una conexión de banda ancha, es cualitativamente diferente a la de un
módem que se marca analógicamente la línea telefónica (también un estándar en los
90’s).
Cuando Kay y sus colegas implementaron varios editores de medios en su “Dynabook” de
principios de los 1970’s, mucho de este software no podía competir con sus herramientas
físicas y electrónicas equivalentes. Recuerdo mi experiencia al trabajar con una Macintosh
de primera generación en 1984: sólo podía mostrar 16 niveles de grises en una pantallas
de 512 x 384 pixeles que medía 9 pulgadas. Obviamente esto no era suficiente para que
yo pudiera dejar mis pinceles y pinturas al óleo y cambiarme a la computadora. Así que, de
alguna manera, la primer etapa de la informatización de medios (entre Sketchpad de
1963, y PageMaker de 1985) fue teórica. Durante este periodo, los principios
conceptuales y los algoritmos clave necesarios para la simulación detalladas de medios
270 Algunas fotografías del HIPerSpace están disponibles en:
http://lab.softwarestudies.com/2008/12/cultural-analyticshiperspace-and.html. Una descripción de
la computadora HIPerSpace se encuentra en:
deschttp://vis.ucsd.edu/mediawiki/index.php/Research_Projects:_HIPerSpace

287
físicos fueron desarrollados, antes de que el hardware accesible estuviera disponible. Por
ejemplo, durante los 1960’s, muchos ingenieros informáticos aprendieron Sketchpad al
leer la tesis de doctorado de Sutherland, debido a que la máquina que lo corría (la TX-2)
sólo existía en el MIT. (Esta es otra característica importante de la revolución de los medios
informáticos, fue teorizada a detalle antes de que ocurriera en la práctica).
Pero durante la segunda parte de los 1990’s, el hardware de computadoras personales
avanzó suficientemente como para correr simulaciones de la mayoría de los medios, con
fidelidad comparable a los estándares profesionales existentes. Esto incluía software para
el diseño gráfico, CAD (Diseño Asistido por Computadora), modelación 3D, animación 2D y
3D, layout de impresión, manipulación de fotos digitales y edición no lineal de audio y
video. En muchos casos, estas aplicaciones respondían a las acciones de los usuarios de
forma tan rápida como para alcanzar el nivel de interactividad conceptualizado por Kay y
Goldberg en 1977 (aunque la animación 3D y la composición de video necesitan, todavía
hoy, largos periodos de render). Como resultado, los medios simulados se volvieron
verdaderamente útiles y accesibles a un gran número de personas que no estaban en
laboratorios de computación ni en grandes compañías de medios. En un lustro, muchos de
los profesionales de la cultura abandonaron los medios físicos por sus equivalentes
simulados.
Cuando visité al famoso músico electrónico, escritor y artista DJ Spooky en su
departamento de Tribeca, Nueva York, en enero 2005, no vi ningún instrumento musical,
tradicional o electrónico. El único “instrumento” que tenía Paul Miller (mejor conocido
como DJ Spooky That Subliminal Kid) era su laptop Apple PowerBook de 15 pulgadas.
Ésta era su “Dynabook”: un “contenedor para la manipulación de conocimientos en un
paquete portátil del tamaño y forma de un cuaderno ordinario”271. Aunque este
“Dynabook” no tenía instalado Smalltalk, sí corría otro ambiente de programación
poderoso, rápido y basado en la programación visual: MAX, el lenguaje preferido de
decenas de miles de músicos electrónicos, VJ’s, bailarinas, coreógrafos y demás artistas
de performance en tiempo real.
271 Kay & Goldberg, “Personal Dynamic Media,” p. 394.

288
Mientras la evolución del hardware permitió la diseminación de software de medios a
comunidades profesionales en los 1990’s, en los 2000’s esta evolución también permitió
la nueva etapa en el diseño y uso de software: los medios sociales (a partir del 2004). A las
pocas compañías que dominaban el campo de aplicaciones de medios profesionales
(Adobe, Apple, Autodesk) se les unieron una multitud de nuevas compañías e
innumerables pequeñas empresas que desarrollaban herramientas y servicios para el web
y plataformas móviles.
Las nuevas categorías de software incluyen: redes sociales (como Facebook); servicios de
micro-contenidos (como Twitter); sitios de intercambio en línea (como YouTube, Vimeo,
Picasa, Flickr, etc.); software de organización y edición rápida de medios para
consumidores (como iPhoto); editores de blog (como Blogger, WordPress); y, muchas más.
Recordemos que el software (especialmente las aplicaciones y servicios web y móviles
diseñados para consumidores) evoluciona constantemente, así que algunas de estas
categorías, su popularidad y sus funciones pueden cambiar para cuando leas esto. Un
ejemplo gráfico es el cambio en la identidad de Facebook. En 2007, se cambió de ser una
aplicación más de medios sociales que competía con MySpace a un “sistema operativo
social” con el objetivo de combinar funcionalidad de diferentes aplicaciones en un solo
lugar (reemplazando, por ejemplo, software de e-mail disponible por separado).
Ninguna de las apps y sitios web de la “era de los medios sociales” tiene la función de
aislamiento. Más bien, participan en una ecología más larga, que incluye motores de
búsqueda, motores de recomendación, sistemas de blog, flujos RSS y demás tecnologías
web; aparatos electrónicos de bajo costo disponibles a los consumidores para capturar y
acceder a medios (cámaras digitales, teléfonos móviles, reproductores de música y video,
portarretratos digitales, televisiones con Internet); y, tecnologías que permiten la
transferencia de medios entre aparatos, gente y el web (aparatos de almacenamiento,
tecnologías inalámbricas como WiFi y WiMax, estándares de comunicación como USB y
4G). Sin esta ecología, muchos de los servicios web y apps móviles no serian posibles. Por
consecuencia, esta ecología debe ser tomada en cuenta en cualquier estudio sobre las
redes sociales y su software (así como del acceso a contenidos a nivel del consumidor y
desarrollo de software de medios diseñado para trabajar con sitios de intercambio
basados en el web). Y aunque los elementos particulares en esta ecología, y sus

289
relaciones, tiendan a cambiar (por ejemplo, casi todo el contenido de medios se puede
volver disponible vía redes informáticas, la comunicación entre aparatos se puede volver
totalmente transparente, y la rígida separación entre personas, los aparatos que controlan
y el espacio pasivo “poco inteligente”, se puede empezar a borrar) la idea misma de una
ecología tecnológica con muchas partes interactuando, incluyendo software, no es poco
probable que desaparezca en el corto plazo. Así que si un día debo escribir un estudio
detallado de los medios sociales, necesitaré considerar los aparatos electrónicos de los
consumidores, las arquitecturas de redes y protocolos, y demás elementos de esta
ecología, además del software social mismo.
Los medios después del software
Un resumen de los argumentos teóricos de un libro de 100,000 palabras no puede
abarcar todos los puntos importantes. Sin embargo, tomaré el riesgo porque creo que
incluso un resumen incompleto será útil para los lectores. He aquí algunas de las
propuestas desarrolladas en este libro sobre las experiencias y significados de los
“medios” para los diseñadores contemporáneos, quienes los crean con aplicaciones de
software, y paras los usuarios de las aplicaciones interactivas y servicios de medios:
1. La computadora no es un nuevo “medio”, es el primer “metamedio”: una
combinación de medios existentes, nuevos y aún no inventados (este es el
argumento de Kay, que tomo como mi punto de partida).
2. Un “medio” (tal como existe en el software) es una combinación de técnicas
particulares para la generación, edición y acceso a contenidos (empleo el término
genérico “acceso” como un alias para una larga lista de términos: navegación,
búsqueda, visualización, escucha, lectura e interacción). La softwareización
virtualiza las técnicas existentes y añade muchas nuevas más. Todas estas
técnicas juntas forman el “metamedio informático”. Cualquier “medio” usa un
subconjunto de éstas.
Nuevas técnicas, y sus variaciones, están constante desarrollo, lo que cambia la
identidad de cada medio que las usa. Para los usuarios de software comercial y
popular de medios, un medio cambia con cada versión del software.

290
3. “Lo que identificamos por inercia conceptual como “propiedades” de los
diferentes medios son en realidad las propiedades del software de medios: sus
interfaces, herramientas y técnicas que hacen posible el acceso, navegación,
creación, modificación, publicación e intercambio de documentos de medios”.
4. Siguiendo la misma lógica, las “propiedades” de cualquier objeto de medios ya no
está definidas por el contenido y los formatos de los archivos que almacenan la
información. Ahora también dependen del software usado para acceder a este
objeto. Por ejemplo, dependiendo de cómo una misma imagen es accedida vía un
visualizador de medios por default, una app para consumidores, o un software
profesional de producción como Photoshop, sus “propiedades” cambian
significativamente”.
5. Las técnicas que componen el metamedio informático pueden ser divididas en
dos categorías. Las técnicas de propósito general (es decir, “independientes de
los medios”) son implementadas para trabajar de forma similar en todos los tipos
de medios (por ejemplo, seleccionar, copiar, buscar, filtrar, etc.). Las técnicas
específicas a medios sólo pueden funcionar en estructuras de datos particulares
(por ejemplo, podemos incrementar la amplitud de una pista de sonido o reducir el
número de vértices de un modelo 3D, pero no viceversa). Cada “medio” de
software combina técnicas independientes y específicas.
6. La idea de una estructura de datos lleva a una definición alternativa de medio de
software. “Como es definido por el software de aplicación y experimentado por los
usuarios, un “medio” es el vínculo de una estructura de datos particular y de los
algoritmos de creación, edición y visualización de contenido almacenados en esta
estructura”.
Desde este perspectiva, se puede decir que cada una de las categorías de
software de desarrollo de medios definen su propio “medio”, debido a que los
programas ofrecidos en cada una de esta categorías comparte habitualmente
(aunque no siempre) las mismas estructuras de datos fundamentales. Los
ejemplos que caben aquí son los editores de gráficos vectoriales, editores de
gráficos bitmap, software de animación 2D y motion graphics, software de gráficas
computacionales 3D, editores de sonido, procesadores de texto y editores HTML.
7. Siguiendo la primera etapa de la invención del metamedio informático, entramos
a la siguiente etapa de la “hibridación” y “remezcla profunda” de los medios. “Las

291
propiedades y técnicas únicas de diferentes medios se vuelven elementos de
software que se pueden combinar en formas previamente imposibles”. “Tanto los
tipos de medios simulados como los nuevos (texto, hipertexto, fotografías fijas,
video digital, animación 2D, animación 3D, espacios navegables 3D, mapas,
información geográfica) funcionan como componentes básicos para muchas
combinaciones de nuevos medios”.
Esta condición no es la simple consecuencia del código digital universal usado por
los tipos de medios. Más bien, es el resultado de un desarrollo gradual de
tecnologías interoperables, incluyendo formatos de archivo estándares, funciones
de importar/exportar en las aplicaciones y los protocolos de red.
8. Las formulaciones previas nos llevan a considerar el desarrollo de medios
contemporáneos con el modelo de la evolución biológica y su concepto de número
masivo de especies que comparten rasgos comunes (separándonos del modelo
moderno de un número pequeña de diferentes medios que sus lenguajes únicos).
En lugar de intentar ubicar un proyecto, app o servicio web particular en alguna
categoría elegida de un número pequeño, más bien lo podemos ver como una
combinación de técnicas seleccionadas de una conjunto muy grande. Algunas de
estas combinaciones suceden más a menudo; otras sólo ocurren una vez. Las
combinaciones exitosas se vuelven populares, llevando a proyectos similares y se
pueden convertir patrones de diseño usados en numerosas aplicaciones.
Epistemología del software
Una de las ideas clave de este libro es que el metamedio informático se caracteriza por su
“extensibilidad permanente”. Nuevas técnicas y algoritmos que funcionan con un tipos
comunes de datos de medios y formatos de archivo puede ser inventado en todos
momento por cualquiera con las habilidades necesarias. Estas invenciones puede
distribuirse instantáneamente por el web, sin necesidad de los grandes recursos que se
requerían en el siglo XX para introducir un nuevo aparato comercial de medios. El uso de
código abierto y licencias de software libre, y de repositorios web para como GitHub,
estimula la expansión colectiva de herramientas de software existentes, que seguido lleva
a su rápida evolución.

292
La extensibilidad permanente del metamedio informático tiene consecuencias
importantes, no solo en la forma en cómo creamos e interactuamos con los medios, sino
también en las técnicas de conocimiento en una “sociedad informatizada” (“Conocimiento
en las sociedades informatizadas” es el subtítulo de la primera sección del famoso libro La
condition postmoderne : rapport sur le savoir de Jean-François Lyotard en 1979).
Convertir todo en datos y usar algoritmos para analizarlo, cambia lo que significa conocer
algo. Crea nuevas estrategias que, unidas, crean una epistemología del software. La
epistemología es una rama de la filosofía que se interroga sobre lo que es el conocimiento,
cómo se adquiere y en qué medida un sujeto puede ser conocido. El código digital, la
visualización de datos, los GIS, la búsqueda de información, las técnicas de aprendizaje
automático, el constante incremento de la velocidad de procesadores y el decremento de
sus costos, las tecnologías de análisis de big data, los medios sociales y otras partes del
moderno universo tecno-social introducen nuevas formas de adquisición de conocimientos
y, en el proceso, redefinen lo que es el conocimiento.
Por ejemplo, siempre es posible inventar nuevos algoritmos (o nuevas formas de escalar
algoritmos existentes para analizar más rápido el big data) que puedan analizar los datos
existentes de hoy en formas que los algoritmos previos no podían. Como resultado,
podemos extraer patrones adicionales y generar nuevas información de los datos viejos ya
analizados.
Los algoritmos y las aplicaciones de software que analizan imágenes y video proveen
ejemplos particularmente llamativos de esta capacidad de generar información adicional a
partir de los datos que hace años, e incluso décadas, fueron grabados.
En Blowup, una película de Michelangelo Antonioni (1966), el fotógrafo que toma fotos
de una pareja besándose en el parque usa la estrategia de impresión progresiva de
agrandamientos de un área de la fotografía, hasta que un granulado close-up revela un
cuerpo en el césped y un hombre armado escondido detrás de los árboles.

293
Durante el periodo en que este film fue creado, y con desconocimiento para su director, los
investigadores en ciencias computacionales estaban desarrollando el nuevo campo del
procesamiento digital de imágenes, incluyendo algoritmos para el mejoramiento de
imágenes como el enfoque de bordes, ampliación de contraste y reducción de ruido. Los
primeros artículos en el campo mostraban las fotografías borrosas tomadas por aviones de
seguridad, que después eran enfocadas por algoritmos. Como ya he mencionado antes,
hoy muchas de estas técnicas están integradas en todos los software de manipulación de
imágenes, como Photoshop, así como en el firmware de cámaras digitales. Se han vuelto
esenciales para los fotografía amateur y los medios visuales comerciales, a medida que
cada fotografía publicada en los medios masivos pasa primero por algunos ajustes con
software.
Los modelos DSLR contemporáneos y las cámaras compactas de alta gama pueden grabar
imágenes en formato JPEG y Raw. Con JPEG, una imagen está comprimida, lo que limita
sus posibilidades para una posterior extracción de información adicional con software. El
formato Raw almacena datos del sensor de la cámara sin procesarlos. El uso de este
formato asume que el fotógrafo manipulará la foto con software para obtener los mejores
resultados de los millones de pixeles grabados por la cámara. En la guía para el uso de
ambos formatos, William Porter explica: “al trabajar con archivos Raw, tenemos mayores
oportunidades de recuperar reflejos en nubes sobreexpuestas, rescatar detalles en áreas
de sombra u oscuras, y corregir los problemas de balance de blancos. Podemos reducir el
ruido de las imágenes y maximizar su nitidez, con mayor finura”272.
Esto significa que nuevos software con mejores algoritmos pueden generar nueva
información de una foto en formato Raw capturada años atrás. (Para una dramática
demostración de esto, ver los ejemplos del artículo de Porter, disponibles en línea)273. En
otro ejemplo de epistemología del software, en el demo presentado en una de las
conferencias anuales SIGGRAPH, unas cuantas tomas de film de Hollywood de los 1940’s
272 William Porter, “Raw vs. JPEG: Which should you shoot and when?”, techhive.com (Septiembre
19, 2012).
273 Ibid.

294
fueron manipuladas con software para re-generar las mismas tomas desde un punto de
vista diferente.
En la producción de efectos visuales de hoy, uno de las operaciones usadas más
ampliamente es la extracción algorítmica de la posición de la cámara de video que se usó
para capturar la toma. Esta operación se llama “rastreo de movimiento” (“motion
tracking”) y ejemplifica cómo la información que no está directamente disponible en los
datos puede ser inferida con algoritmos. (Esta posición de cámara extraída, es usada para
insertar gráficas computacionales sobre la acción viva en la perspectiva correcta).
Otro tipo importante de epistemología del software es la fusión de datos: usar datos de
diferentes fuentes para crear nuevo conocimiento que no está explícitamente contenido
en ninguno de ellos. Por ejemplo, usando las fuentes del web es posible crear la
descripción de un individuo mediante la combinación de piezas de información de sus
diferentes perfiles de medios sociales y haciendo deducciones a partir de estos.
Combinar fuentes separadas de medio también da significado adicional a cada una de
estas fuentes. Consideremos la técnica de costura automática de varias fotos separadas
en un solo panorama, que está disponible en la mayoría de cámaras digitales. En sentido
estricto, los algoritmos subyacentes no añaden nueva información a cada una de las
imágenes (es decir, sus pixeles no se modifican). Pero como cada imagen es ahora parte
de un panorama más largo, su significado cambia frente a un observador humano.
Las habilidades de generar nueva información de datos viejos, fusionar/separar fuentes
de información, y crear nuevo conocimiento de viejas fuentes analógicas, son sólo algunas
técnicas de la epistemología del software. En mis próximas publicaciones espero ir
describiendo gradualmente otras técnicas, a medida que auto-aprendo minería de datos y
demás técnicas de conocimiento algorítmico usadas comúnmente por las sociedades de
software contemporáneas.
Al inicio del libro preguntaba: ¿qué son los medios después del software? Si los medios
artísticos eran definidos tradicionalmente por las técnicas y capacidades de
representación de herramientas y máquinas particulares (pinceles, tinta, papel,

295
instrumentos musicales, imprenta, cámara fotográfica, cámara de cine, cámara de video y
equipo de edición), ¿qué pasa con este concepto después de que la mayoría de estas
técnicas y herramientas han sido simuladas en un solo ambiente de software? En otras
palabras, ¿qué es un “medio”, tal como es definido por las aplicaciones de software
usadas para crearlo, editarlo, distribuirlo y accederlo?
Mientras los “efectos de los medios”, la “representación de los medios”, la “industria de
los medios”, la “teoría de los medios” y la “historia de los medios” han sido estudiadas
ampliamente en gran cantidad de libros y artículos de varias disciplinas, ésta literatura no
incluye el análisis de herramientas y plataformas de software. En contraste, el vasto
universo de libros “hágalo usted mismo”, videos tutoriales, contienen muy poca teoría,
debido a que el objetivo de estas publicaciones es la instrucción práctica. (Mi búsqueda de
“Photoshop” en Amazon.com bajo la rúbrica libros, el 12 de agosto de 2013, dio como
resultado 9,405 publicaciones; la búsqueda de “After Effects” dio 1,201 resultados y la de
“3ds Max” 1,972). El objetivo de mi libro ha sido reducir el hueco entre estos dos
universos separados de teoría y práctica.
Siguiendo la cuestión de lo que significa crear medios con software nos tomó un viaje largo
a través de algunas décadas. Efectivamente encontramos posibles respuestas que ojalá
hayas encontrado interesantes y provocadoras. Pero claro, como mis libros y artículos
están dirigidos a practicantes de medios (profesionales y estudiantes creando nuevas
herramientas y software, diseños gráficos, motion graphics, animaciones, películas,
diseños de espacios, arquitectura, objetos, aparatos y arte digital) puedes hacer algo más
que simplemente estar en acuerdo o desacuerdo con mi análisis. Al inventar nuevas
técnicas, o mediante la aplicación innovadora de técnicas existentes, y encontrando
nuevas formas de representar el mundo, el ser humano y los datos, y nuevas formas para
que las personas se conecten, compartan y colaboren, puedes expandir las fronteras de
los “medios después del software”.